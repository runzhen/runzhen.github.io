<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>使用 Kubernetes 遇到的一些问题和解决思路 | Mind in the Wind</title>
<meta name=keywords content="linux,k8s"><meta name=description content="update on 2022-05-21
今天在 homelab 的 k8s 集群上发生了同样的情况，我想删除一个 namespace，再确认已经把 namespace 里面所有其他资源都删除的情况下，namespace 始终是 Terminating, 找了很多资料，方法也众说纷纭 。
最后通过看 api-server log 发现原来又是 Unable to authenticate the request due to an error: x509: certificate has expired or is not yet valid
root cause 还是我更新 cert 的时候又漏了某些步骤。
事情的起因是 k8s 的 cert 过期了，在目录 /etc/kubernetes/pki/ 下面的这些 cert 都与 k8s 的核心服务息息相关，因此 cert 过期了，整个 k8s 集群就停止服务了。
这个集群是 kubernetes 1.14, 因此需要运行几个命令完成更新，而 1.15 版本以上这个过程简化了不少。 由于之前已经 renew cert 两次了，因此正常按部就班几个操作就完事了，但是这个因为一点小疏忽，加上系统死机重启了一次，花了很多时间去恢复各种服务。
本文记录 debug 的过程中遇到的一些症状，以及后来发现的解决方法，为以后遇到类似问题提供思路。"><meta name=author content="Me"><link rel=canonical href=https://runzhen.github.io/posts/k8s-debug-stories/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.a8f620eb24ba9442cd3765590f9208a0752be50e5a7b9dd9e3e555c8cb5e74e6.css integrity="sha256-qPYg6yS6lELNN2VZD5IIoHUr5Q5ae53Z4+VVyMtedOY=" rel="preload stylesheet" as=style><link rel=icon href=https://runzhen.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://runzhen.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://runzhen.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://runzhen.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://runzhen.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://runzhen.github.io/posts/k8s-debug-stories/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="使用 Kubernetes 遇到的一些问题和解决思路"><meta property="og:description" content="update on 2022-05-21
今天在 homelab 的 k8s 集群上发生了同样的情况，我想删除一个 namespace，再确认已经把 namespace 里面所有其他资源都删除的情况下，namespace 始终是 Terminating, 找了很多资料，方法也众说纷纭 。
最后通过看 api-server log 发现原来又是 Unable to authenticate the request due to an error: x509: certificate has expired or is not yet valid
root cause 还是我更新 cert 的时候又漏了某些步骤。
事情的起因是 k8s 的 cert 过期了，在目录 /etc/kubernetes/pki/ 下面的这些 cert 都与 k8s 的核心服务息息相关，因此 cert 过期了，整个 k8s 集群就停止服务了。
这个集群是 kubernetes 1.14, 因此需要运行几个命令完成更新，而 1.15 版本以上这个过程简化了不少。 由于之前已经 renew cert 两次了，因此正常按部就班几个操作就完事了，但是这个因为一点小疏忽，加上系统死机重启了一次，花了很多时间去恢复各种服务。
本文记录 debug 的过程中遇到的一些症状，以及后来发现的解决方法，为以后遇到类似问题提供思路。"><meta property="og:type" content="article"><meta property="og:url" content="https://runzhen.github.io/posts/k8s-debug-stories/"><meta property="og:image" content="https://runzhen.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-04-27T00:00:00+00:00"><meta property="article:modified_time" content="2022-04-27T00:00:00+00:00"><meta property="og:site_name" content="Mind in the Wind"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://runzhen.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="使用 Kubernetes 遇到的一些问题和解决思路"><meta name=twitter:description content="update on 2022-05-21
今天在 homelab 的 k8s 集群上发生了同样的情况，我想删除一个 namespace，再确认已经把 namespace 里面所有其他资源都删除的情况下，namespace 始终是 Terminating, 找了很多资料，方法也众说纷纭 。
最后通过看 api-server log 发现原来又是 Unable to authenticate the request due to an error: x509: certificate has expired or is not yet valid
root cause 还是我更新 cert 的时候又漏了某些步骤。
事情的起因是 k8s 的 cert 过期了，在目录 /etc/kubernetes/pki/ 下面的这些 cert 都与 k8s 的核心服务息息相关，因此 cert 过期了，整个 k8s 集群就停止服务了。
这个集群是 kubernetes 1.14, 因此需要运行几个命令完成更新，而 1.15 版本以上这个过程简化了不少。 由于之前已经 renew cert 两次了，因此正常按部就班几个操作就完事了，但是这个因为一点小疏忽，加上系统死机重启了一次，花了很多时间去恢复各种服务。
本文记录 debug 的过程中遇到的一些症状，以及后来发现的解决方法，为以后遇到类似问题提供思路。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://runzhen.github.io/posts/"},{"@type":"ListItem","position":2,"name":"使用 Kubernetes 遇到的一些问题和解决思路","item":"https://runzhen.github.io/posts/k8s-debug-stories/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"使用 Kubernetes 遇到的一些问题和解决思路","name":"使用 Kubernetes 遇到的一些问题和解决思路","description":"update on 2022-05-21\n今天在 homelab 的 k8s 集群上发生了同样的情况，我想删除一个 namespace，再确认已经把 namespace 里面所有其他资源都删除的情况下，namespace 始终是 Terminating, 找了很多资料，方法也众说纷纭 。\n最后通过看 api-server log 发现原来又是 Unable to authenticate the request due to an error: x509: certificate has expired or is not yet valid\nroot cause 还是我更新 cert 的时候又漏了某些步骤。\n事情的起因是 k8s 的 cert 过期了，在目录 /etc/kubernetes/pki/ 下面的这些 cert 都与 k8s 的核心服务息息相关，因此 cert 过期了，整个 k8s 集群就停止服务了。\n这个集群是 kubernetes 1.14, 因此需要运行几个命令完成更新，而 1.15 版本以上这个过程简化了不少。 由于之前已经 renew cert 两次了，因此正常按部就班几个操作就完事了，但是这个因为一点小疏忽，加上系统死机重启了一次，花了很多时间去恢复各种服务。\n本文记录 debug 的过程中遇到的一些症状，以及后来发现的解决方法，为以后遇到类似问题提供思路。","keywords":["linux","k8s"],"articleBody":"update on 2022-05-21\n今天在 homelab 的 k8s 集群上发生了同样的情况，我想删除一个 namespace，再确认已经把 namespace 里面所有其他资源都删除的情况下，namespace 始终是 Terminating, 找了很多资料，方法也众说纷纭 。\n最后通过看 api-server log 发现原来又是 Unable to authenticate the request due to an error: x509: certificate has expired or is not yet valid\nroot cause 还是我更新 cert 的时候又漏了某些步骤。\n事情的起因是 k8s 的 cert 过期了，在目录 /etc/kubernetes/pki/ 下面的这些 cert 都与 k8s 的核心服务息息相关，因此 cert 过期了，整个 k8s 集群就停止服务了。\n这个集群是 kubernetes 1.14, 因此需要运行几个命令完成更新，而 1.15 版本以上这个过程简化了不少。 由于之前已经 renew cert 两次了，因此正常按部就班几个操作就完事了，但是这个因为一点小疏忽，加上系统死机重启了一次，花了很多时间去恢复各种服务。\n本文记录 debug 的过程中遇到的一些症状，以及后来发现的解决方法，为以后遇到类似问题提供思路。\nNode 重启后 kubelet 没运行 前面提到，可能是因为 cert 过期后触发某些 bug 导致 Master Node 不能 ssh（之前 renew cert 没有类似问题），所以只能去机房按电源开关重启了。\n重启之后，发现 kubelet 没有运行，于是尝试 systemctl restart kubelet, 发现没有效果，那就看看 status\n$ systemctl status kubelet.service ● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubeadm.conf Active: activating (auto-restart) (Result: exit-code) since Tue 2022-04-26 22:09:42 PDT; 4s ago Docs: https://kubernetes.io/docs/home/ Process: 18121 ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS (code=exited, status=255) Main PID: 18121 (code=exited, status=255) 一搜索，发现结果都是告诉你 cert 过期了，一切似乎很简单，我已经 renew 过几次 cert 了，操作起来轻车熟路。 正因为轻车熟路，所以忽略了要删除原来的旧文件，导致更新 cert 后 kubelet.service 仍然无法启动。\n这就要说到下面这个坑了。\n更新 cert 需要删除原来的文件 k8s cert 过期后如何 renew？ 网上有很多文章不再赘述，针对 1.14 版本，简而言之是运行下面几个命令\n1. kubeadm alpha certs renew all 2. kubeadm init phase kubeconfig all --apiserver-advertise-address=192.168.31.50 3. cp /etc/kubernetes/admin.conf ./kube/config 4. systemctl restart kubelet 其中第一步运行完之后，/etc/kubernetes/pki 目录下除了 CA 以外，所有的 cert 都被更新了，这一步很简单不需要特殊注意什么。\n/etc/kubernetes/pki$ ls -al total 72 drwxr-xr-x 3 root root 4096 Aug 3 2020 . drwxr-xr-x 4 root root 4096 Apr 27 01:26 .. -rw-r--r-- 1 root root 1216 Apr 26 22:47 apiserver.crt -rw-r--r-- 1 root root 1090 Apr 26 22:47 apiserver-etcd-client.crt -rw------- 1 root root 1679 Apr 26 22:47 apiserver-etcd-client.key -rw------- 1 root root 1679 Apr 26 22:47 apiserver.key -rw-r--r-- 1 root root 1099 Apr 26 22:47 apiserver-kubelet-client.crt -rw------- 1 root root 1679 Apr 26 22:47 apiserver-kubelet-client.key -rw------- 1 root root 38 Aug 3 2020 basic_auth_file -rw-r--r-- 1 root root 1025 Apr 23 2019 ca.crt -rw------- 1 root root 1675 Apr 23 2019 ca.key drwxr-xr-x 2 root root 4096 Apr 22 2020 etcd -rw-r--r-- 1 root root 1038 Apr 23 2019 front-proxy-ca.crt -rw------- 1 root root 1675 Apr 23 2019 front-proxy-ca.key -rw-r--r-- 1 root root 1058 Apr 26 22:47 front-proxy-client.crt -rw------- 1 root root 1679 Apr 26 22:47 front-proxy-client.key -rw------- 1 root root 1679 Apr 23 2019 sa.key -rw------- 1 root root 451 Apr 23 2019 sa.pub 第二步是用新的 cert 生成 /etc/kubernetes 目录下的配置文件。\n/etc/kubernetes$ ls -al drwxr-xr-x 4 root root 4096 Apr 27 01:26 . drwxr-xr-x 97 root root 4096 Aug 20 2021 .. -rw------- 1 root root 5451 Apr 26 23:00 admin.conf -rw------- 1 root root 5451 Apr 23 2021 admin.conf.old -rw------- 1 root root 5487 Apr 26 23:00 controller-manager.conf -rw------- 1 root root 5487 Apr 23 2021 controller-manager.conf.old -rw------- 1 root root 5459 Apr 26 23:00 kubelet.conf -rw------- 1 root root 5459 Apr 26 22:47 kubelet.conf.old drwxr-xr-x 2 root root 4096 Aug 3 2020 manifests drwxr-xr-x 3 root root 4096 Aug 3 2020 pki -rw------- 1 root root 5431 Apr 26 23:00 scheduler.conf -rw------- 1 root root 5435 Apr 23 2021 scheduler.conf.old 注意，在运行第二条命令前，需要把所有 .config 文件删除或者是 rename，如果保留 .config 文件的话，这些文件不会被更新，所以导致 kubelet 启动读取配置文件后，仍然是过期的 cert，因此 systemctl 无法启动这个服务，错误内容在第一节里面已经展示了。\n这一点如果错了，很难发现错在哪。 如果上网搜索 kubelet error code 255，结果都是告诉你需要更新 cert，但是明明已经更新了cert 了啊，为什么还不行呢？ 然后你再更新一遍 cert，会发现还是没用，因为 config file 没有更新。\n重启 Docker 完成上面 2 步后，kubelet 还是无法启动，这就很奇怪了，琢磨了半天，我想到重启下 docker 试试？ 在重启之前，我还用 docker container prune 命令把旧的 container 都删了\n$ systemctl restart docker 果然，之后用 docker ps 查看，各种 container 都运行起来了。\n不知道这个问题有没有共性，还是只是我遇到的特例，总之，kubelet 服务有问题的时候，也可以尝试重启下 docker。\nPod 没有正常运行 到这里，用 kubectl 查看各个 namespace，pod 基本都显示 running 状态，我以为集群基本正常了。\n这时，我看到有一个 mysql 的 pod 处于 CrashBackoff 的状态，按照以往的经验，mysql 属于有状态应用，可能因为 node 重启产生了一些错误，之前我的解决方法就是删除这个 pod，然后 k8s 会自动起一个新的。\n于是故技重施，我删除了这个 pod，等待 k8s 新建一个，可是等了又等，迟迟不见 k8s 创建新的 Pod。\n用 kubectl 查看 deployment 或者 ReplicaSet 的状态，看起来都正常，\n$ kubectl -n kubeflow describe deployment katib-mysql #..... 省略 Replicas: 1 desired | 1 updated | 1 total | 0 available | 0 unavailable #..... 省略 Conditions: Type Status Reason ---- ------ ------ Available True 某某原因（我没有记下来） Progressing False 某某原因（我没有记下来） 上面这个内容是经过我编辑的，原始的状态信息我没有记下来，总之，此时的情况是：\ndesired/updated 这些都是 1，唯独 available 是 0，也就是说 k8s 迟迟不创建一个新的 pod Available/Progressing 也显示了一些 reason，但是还是没告诉我哪里有问题 对于这个问题，我也是琢磨了好久，始终不明白： cert 也更新了，集群状态大部分也都在 running 了，为啥唯独个别 pod 无法创建呢？\n然后，我有删除了一个 running 的 pod，看看正常的 pod 能否自动重建，发现也不可以。\n最终，我想到看看 k8s apiserver 和 scheduler 的 log， 一看，发现好像还是有很多 Unauthorized, connection error, cert error 之类的错，但是明明 cert 已经更新了啊 ？\n这就回到了第二小节的问题，不单单是 kubelet.conf 的 config 要删除才能重建，其他所有的 config 都要删除。 于是问题解决。\n所以，这个问题给我的思路就是，如果发现 k8s 在创建 Pod 方面有问题，首先要想到的是 API server，scheduler，kubelet 是否正常。 因为这几个组件与 pod 的生命周期息息相关。\nImage 不是原来的版本 这个问题很奇怪，至今没想明白是谁动了我的 image tag ：-）\n这次的问题还是某个 mysql pod 无法启动，查看它的 log\n$ kubectl -n kubeflow logs -f katib-mysql-57884cb488-qjdct 2022-04-27 08:45:03+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.25-1debian10 started. 2022-04-27 08:45:03+00:00 [Note] [Entrypoint]: Switching to dedicated user 'mysql' 2022-04-27 08:45:03+00:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.25-1debian10 started. 2022-04-27T08:45:03.559461Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.25) starting as process 1 2022-04-27T08:45:03.580431Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started. 2022-04-27T08:45:04.193878Z 1 [ERROR] [MY-013171] [InnoDB] Cannot boot server version 80025 on data directory built by version 80028. Downgrade is not supported mysqld: Can't open file: 'mysql.ibd' (errno: 0 - ) 2022-04-27T08:45:09.194257Z 1 [ERROR] [MY-010334] [Server] Failed to initialize DD Storage Engine 2022-04-27T08:45:09.194591Z 0 [ERROR] [MY-010020] [Server] Data Dictionary initialization failed. 2022-04-27T08:45:09.195342Z 0 [ERROR] [MY-010119] [Server] Aborting 2022-04-27T08:45:09.196992Z 0 [System] [MY-010910] [Server] /usr/sbin/mysqld: Shutdown complete (mysqld 8.0.25) MySQL Community Server - GPL. 问题似乎很清楚： “Cannot boot server version 80025 on data directory built by version 80028”。\n但是我明明运行的是 docker container 啊，同样的 image 为什么会存在版本不同的问题呢？ 要知道，docker 本质上解决的就是版本不一致的问题。\n不过既然错误这么说了，我就看了下 deployment 信息，image 版本是这样指定的 mysql:8。\n注意，此处是 mysql:8, 我又去 dockerhub 上看了下 mysql:8 发现最近有更新，同时，也有 error 里面提到的 mysql:8.0.28。\n于是，我把 deployment 中 image 替换成了 mysql:8.0.28，问题解决。\n但是，始终没有理解为什么会出现这样的问题，按理说一个 image:tag 组合就唯一定义了一个版本，不可能出现一个是 8.0.25, 另一个是 8.0.28 的问题。\n$ docker image ls | grep mysql mysql 8.0.28 f2ad9f23df82 6 days ago 521MB mysql 5.6 dd3b2a5dcb48 4 months ago 303MB mysql 8 5c62e459e087 10 months ago 556MB mysql 8.0.3 00400babc1b7 4 years ago 343M 好吧，这个问题始终没想明白。\n全文完。\n","wordCount":"893","inLanguage":"en","image":"https://runzhen.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2022-04-27T00:00:00Z","dateModified":"2022-04-27T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://runzhen.github.io/posts/k8s-debug-stories/"},"publisher":{"@type":"Organization","name":"Mind in the Wind","logo":{"@type":"ImageObject","url":"https://runzhen.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://runzhen.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://runzhen.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://runzhen.github.io/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://runzhen.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://runzhen.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://runzhen.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">使用 Kubernetes 遇到的一些问题和解决思路</h1><div class=post-meta><span title='2022-04-27 00:00:00 +0000 UTC'>2022-04-27</span>&nbsp;·&nbsp;Me</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#node-%e9%87%8d%e5%90%af%e5%90%8e-kubelet-%e6%b2%a1%e8%bf%90%e8%a1%8c aria-label="Node 重启后 kubelet 没运行">Node 重启后 kubelet 没运行</a></li><li><a href=#%e6%9b%b4%e6%96%b0-cert-%e9%9c%80%e8%a6%81%e5%88%a0%e9%99%a4%e5%8e%9f%e6%9d%a5%e7%9a%84%e6%96%87%e4%bb%b6 aria-label="更新 cert 需要删除原来的文件">更新 cert 需要删除原来的文件</a></li><li><a href=#%e9%87%8d%e5%90%af-docker aria-label="重启 Docker">重启 Docker</a></li><li><a href=#pod-%e6%b2%a1%e6%9c%89%e6%ad%a3%e5%b8%b8%e8%bf%90%e8%a1%8c aria-label="Pod 没有正常运行">Pod 没有正常运行</a></li><li><a href=#image-%e4%b8%8d%e6%98%af%e5%8e%9f%e6%9d%a5%e7%9a%84%e7%89%88%e6%9c%ac aria-label="Image 不是原来的版本">Image 不是原来的版本</a></li></ul></div></details></div><div class=post-content><p>update on 2022-05-21</p><p>今天在 homelab 的 k8s 集群上发生了同样的情况，我想删除一个 namespace，再确认已经把 namespace 里面所有其他资源都删除的情况下，namespace 始终是 Terminating, 找了很多资料，方法也<a href=https://github.com/kubernetes/kubernetes/issues/60807#issuecomment-524772920>众说纷纭</a> 。</p><p>最后通过看 <code>api-server</code> log 发现原来又是 Unable to authenticate the request due to an error: x509: certificate has expired or is not yet valid</p><p>root cause 还是我更新 cert 的时候又漏了某些步骤。</p><hr><p>事情的起因是 k8s 的 cert 过期了，在目录 <code>/etc/kubernetes/pki/</code> 下面的这些 cert 都与 k8s 的核心服务息息相关，因此 cert 过期了，整个 k8s 集群就停止服务了。</p><p>这个集群是 kubernetes 1.14, 因此需要运行几个命令完成更新，而 1.15 版本以上这个过程简化了不少。 由于之前已经 renew cert 两次了，因此正常按部就班几个操作就完事了，但是这个因为一点小疏忽，加上系统死机重启了一次，花了很多时间去恢复各种服务。</p><p>本文记录 debug 的过程中遇到的一些症状，以及后来发现的解决方法，为以后遇到类似问题提供思路。</p><h2 id=node-重启后-kubelet-没运行>Node 重启后 kubelet 没运行<a hidden class=anchor aria-hidden=true href=#node-重启后-kubelet-没运行>#</a></h2><p>前面提到，可能是因为 cert 过期后触发某些 bug 导致 Master Node 不能 ssh（之前 renew cert 没有类似问题），所以只能去机房按电源开关重启了。</p><p>重启之后，发现 kubelet 没有运行，于是尝试 <code>systemctl restart kubelet</code>, 发现没有效果，那就看看 status</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ systemctl status kubelet.service
</span></span><span class=line><span class=cl>● kubelet.service - kubelet: The Kubernetes Node Agent
</span></span><span class=line><span class=cl>   Loaded: loaded <span class=o>(</span>/lib/systemd/system/kubelet.service<span class=p>;</span> enabled<span class=p>;</span> vendor preset: enabled<span class=o>)</span>
</span></span><span class=line><span class=cl>  Drop-In: /etc/systemd/system/kubelet.service.d
</span></span><span class=line><span class=cl>           └─10-kubeadm.conf
</span></span><span class=line><span class=cl>   Active: activating <span class=o>(</span>auto-restart<span class=o>)</span> <span class=o>(</span>Result: exit-code<span class=o>)</span> since Tue 2022-04-26 22:09:42 PDT<span class=p>;</span> 4s ago
</span></span><span class=line><span class=cl>     Docs: https://kubernetes.io/docs/home/
</span></span><span class=line><span class=cl>  Process: <span class=m>18121</span> <span class=nv>ExecStart</span><span class=o>=</span>/usr/bin/kubelet <span class=nv>$KUBELET_KUBECONFIG_ARGS</span> <span class=nv>$KUBELET_CONFIG_ARGS</span> <span class=nv>$KUBELET_KUBEADM_ARGS</span> <span class=nv>$KUBELET_EXTRA_ARGS</span> <span class=o>(</span><span class=nv>code</span><span class=o>=</span>exited, <span class=nv>status</span><span class=o>=</span>255<span class=o>)</span>
</span></span><span class=line><span class=cl> Main PID: <span class=m>18121</span> <span class=o>(</span><span class=nv>code</span><span class=o>=</span>exited, <span class=nv>status</span><span class=o>=</span>255<span class=o>)</span>
</span></span></code></pre></div><p>一搜索，发现结果都是告诉你 cert 过期了，一切似乎很简单，我已经 renew 过几次 cert 了，操作起来轻车熟路。 正因为轻车熟路，所以忽略了要删除原来的旧文件，导致更新 cert 后 kubelet.service 仍然无法启动。</p><p>这就要说到下面这个坑了。</p><h2 id=更新-cert-需要删除原来的文件>更新 cert 需要删除原来的文件<a hidden class=anchor aria-hidden=true href=#更新-cert-需要删除原来的文件>#</a></h2><p>k8s cert 过期后如何 renew？ 网上有很多文章不再赘述，针对 1.14 版本，简而言之是运行下面几个命令</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>1. kubeadm alpha certs renew all
</span></span><span class=line><span class=cl>2. kubeadm init phase kubeconfig all --apiserver-advertise-address<span class=o>=</span>192.168.31.50
</span></span><span class=line><span class=cl>3. cp /etc/kubernetes/admin.conf ./kube/config
</span></span><span class=line><span class=cl>4. systemctl restart kubelet
</span></span></code></pre></div><p>其中第一步运行完之后，<code>/etc/kubernetes/pki</code> 目录下除了 CA 以外，所有的 cert 都被更新了，这一步很简单不需要特殊注意什么。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>/etc/kubernetes/pki$ ls -al
</span></span><span class=line><span class=cl>total <span class=m>72</span>
</span></span><span class=line><span class=cl>drwxr-xr-x <span class=m>3</span> root root <span class=m>4096</span> Aug  <span class=m>3</span>  <span class=m>2020</span> .
</span></span><span class=line><span class=cl>drwxr-xr-x <span class=m>4</span> root root <span class=m>4096</span> Apr <span class=m>27</span> 01:26 ..
</span></span><span class=line><span class=cl>-rw-r--r-- <span class=m>1</span> root root <span class=m>1216</span> Apr <span class=m>26</span> 22:47 apiserver.crt
</span></span><span class=line><span class=cl>-rw-r--r-- <span class=m>1</span> root root <span class=m>1090</span> Apr <span class=m>26</span> 22:47 apiserver-etcd-client.crt
</span></span><span class=line><span class=cl>-rw------- <span class=m>1</span> root root <span class=m>1679</span> Apr <span class=m>26</span> 22:47 apiserver-etcd-client.key
</span></span><span class=line><span class=cl>-rw------- <span class=m>1</span> root root <span class=m>1679</span> Apr <span class=m>26</span> 22:47 apiserver.key
</span></span><span class=line><span class=cl>-rw-r--r-- <span class=m>1</span> root root <span class=m>1099</span> Apr <span class=m>26</span> 22:47 apiserver-kubelet-client.crt
</span></span><span class=line><span class=cl>-rw------- <span class=m>1</span> root root <span class=m>1679</span> Apr <span class=m>26</span> 22:47 apiserver-kubelet-client.key
</span></span><span class=line><span class=cl>-rw------- <span class=m>1</span> root root   <span class=m>38</span> Aug  <span class=m>3</span>  <span class=m>2020</span> basic_auth_file
</span></span><span class=line><span class=cl>-rw-r--r-- <span class=m>1</span> root root <span class=m>1025</span> Apr <span class=m>23</span>  <span class=m>2019</span> ca.crt
</span></span><span class=line><span class=cl>-rw------- <span class=m>1</span> root root <span class=m>1675</span> Apr <span class=m>23</span>  <span class=m>2019</span> ca.key
</span></span><span class=line><span class=cl>drwxr-xr-x <span class=m>2</span> root root <span class=m>4096</span> Apr <span class=m>22</span>  <span class=m>2020</span> etcd
</span></span><span class=line><span class=cl>-rw-r--r-- <span class=m>1</span> root root <span class=m>1038</span> Apr <span class=m>23</span>  <span class=m>2019</span> front-proxy-ca.crt
</span></span><span class=line><span class=cl>-rw------- <span class=m>1</span> root root <span class=m>1675</span> Apr <span class=m>23</span>  <span class=m>2019</span> front-proxy-ca.key
</span></span><span class=line><span class=cl>-rw-r--r-- <span class=m>1</span> root root <span class=m>1058</span> Apr <span class=m>26</span> 22:47 front-proxy-client.crt
</span></span><span class=line><span class=cl>-rw------- <span class=m>1</span> root root <span class=m>1679</span> Apr <span class=m>26</span> 22:47 front-proxy-client.key
</span></span><span class=line><span class=cl>-rw------- <span class=m>1</span> root root <span class=m>1679</span> Apr <span class=m>23</span>  <span class=m>2019</span> sa.key
</span></span><span class=line><span class=cl>-rw------- <span class=m>1</span> root root  <span class=m>451</span> Apr <span class=m>23</span>  <span class=m>2019</span> sa.pub
</span></span></code></pre></div><p>第二步是用新的 cert 生成 <code>/etc/kubernetes</code> 目录下的配置文件。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>/etc/kubernetes$ ls -al
</span></span><span class=line><span class=cl>drwxr-xr-x  <span class=m>4</span> root root <span class=m>4096</span> Apr <span class=m>27</span> 01:26 .
</span></span><span class=line><span class=cl>drwxr-xr-x <span class=m>97</span> root root <span class=m>4096</span> Aug <span class=m>20</span>  <span class=m>2021</span> ..
</span></span><span class=line><span class=cl>-rw-------  <span class=m>1</span> root root <span class=m>5451</span> Apr <span class=m>26</span> 23:00 admin.conf
</span></span><span class=line><span class=cl>-rw-------  <span class=m>1</span> root root <span class=m>5451</span> Apr <span class=m>23</span>  <span class=m>2021</span> admin.conf.old
</span></span><span class=line><span class=cl>-rw-------  <span class=m>1</span> root root <span class=m>5487</span> Apr <span class=m>26</span> 23:00 controller-manager.conf
</span></span><span class=line><span class=cl>-rw-------  <span class=m>1</span> root root <span class=m>5487</span> Apr <span class=m>23</span>  <span class=m>2021</span> controller-manager.conf.old
</span></span><span class=line><span class=cl>-rw-------  <span class=m>1</span> root root <span class=m>5459</span> Apr <span class=m>26</span> 23:00 kubelet.conf
</span></span><span class=line><span class=cl>-rw-------  <span class=m>1</span> root root <span class=m>5459</span> Apr <span class=m>26</span> 22:47 kubelet.conf.old
</span></span><span class=line><span class=cl>drwxr-xr-x  <span class=m>2</span> root root <span class=m>4096</span> Aug  <span class=m>3</span>  <span class=m>2020</span> manifests
</span></span><span class=line><span class=cl>drwxr-xr-x  <span class=m>3</span> root root <span class=m>4096</span> Aug  <span class=m>3</span>  <span class=m>2020</span> pki
</span></span><span class=line><span class=cl>-rw-------  <span class=m>1</span> root root <span class=m>5431</span> Apr <span class=m>26</span> 23:00 scheduler.conf
</span></span><span class=line><span class=cl>-rw-------  <span class=m>1</span> root root <span class=m>5435</span> Apr <span class=m>23</span>  <span class=m>2021</span> scheduler.conf.old
</span></span></code></pre></div><p><strong>注意</strong>，在运行第二条命令前，需要把所有 <code>.config</code> 文件删除或者是 rename，如果保留 .config 文件的话，这些文件不会被更新，所以导致 kubelet 启动读取配置文件后，仍然是过期的 cert，因此 systemctl 无法启动这个服务，错误内容在第一节里面已经展示了。</p><p>这一点如果错了，很难发现错在哪。 如果上网搜索 <code>kubelet error code 255</code>，结果都是告诉你需要更新 cert，但是明明已经更新了cert 了啊，为什么还不行呢？ 然后你再更新一遍 cert，会发现还是没用，因为 config file 没有更新。</p><h2 id=重启-docker>重启 Docker<a hidden class=anchor aria-hidden=true href=#重启-docker>#</a></h2><p>完成上面 2 步后，kubelet 还是无法启动，这就很奇怪了，琢磨了半天，我想到重启下 docker 试试？ 在重启之前，我还用 docker container prune 命令把旧的 container 都删了</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ systemctl restart docker
</span></span></code></pre></div><p>果然，之后用 docker ps 查看，各种 container 都运行起来了。</p><p>不知道这个问题有没有共性，还是只是我遇到的特例，总之，kubelet 服务有问题的时候，也可以尝试重启下 docker。</p><h2 id=pod-没有正常运行>Pod 没有正常运行<a hidden class=anchor aria-hidden=true href=#pod-没有正常运行>#</a></h2><p>到这里，用 kubectl 查看各个 namespace，pod 基本都显示 running 状态，我以为集群基本正常了。</p><p>这时，我看到有一个 mysql 的 pod 处于 CrashBackoff 的状态，按照以往的经验，mysql 属于有状态应用，可能因为 node 重启产生了一些错误，之前我的解决方法就是删除这个 pod，然后 k8s 会自动起一个新的。</p><p>于是故技重施，我删除了这个 pod，等待 k8s 新建一个，可是等了又等，迟迟不见 k8s 创建新的 Pod。</p><p>用 kubectl 查看 deployment 或者 ReplicaSet 的状态，看起来都正常，</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ kubectl -n kubeflow describe deployment katib-mysql
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#..... 省略 </span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Replicas:               <span class=m>1</span> desired <span class=p>|</span> <span class=m>1</span> updated <span class=p>|</span> <span class=m>1</span> total <span class=p>|</span> <span class=m>0</span> available <span class=p>|</span> <span class=m>0</span> unavailable
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#..... 省略 </span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Conditions:
</span></span><span class=line><span class=cl>  Type           Status  Reason
</span></span><span class=line><span class=cl>  ----           ------  ------
</span></span><span class=line><span class=cl>  Available      True    某某原因（我没有记下来）
</span></span><span class=line><span class=cl>  Progressing    False    某某原因（我没有记下来）
</span></span></code></pre></div><p>上面这个内容是经过我编辑的，原始的状态信息我没有记下来，总之，此时的情况是：</p><ol><li>desired/updated 这些都是 1，唯独 available 是 0，也就是说 k8s 迟迟不创建一个新的 pod</li><li>Available/Progressing 也显示了一些 reason，但是还是没告诉我哪里有问题</li></ol><p>对于这个问题，我也是琢磨了好久，始终不明白： cert 也更新了，集群状态大部分也都在 running 了，为啥唯独个别 pod 无法创建呢？</p><p>然后，我有删除了一个 running 的 pod，看看正常的 pod 能否自动重建，发现也不可以。</p><p>最终，我想到看看 k8s apiserver 和 scheduler 的 log， 一看，发现好像还是有很多 <code>Unauthorized</code>, <code>connection error</code>, <code>cert error</code> 之类的错，但是明明 cert 已经更新了啊 ？</p><p>这就回到了第二小节的问题，不单单是 kubelet.conf 的 config 要删除才能重建，其他所有的 config 都要删除。 于是问题解决。</p><p>所以，这个问题给我的思路就是，如果发现 k8s 在创建 Pod 方面有问题，<strong>首先要想到的是 API server，scheduler，kubelet 是否正常</strong>。 因为这几个组件与 pod 的生命周期息息相关。</p><h2 id=image-不是原来的版本>Image 不是原来的版本<a hidden class=anchor aria-hidden=true href=#image-不是原来的版本>#</a></h2><p>这个问题很奇怪，至今没想明白是谁动了我的 image tag ：-）</p><p>这次的问题还是某个 mysql pod 无法启动，查看它的 log</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ kubectl -n kubeflow logs -f katib-mysql-57884cb488-qjdct
</span></span><span class=line><span class=cl>2022-04-27 08:45:03+00:00 <span class=o>[</span>Note<span class=o>]</span> <span class=o>[</span>Entrypoint<span class=o>]</span>: Entrypoint script <span class=k>for</span> MySQL Server 8.0.25-1debian10 started.
</span></span><span class=line><span class=cl>2022-04-27 08:45:03+00:00 <span class=o>[</span>Note<span class=o>]</span> <span class=o>[</span>Entrypoint<span class=o>]</span>: Switching to dedicated user <span class=s1>&#39;mysql&#39;</span>
</span></span><span class=line><span class=cl>2022-04-27 08:45:03+00:00 <span class=o>[</span>Note<span class=o>]</span> <span class=o>[</span>Entrypoint<span class=o>]</span>: Entrypoint script <span class=k>for</span> MySQL Server 8.0.25-1debian10 started.
</span></span><span class=line><span class=cl>2022-04-27T08:45:03.559461Z <span class=m>0</span> <span class=o>[</span>System<span class=o>]</span> <span class=o>[</span>MY-010116<span class=o>]</span> <span class=o>[</span>Server<span class=o>]</span> /usr/sbin/mysqld <span class=o>(</span>mysqld 8.0.25<span class=o>)</span> starting as process <span class=m>1</span>
</span></span><span class=line><span class=cl>2022-04-27T08:45:03.580431Z <span class=m>1</span> <span class=o>[</span>System<span class=o>]</span> <span class=o>[</span>MY-013576<span class=o>]</span> <span class=o>[</span>InnoDB<span class=o>]</span> InnoDB initialization has started.
</span></span><span class=line><span class=cl>2022-04-27T08:45:04.193878Z <span class=m>1</span> <span class=o>[</span>ERROR<span class=o>]</span> <span class=o>[</span>MY-013171<span class=o>]</span> <span class=o>[</span>InnoDB<span class=o>]</span> Cannot boot server version <span class=m>80025</span> on data directory built by version 80028. Downgrade is not supported
</span></span><span class=line><span class=cl>mysqld: Can<span class=s1>&#39;t open file: &#39;</span>mysql.ibd<span class=err>&#39;</span> <span class=o>(</span>errno: <span class=m>0</span> - <span class=o>)</span>
</span></span><span class=line><span class=cl>2022-04-27T08:45:09.194257Z <span class=m>1</span> <span class=o>[</span>ERROR<span class=o>]</span> <span class=o>[</span>MY-010334<span class=o>]</span> <span class=o>[</span>Server<span class=o>]</span> Failed to initialize DD Storage Engine
</span></span><span class=line><span class=cl>2022-04-27T08:45:09.194591Z <span class=m>0</span> <span class=o>[</span>ERROR<span class=o>]</span> <span class=o>[</span>MY-010020<span class=o>]</span> <span class=o>[</span>Server<span class=o>]</span> Data Dictionary initialization failed.
</span></span><span class=line><span class=cl>2022-04-27T08:45:09.195342Z <span class=m>0</span> <span class=o>[</span>ERROR<span class=o>]</span> <span class=o>[</span>MY-010119<span class=o>]</span> <span class=o>[</span>Server<span class=o>]</span> Aborting
</span></span><span class=line><span class=cl>2022-04-27T08:45:09.196992Z <span class=m>0</span> <span class=o>[</span>System<span class=o>]</span> <span class=o>[</span>MY-010910<span class=o>]</span> <span class=o>[</span>Server<span class=o>]</span> /usr/sbin/mysqld: Shutdown <span class=nb>complete</span> <span class=o>(</span>mysqld 8.0.25<span class=o>)</span>  MySQL Community Server - GPL.
</span></span></code></pre></div><p>问题似乎很清楚： “Cannot boot server version 80025 on data directory built by version 80028”。</p><p>但是我明明运行的是 docker container 啊，同样的 image 为什么会存在版本不同的问题呢？ 要知道，docker 本质上解决的就是版本不一致的问题。</p><p>不过既然错误这么说了，我就看了下 deployment 信息，image 版本是这样指定的 <code>mysql:8</code>。</p><p>注意，此处是 <code>mysql:8</code>, 我又去 dockerhub 上看了下 <code>mysql:8</code> 发现最近有更新，同时，也有 error 里面提到的 <code>mysql:8.0.28</code>。</p><p>于是，我把 deployment 中 image 替换成了 <code>mysql:8.0.28</code>，问题解决。</p><p>但是，始终没有理解为什么会出现这样的问题，按理说一个 <code>image:tag</code> 组合就唯一定义了一个版本，不可能出现一个是 8.0.25, 另一个是 8.0.28 的问题。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>$ docker image ls  <span class=p>|</span> grep mysql
</span></span><span class=line><span class=cl>mysql          8.0.28                                     f2ad9f23df82        <span class=m>6</span> days ago          521MB
</span></span><span class=line><span class=cl>mysql          5.6                                        dd3b2a5dcb48        <span class=m>4</span> months ago        303MB
</span></span><span class=line><span class=cl>mysql          <span class=m>8</span>                                          5c62e459e087        <span class=m>10</span> months ago       556MB
</span></span><span class=line><span class=cl>mysql          8.0.3                                      00400babc1b7        <span class=m>4</span> years ago         343M
</span></span></code></pre></div><p>好吧，这个问题始终没想明白。</p><p>全文完。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://runzhen.github.io/tags/linux/>Linux</a></li><li><a href=https://runzhen.github.io/tags/k8s/>K8s</a></li></ul><nav class=paginav><a class=prev href=https://runzhen.github.io/posts/go-redis-latency-debug/><span class=title>« Prev</span><br><span>记一次 go-redis 的 debug 过程</span>
</a><a class=next href=https://runzhen.github.io/posts/how-does-grpc-client-reconnect-tcp/><span class=title>Next »</span><br><span>gRPC client 如何实现 TCP 重连</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 使用 Kubernetes 遇到的一些问题和解决思路 on x" href="https://x.com/intent/tweet/?text=%e4%bd%bf%e7%94%a8%20Kubernetes%20%e9%81%87%e5%88%b0%e7%9a%84%e4%b8%80%e4%ba%9b%e9%97%ae%e9%a2%98%e5%92%8c%e8%a7%a3%e5%86%b3%e6%80%9d%e8%b7%af&amp;url=https%3a%2f%2frunzhen.github.io%2fposts%2fk8s-debug-stories%2f&amp;hashtags=linux%2ck8s"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 使用 Kubernetes 遇到的一些问题和解决思路 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frunzhen.github.io%2fposts%2fk8s-debug-stories%2f&amp;title=%e4%bd%bf%e7%94%a8%20Kubernetes%20%e9%81%87%e5%88%b0%e7%9a%84%e4%b8%80%e4%ba%9b%e9%97%ae%e9%a2%98%e5%92%8c%e8%a7%a3%e5%86%b3%e6%80%9d%e8%b7%af&amp;summary=%e4%bd%bf%e7%94%a8%20Kubernetes%20%e9%81%87%e5%88%b0%e7%9a%84%e4%b8%80%e4%ba%9b%e9%97%ae%e9%a2%98%e5%92%8c%e8%a7%a3%e5%86%b3%e6%80%9d%e8%b7%af&amp;source=https%3a%2f%2frunzhen.github.io%2fposts%2fk8s-debug-stories%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 使用 Kubernetes 遇到的一些问题和解决思路 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2frunzhen.github.io%2fposts%2fk8s-debug-stories%2f&title=%e4%bd%bf%e7%94%a8%20Kubernetes%20%e9%81%87%e5%88%b0%e7%9a%84%e4%b8%80%e4%ba%9b%e9%97%ae%e9%a2%98%e5%92%8c%e8%a7%a3%e5%86%b3%e6%80%9d%e8%b7%af"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 使用 Kubernetes 遇到的一些问题和解决思路 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frunzhen.github.io%2fposts%2fk8s-debug-stories%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 使用 Kubernetes 遇到的一些问题和解决思路 on whatsapp" href="https://api.whatsapp.com/send?text=%e4%bd%bf%e7%94%a8%20Kubernetes%20%e9%81%87%e5%88%b0%e7%9a%84%e4%b8%80%e4%ba%9b%e9%97%ae%e9%a2%98%e5%92%8c%e8%a7%a3%e5%86%b3%e6%80%9d%e8%b7%af%20-%20https%3a%2f%2frunzhen.github.io%2fposts%2fk8s-debug-stories%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 使用 Kubernetes 遇到的一些问题和解决思路 on telegram" href="https://telegram.me/share/url?text=%e4%bd%bf%e7%94%a8%20Kubernetes%20%e9%81%87%e5%88%b0%e7%9a%84%e4%b8%80%e4%ba%9b%e9%97%ae%e9%a2%98%e5%92%8c%e8%a7%a3%e5%86%b3%e6%80%9d%e8%b7%af&amp;url=https%3a%2f%2frunzhen.github.io%2fposts%2fk8s-debug-stories%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 使用 Kubernetes 遇到的一些问题和解决思路 on ycombinator" href="https://news.ycombinator.com/submitlink?t=%e4%bd%bf%e7%94%a8%20Kubernetes%20%e9%81%87%e5%88%b0%e7%9a%84%e4%b8%80%e4%ba%9b%e9%97%ae%e9%a2%98%e5%92%8c%e8%a7%a3%e5%86%b3%e6%80%9d%e8%b7%af&u=https%3a%2f%2frunzhen.github.io%2fposts%2fk8s-debug-stories%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://runzhen.github.io/>Mind in the Wind</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>