<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Blog on Mind in the Wind</title>
    <link>https://runzhen.github.io/posts/</link>
    <description>Recent content in Blog on Mind in the Wind</description>
    <image>
      <title>Mind in the Wind</title>
      <url>https://runzhen.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://runzhen.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.125.1</generator>
    <language>en</language>
    <lastBuildDate>Fri, 02 Feb 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://runzhen.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Distributed Rate Limiting</title>
      <link>https://runzhen.github.io/posts/distributed-rate-limit/</link>
      <pubDate>Fri, 02 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/distributed-rate-limit/</guid>
      <description>限流算法 通常有下面4种：
固定时间窗口(计数器)算法 基本思想是：在固定时间窗口内对请求数进行统计，然后与阈值比较确定是否进行限流，一旦到了时间临界点，就将计数器清零 缺陷： 可能存在在某个时间窗口前90%时间里没有请求，所有的请求都集中在最后10%，这个在该算法中是允许的 滑动时间窗口算法 基本思想是：一个较大的时间窗口内细分成多个小窗口，大窗口按照时间顺序每次向后移动一个小窗口，并保证每次大窗口内的请求总数不超过阈值。 缺陷：滑动窗口是对固定窗口算法的一种改进，但是并没有真正解决固定窗口的临界突发瞬时大流量问题。 漏桶算法 Leaky Bucket 基本思想是：漏桶算法通过一个固定容量的桶，控制进入桶中的请求总数，然后以一定速率从桶中取出请求进行处理，如果桶已经满了，则直接丢弃请求。 缺陷： 漏桶算法因为是先进先出队列，在突发瞬时大流量情况下，会出现大量请求失败情况，不适合抢购，热点事件等场景 适用场景：就像漏斗一样，出口处的速率是恒定的。因此漏桶算法是流量最均衡的限流算法，用于对流量进行整型，保证流量以固定的速率进入系统。 令牌桶算法 基本思想是：令牌桶相当于反向漏桶算法，即以固定速率生成令牌放入固定容量的桶中，每个请求从桶中获取到令牌就允许执行，没有获取到就丢弃。 令牌桶算法弥补了漏桶算法无法应对突发大流量问题，即可以针对突发大流量进行限流。 单机 ratelimit 参考资料 1 里面有上面四种算法的实现，这里仅列举一下固定窗口法和漏桶算法。
固定窗口算法
type FixedWindowRateLimiter struct { threshold int // 阈值 stime time.Time // 开始时间 interval time.Duration // 时间窗口 counter int // 当前计数 lock sync.Mutex } func NewFixedWindowRateLimiter(threshold int, interval time.Duration) *FixedWindowRateLimiter { return &amp;amp;FixedWindowRateLimiter{ threshold: threshold, stime: time.Now(), interval: interval, counter: threshold - 1, // 让其处于下一个时间窗口开始的时间临界点 } } func (l *FixedWindowRateLimiter) Allow() bool { l.</description>
    </item>
    <item>
      <title>Set GOMAXPROCS Properly in Go Program</title>
      <link>https://runzhen.github.io/posts/go-maxprocs/</link>
      <pubDate>Sun, 02 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/go-maxprocs/</guid>
      <description>从 cgroup 的介绍中，我们知道了通过设置 /sys/fs/cgroup/ 的值，并且使用 cgroup-tools 启动程序同时指定一个 cgroup，可以达到控制进程使用系统资源的目的。
起因 一个 Go 程序运行在 k8s 环境中，在某一行代码前后设置 start timestamp 和 end timestamp，发现有时候 p99 的 latency 非常高，正常情况下在 1-3 ms，极端情况下有 50-90 ms。百思不得其解，猜测各种可能加查阅资料后，发现应该是没有正确的设置 runtime.GOMAXPROCS。设置为 1 后，极高 latency 的情况明显减少。
为什么 出现这个问题有三个条件，缺一不可：
是 Go 程序，并且采用系统默认 GOMAXPROCS 运行在 k8s 或者 docker 这样的容器环境 宿主机上有多个 CPU 核 GOMAXPROCS 是什么 回忆一下 Go 并发的 GPM 模型：
G代表 goroutine，即用户创建的 goroutines P代表 Logical Processor，是类似于 CPU 核心的概念，其用来控制并发的 M 数量 M是操作系统线程。在绝大多数时候，P的数量和M的数量是相等的。每创建一个P, 就会创建一个对应的M 而 go 的 runtime GOMAXPROCS 代表的就是 P 的数量，其底层就是 runtime 直接调用 Linux 系统调用 sched_getaffinity()</description>
    </item>
    <item>
      <title>What is cgroups ?</title>
      <link>https://runzhen.github.io/posts/cgroups-intro/</link>
      <pubDate>Wed, 22 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/cgroups-intro/</guid>
      <description>cgroup 比较有趣的地方是它没有提供任何的系统调用接口，所以你不能用 API Call 的方式使用 cgroup，实际上 cgroup 实现了 linux 虚拟文件系统 vfs，所以类似我们熟悉的 btfrs, ext4， 因此可以用类似文件系统的方式进行操作。
比如用 mount 命令看一下 linux 上挂载了哪些设备：
# mount -t cgroup /dev/sda2 on / type ext4 (rw,relatime) cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,name=systemd) cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio) cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma) cgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate,memory_recursiveprot) 可以看到，
第一行是磁盘 sda2 挂载在根目录 /, 它的类型是 ext4 后面几行是 cgroup 挂载在了目录 /sys/fs/cgroup/，类型是 cgroup 如果你的内核比较新的话，将看不到上面那些 cgroup 的行，而是只能看到最后这一行 cgroup2，这是因为新版本的内核使用了 cgroup v2 。 另外类似于 “net_cls”， “rdma” 这些都是 cgroup 子系统的名字，详见本文结尾的附录。</description>
    </item>
    <item>
      <title>B&#43; Tree Implementation in boltdb</title>
      <link>https://runzhen.github.io/posts/boltdb-b-plus-tree/</link>
      <pubDate>Wed, 04 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/boltdb-b-plus-tree/</guid>
      <description>如何存储和表示 B+ 树 前面已经知道，page 是代表 B+ 树被序列化到了磁盘上的结构，一个 page 就是一个 B+ 树节点，通过 mmap 把磁盘上的页映射到内存，然后用 unsafe.Pointer(p) 直接把二进制序列化成 page 结构。也正因为如此，x86 架构上生成的 db 文件是不能在 ARM 架构的机器上打开的。
而 node 结构同样也表示内存中一个 B+ 树的节点，node 与 page 的区别是 node 按需创建的，对于不需要修改的B+树节点，boltdb直接从page中读取数据，当需要修改某个 B+ 树节点时，比如插入删除数据，boltdb 从 page 结构生成成 node 。（此处存疑，我觉得应该是 Cursor 在游走的过程中就会把 page 转化成 node）
Bucket.node() 函数中就有如下一段话
func (b *Bucket) node(pgid pgid, parent *node) *node { // Retrieve node if it&amp;#39;s already been created. if n := b.nodes[pgid]; n != nil { return n } // Read the page into the node and cache it.</description>
    </item>
    <item>
      <title>Play with Hackintosh!</title>
      <link>https://runzhen.github.io/posts/hackintosh/</link>
      <pubDate>Sat, 05 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/hackintosh/</guid>
      <description>主机配置
CPU：i7-9700k CPU 集显：Intel UHD 630 主板：MSI B365M Pro-VDH 黑苹果镜像来源于黑果小兵的 Monterey 12.6，下载链接 , 安装的过程就不赘述了，网上有很多资料。
安装之后最大的问题就是显卡只显示 14 MB 显存，意思就是 MacOS 没有正确的驱动 CPU 的集成显卡，需要改 config.plist 文件。
以上都是安装黑苹果大多数人都会遇到的问题，网上也有很多教程教你把 AAPL,ig-platform-id 的值改成 07009B3E，以及其他的一系列参数等等，但是问题是按照他们的参数设置后，显卡依然无法正常工作。
尝试了很多参数，遇到的问题主要是以下两种
显存依然是 14 MB，但其他正常。但是因为无法驱动导致显示性能所限，无法打开消耗GPU性能的应用，比如 Docker 无法启动，因为 Mac 版的 Docker 初试启动需要开启一个桌面，无法打开。VS Code 也无法使用。 Goland 可以使用。 显存显示正常 1560 MB，但是屏幕的色彩全变了，蓝色显示为橘黄，红色显示为蓝色。 这是困扰我最大的问题，尝试了几天都没有找到解决办法。 最后，通过这个 Youtube 视频介绍的方法 https://www.youtube.com/watch?v=4EU8oT0-Ea8 居然试验成功了！！
为什么成功了呢？ 我只做了以下几个操作，可能是其中一个，或者全部组合起了作用。
在 ACPI -&amp;gt; Patch 里面添加了 2 项。 NVRAM “7C436110” 那一项的 boot-args 参数里面加了 -cdfon 在 PlatformInfo SMBIOS 里面选择了 iMac19,1 注意：</description>
    </item>
    <item>
      <title>Go Assembly - 3</title>
      <link>https://runzhen.github.io/posts/go-asm3/</link>
      <pubDate>Wed, 26 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/go-asm3/</guid>
      <description>本文主要收集一些例子，以后阅读 Go 汇编时遇到忘记的指令可以查询。
例子 1 package main func main() { l := []int{9, 45, 23, 67, 78} t := 0 for _, v := range l { t += v } println(t) } 截取了一段汇编如下
0x0026 00038 (3.go:4) MOVUPS X15, &amp;#34;&amp;#34;..autotmp_5+40(SP) 0x002c 00044 (3.go:4) MOVUPS X15, &amp;#34;&amp;#34;..autotmp_5+48(SP) 0x0032 00050 (3.go:4) MOVUPS X15, &amp;#34;&amp;#34;..autotmp_5+64(SP) 0x0038 00056 (3.go:4) LEAQ &amp;#34;&amp;#34;..autotmp_5+40(SP), AX 0x003d 00061 (3.go:4) MOVQ AX, &amp;#34;&amp;#34;..autotmp_4+80(SP) 0x0042 00066 (3.go:4) TESTB AL, (AX) 其中</description>
    </item>
    <item>
      <title>Go Assembly - 2</title>
      <link>https://runzhen.github.io/posts/go-asm2/</link>
      <pubDate>Mon, 24 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/go-asm2/</guid>
      <description>本文翻译自 https://github.com/teh-cmc/go-internals/tree/master/chapter1_assembly_primer
先看一个简单的 code
// go tool compile -N -l -S once.go // go build -gcflags -S once.go package main //go:noinline func add(a, b int32) (int32, bool) { return a + b, true } func main() { add(10, 32) } 生成汇编
GOOS=linux GOARCH=amd64 go tool compile -S x.go 在我的机器 Ubuntu kernel 5.4.0, Go version go1.18.3 amd64 上出来的结果与原文中还是有些差异的，但为了文章通顺，下面还是用的原文的结果。
0x0000 TEXT	&amp;#34;&amp;#34;.add(SB), NOSPLIT, $0-16 0x0000 FUNCDATA	$0, gclocals·f207267fbf96a0178e8758c6e3e0ce28(SB) 0x0000 FUNCDATA	$1, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB) 0x0000 MOVL	&amp;#34;&amp;#34;.</description>
    </item>
    <item>
      <title>Go Assembly - 1</title>
      <link>https://runzhen.github.io/posts/go-asm1/</link>
      <pubDate>Sun, 23 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/go-asm1/</guid>
      <description>寄存器 学过 X86 汇编的同学都知道汇编有AX，BX等寄存器，除此之外，Go 还添加了 PC、FP、SP、SB四个伪寄存器。如下图所示，其中第二列为 GO 添加的4 个伪寄存器，第三列为 X86 寄存器。
看到这里，尘封已久的汇编语言知识需要拿出来复习一下。
FLAGS 是状态寄存器。 IP 是指令寄存器。 AX、BX、CX、DX、SI、DI、BP、SP 是通用寄存器。在X86-64中又增加了八个以R8-R15 方式命名的通用寄存器。 另外 GO 的 4 个伪寄存器作用如下：
FP: Frame pointer：伪FP寄存器对应函数的栈帧指针，一般用来访问函数的参数和返回值；golang语言中，函数的参数和返回值，函数中的局部变量，函数中调用子函数的参数和返回值都是存储在栈中的，我们把这一段栈内存称为栈帧（frame），伪FP寄存器对应栈帧的底部，但是伪FP只包括函数的参数和返回值这部分内存，其他部分由伪SP寄存器表示；注意golang中函数的返回值也是通过栈帧返回的，这也是golang函数可以有多个返回值的原因； PC: Program counter：指令计数器，用于分支和跳转，它是汇编的IP寄存器的别名； SB: Static base pointer：一般用于声明函数或者全局变量，对应代码区（text）内存段底部； SP: Stack pointer：指向当前栈帧的局部变量的开始位置，一般用来引用函数的局部变量，这里需要注意汇编中也有一个SP寄存器，它们的区别是：1.伪SP寄存器指向栈帧（不包括函数参数和返回值部分）的底部，真SP寄存器对应栈的顶部；所以伪SP寄存器一般用于寻址函数局部变量，真SP寄存器一般用于调用子函数时，寻址子函数的参数和返回值（后面会有具体示例演示）；2.当需要区分伪寄存器和真寄存器的时候只需要记住一点：伪寄存器一般需要一个标识符和偏移量为前缀，如果没有标识符前缀则是真寄存器。比如(SP)、+8(SP)没有标识符前缀为真SP寄存器，而a(SP)、b+8(SP)有标识符为前缀表示伪寄存器； Symbols 符号 有些符号比如 R1、LR 是不同架构预定义的寄存器。除此之外，还有 GO 定义的 4 个伪寄存器。
FP: Frame pointer: arguments and locals. PC: Program counter: jumps and branches. SB: Static base pointer: global symbols. SP: Stack pointer: the highest address within the local stack frame.</description>
    </item>
    <item>
      <title>ByteGraph and OceanBase</title>
      <link>https://runzhen.github.io/posts/bytegraph-and-oceanbase/</link>
      <pubDate>Thu, 20 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/bytegraph-and-oceanbase/</guid>
      <description>ByteGraph ByteGraph 是字节跳动开发的一个分布式图数据库。之前只是听说过图数据库，但并没有用过，因此在阅读的过程中难免对一些概念理解的不够深入。
为什么字节要开发图数据库呢？因为字节的产品都是社交App，因此用户，短视频，专注，点赞，粉丝所有的这些构成了一个巨大的图。
为什么现有的数据库无法满足呢？ 关系型数据库和文档型数据库显然不适合这样的应用场景，比如要获取两个用户之间的关系，即图中两个节点之间的路径，这个路径可以是关注，可以是都点赞了某个视频，关系型数据库无法满足性能需求。其他的图数据库有的是单机，有的是单 master，都不满足要求，因此需要造轮子。
字节的 Workload 分成了 3 种，比我平时听说的多了一种
OLTP，在线处理，比如一个用户发布了新文章，那么 (user,article),(user,tag), (article,tag) 这三条边就要被插入数据库。 OLAP，在线分析数据，一次需要查询大量数据做分析，比如做风险管理分析。 OLSP，这个第一次听说，Online Serving Processing。比如一个用户点赞了某个视频，那么后台需要实时计算他的喜好，然后推荐类似的视频。 整体架构如下所示：
BGE, ByteGraph Execution Engine 负责执行 SQL 语句。 BGS, A cache layer in ByteGraph，负责存储相关。 底层的 KV Stroage 可以选用 RocksDB 或者 TerarkDB。 BGE 使用了 Gremlin 作为解析 query language 的解析器，这是一个专门用于图查询的工具。用户输入的查询语句经过 Gremlin 生成 execution plan 然后传给 BGE。
既然是查询引起，那么就涉及到分布式事务，BGE也是用了 2PC。
上图可以直观的显示 ByteGraph 数据库中所存的数据，可见 KV store 是比较适合存这类数据的，因此 BG 的最底层是 KV store。
实现 5.1 分布式事务处理 前面提到，分布式事务处理用的是 2PC。值得一提的是，BG 不支持 MVCC</description>
    </item>
    <item>
      <title>Latency numbers every programmer should know</title>
      <link>https://runzhen.github.io/posts/latency-numbers-every-programer-should-know/</link>
      <pubDate>Sun, 02 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/latency-numbers-every-programer-should-know/</guid>
      <description>这期水一篇文章。 网上有很多人流传 Jeff Dean 的这个 Latency numbers，我看过很多遍但总是记不住，干脆把它抄下来算了。
L1 cache reference ......................... 0.5 ns Branch mispredict ............................ 5 ns L2 cache reference ........................... 7 ns Mutex lock/unlock ........................... 25 ns Main memory reference ...................... 100 ns Compress 1K bytes with Zippy ............. 3,000 ns = 3 µs Send 2K bytes over 1 Gbps network ....... 20,000 ns = 20 µs SSD random read ........................ 150,000 ns = 150 µs Read 1 MB sequentially from memory .</description>
    </item>
    <item>
      <title>for range and slice in Go</title>
      <link>https://runzhen.github.io/posts/go-for-range-and-slice/</link>
      <pubDate>Fri, 29 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/go-for-range-and-slice/</guid>
      <description>for range 的实现 下面这段代码会永无止境的循环吗 ？
package main import ( &amp;#34;fmt&amp;#34; ) func main() { sl := []int{1,2,3,4} for _, v := range sl{ sl = append(sl, v) } fmt.Println(sl) } 要验证它很简单，运行一下即可得到结果，最后的结果是
[1 2 3 4 1 2 3 4] 要理解为什么会有这样的结果不难，首先我们需要清楚一点 go 语言中的赋值语句都是赋值，那么就意味着
如果赋值的是一个指针, 那么拷贝的是指针指向对象的地址(就是一个数值, 至于这个数值有多大, 具体要看运行的平台)也就是指针的值 如果赋值的是一个对象, 那么就会拷贝这个对象 然后，我们再来看一下，当 for range 遇到不同的迭代对象时，编译器是如何展开代码的
数组 range_temp := range len_temp := len(range) for index_temp = 0; index_temp &amp;lt; len_temp; index_temp++ { value_temp = range_temp[index_temp] index = index_temp value = value_temp original body } slice 切片 for_temp := range len_temp := len(for_temp) for index_temp = 0; index_temp &amp;lt; len_temp; index_temp++ { value_temp = for_temp[index_temp] index = index_temp value = value_temp original body } map // Lower a for range over a map.</description>
    </item>
    <item>
      <title>Consistent Hashing</title>
      <link>https://runzhen.github.io/posts/consistent-hashing/</link>
      <pubDate>Sat, 09 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/consistent-hashing/</guid>
      <description>在理解一致性 hash 之前，先来看看这个问题是怎么产生的。 例如我们有一个数据库，里面存了成千上万张图片，用户访问图片会呈现一定的规律性，比如某段时间内一张照片火了，那么短时间内会有大量的请求访问这张图片，会对数据库造成压力，这时候我们就想到要加一层缓存，这也是系统架构的终极法宝。
所以，这种场景下典型的系统架构如下图所示：
客户端要访问文件名为 A 的图片，代理服务器根据文件名去缓存服务器中查询，如果cache中没有，那么最终去数据库取，同时也把取到的图片文件缓存在某个缓存服务器中。 这里有个题外话，通常我们会把文件名 A 通过 hash 函数转换成一段数字，便于操作，这里的 hash 函数与本文的一致性 hash 是不一样的。
那么问题来了：如何将图片均匀的缓存在缓存服务器上呢？
最简单的方式，以文件名为 key，缓存服务器个数为 N，取模得到余数，即 key % N = i，i 是几，就把图片缓存到对应编号的服务器上。
这种方式确实能够将数据 均匀的 分布在缓存上，但是最大的缺点是一旦 N 的数量发生变化，那么几乎所有的 i 都会改变，导致缓存失效。
例如，
key = 5 的文件，在 N = 3 时，缓存在编号为 2 的缓存服务器上。 增加一台服务器，N = 4，那么 key = 5 的文件应该在 1 号服务器上，但事实上它在 2 号。 导致这种情况的根本原因是什么呢？ 我们想让数据均匀分布，但是均匀的算法却依赖于 N ，而 N 直接依赖于服务器的数量！
一致性 Hash 的原理 消除依赖 所以解决的办法就是，让均匀分布的计算方法不依赖于 Redis 的个数 N。</description>
    </item>
    <item>
      <title>etcd-raft: raftLog</title>
      <link>https://runzhen.github.io/posts/etcd-raft-raftlog/</link>
      <pubDate>Sun, 22 May 2022 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/etcd-raft-raftlog/</guid>
      <description>etcd-raft 有关 log 的实现在分布在log.go，log_unstable.go，storage.go 三个文件中。首先看一下 raftLog 结构体。
raftLog 结构体 type raftLog struct { // storage contains all stable entries since the last snapshot. storage Storage // unstable contains all unstable entries and snapshot. // they will be saved into storage. unstable unstable // committed is the highest log position that is known to be in // stable storage on a quorum of nodes. committed uint64 // applied is the highest log position that the application has // been instructed to apply to its state machine.</description>
    </item>
    <item>
      <title>etcd-raft: Leader Election</title>
      <link>https://runzhen.github.io/posts/etcd-raft-leader-election/</link>
      <pubDate>Sun, 15 May 2022 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/etcd-raft-leader-election/</guid>
      <description>首先看一下 raft node 之间传递的基本消息（比如 leader 选举，AppendLog）类型 Message protobuf 定义
message Message { optional MessageType type = 1 ; optional uint64 to = 2 ; optional uint64 from = 3 ; // 整个消息发出去时，所处的任期 optional uint64 term = 4 ; // logTerm is generally used for appending Raft logs to followers. For example, // (type=MsgApp,index=100,logTerm=5) means leader appends entries starting at // index=101, and the term of entry at index 100 is 5.</description>
    </item>
    <item>
      <title>go-redis Debug Notes</title>
      <link>https://runzhen.github.io/posts/go-redis-latency-debug/</link>
      <pubDate>Sun, 01 May 2022 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/go-redis-latency-debug/</guid>
      <description>Background 本文背景是这样的: 有一个线上服务使用了 go-redis 库连接 redis，目前 QPS 也不是很高，大约每秒一次的样子，但是通过 log 发现每次 redis 操作花费的时间如下：
redis call cost: 0 ms redis call cost: 2 ms redis call cost: 0 ms redis call cost: 1 ms redis call cost: 0 ms redis call cost: 17 ms redis call cost: 0 ms .... 正常一个简单的 redis get 操作耗费 0-3ms 时间可以理解，但是为什么会出现 17 ms 呢？ 而且出现的频率还不低，大概每 30 个正常的中会出现一个。
尝试 debug 首先总结一下场景和条件
service 部署在 k8s 中，大概 10 个 pod 在运行。 整个 service 的 QPS 大概一秒一个，很低。 高延迟的情况大概每 30 个 log 出现一个。 service 使用简单的 redis get()，没有复杂操作。 但是 service 本身是有很多 go routine 并发的。 所以可能出现问题的地方</description>
    </item>
    <item>
      <title>使用 Kubernetes 遇到的一些问题和解决思路</title>
      <link>https://runzhen.github.io/posts/k8s-debug-stories/</link>
      <pubDate>Wed, 27 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/k8s-debug-stories/</guid>
      <description>update on 2022-05-21
今天在 homelab 的 k8s 集群上发生了同样的情况，我想删除一个 namespace，再确认已经把 namespace 里面所有其他资源都删除的情况下，namespace 始终是 Terminating, 找了很多资料，方法也众说纷纭 。
最后通过看 api-server log 发现原来又是 Unable to authenticate the request due to an error: x509: certificate has expired or is not yet valid
root cause 还是我更新 cert 的时候又漏了某些步骤。
事情的起因是 k8s 的 cert 过期了，在目录 /etc/kubernetes/pki/ 下面的这些 cert 都与 k8s 的核心服务息息相关，因此 cert 过期了，整个 k8s 集群就停止服务了。
这个集群是 kubernetes 1.14, 因此需要运行几个命令完成更新，而 1.15 版本以上这个过程简化了不少。 由于之前已经 renew cert 两次了，因此正常按部就班几个操作就完事了，但是这个因为一点小疏忽，加上系统死机重启了一次，花了很多时间去恢复各种服务。
本文记录 debug 的过程中遇到的一些症状，以及后来发现的解决方法，为以后遇到类似问题提供思路。</description>
    </item>
    <item>
      <title>gRPC client 如何实现 TCP 重连</title>
      <link>https://runzhen.github.io/posts/how-does-grpc-client-reconnect-tcp/</link>
      <pubDate>Mon, 04 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/how-does-grpc-client-reconnect-tcp/</guid>
      <description>之前写过一篇 gRPC-go 建立 TCP 连接的过程 博客，主要研究了 client 程序启动后，如何与 server 建立 TCP 连接。
今天，在思考 redis-go 的连接池实现的时候，突然想到：
当 gRPC 的 TCP 连接断开后，能自动重连吗？ 如果可以，是如何实现的 ？ 首先要注意，这里指的是 TCP 连接，而不是 http2 中的 stream。 我们知道，gRPC 数据的传输使用 http2 的多路复用，也就是在一个 TCP 连接上有多个全双工的 http2 stream，这里的 stream 如果被断开后怎么重连与 http2 的实现有关，不在本文讨论范围。
对于上面第一个问题，使用 gRPC 的经验告诉我是可以自动重连的，不妨再做个简单的测试，client 端代码如下：
func main() { conn, _ := grpc.Dial(&amp;#34;127.0.0.1:8080&amp;#34;, grpc.WithInsecure()) defer conn.Close() cli := protobuf.NewTestClient(conn) req := &amp;amp;protobuf.EchoRequest{ Msg: &amp;#34;hi&amp;#34;, } for i := 0; i &amp;lt; 10000; i++ { time.</description>
    </item>
    <item>
      <title>Escape from k8s Pod to the Host using nsenter</title>
      <link>https://runzhen.github.io/posts/nsenter-escape-from-pod-to-host/</link>
      <pubDate>Sun, 03 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/nsenter-escape-from-pod-to-host/</guid>
      <description>本文是读完 Detecting a Container Escape with Cilium and eBPF 和 使用 Cilium 增强 Kubernetes 网络安全 的一个简单总结。
如何从 Pod 逃逸到 Host 通常为了安全起见，生产环境的 docker image 都要求不使用 root，一般都是在 Dockerfile 中指定 USER xxx，这样启动的 container/pod 是使用非特权的 user，这样的 user 是没法用 sudo 安装软件的。
有时为了能临时 debug，需要安装 vim, curl 之类的命令，又不想改动 Dockerfile 重新 build image，该怎么办呢？
一个 k8s 原生支持的方法是在 deployment 里面指定 securityContext，如下所示
$ cat privileged.yaml apiVersion: v1 kind: Pod metadata: name: privileged-the-pod spec: hostPID: true hostNetwork: true containers: - name: privileged-the-pod image: nginx:latest ports: - containerPort: 80 securityContext: privileged: true 对于 docker container，可以在指定 docker run 命令时，设置 --user 为 0 也能获得 root 的 container。</description>
    </item>
    <item>
      <title>Bulild the Tetris Game using Rust WASM </title>
      <link>https://runzhen.github.io/posts/tetris-game-in-rust/</link>
      <pubDate>Sun, 06 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/tetris-game-in-rust/</guid>
      <description>起因 事情的起因是在 Switch 上玩 Tetris99 游戏，由于不喜欢这种吃鸡的形式，只想玩小时候的那种掌机模式，于是想到可不可以自己做一个。
有了这个想法以后，打算使用 Rust + WASM，一方面是学习一下新技术，另一方面考虑到能直接在浏览器运行，可以跨平台，甚至可以在电视机上用浏览器打开网页就可以玩。
选定技术栈以后，在 Github 上搜了一下，发现早有人做了类似的工作，不过没关系，主要还是要自己实现一下。
几种技术方案 学习了一圈以后，理解了用 Rust + WASM 实现一个 web 游戏的大体思路。
首先，Rust 的 wasm-bindgen 库必不可少，这是连接 rust 代码和 wasm 之间的桥梁。
其次，既然是 web 游戏，那么免不了要画图，如何画图呢？ 大家都不约而同的选择了 HTML 的 canvas，这是一种 html 标准自带的画图方式，比如用下面这样简单的代码，就能画一个矩形。
&amp;lt;html&amp;gt; &amp;lt;body&amp;gt; &amp;lt;canvas id=&amp;#34;myCanvas&amp;#34; width=&amp;#34;200&amp;#34; height=&amp;#34;100&amp;#34; style=&amp;#34;border:1px solid #000000;&amp;#34;&amp;gt; &amp;lt;/canvas&amp;gt; &amp;lt;/body&amp;gt; &amp;lt;/html&amp;gt; 所以，本质上我要做的就是用 Rust/WASM 代码 或者 JavaScript 代码，控制这个 &amp;lt;canvas id=&amp;quot;myCanvas&amp;quot; ，并且定期刷新，这样就能显示动画效果了。 如果你是个 JavaScript 高手，并且打算全部用 JavaScript 实现，那么现在就可以开始动手了。
但如果是 Rust WASM 的方式，还需要考虑下是 纯 WASM 实现呢？ 还是 WASM 实现核心算法逻辑，JavaScript 实现画图这样的组合方式？</description>
    </item>
    <item>
      <title>Rust async and tokio</title>
      <link>https://runzhen.github.io/posts/rust-async-and-tokio/</link>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/rust-async-and-tokio/</guid>
      <description>快速入门 Rust 语言原生提供了异步操作的关键字 async 和 await，但通常还需要配合第三方的 runtime，其中最有名的就是 tokio 了。
在开始了解 Rust 的所谓异步是什么样子之前，先看一下如何写一个简单的 Rust 异步程序。
以下是 main.rs
async fn hello_world() { hello_cat().await; println!(&amp;#34;hello, world!&amp;#34;); } async fn hello_cat() { println!(&amp;#34;hello, kitty!&amp;#34;); } #[tokio::main] async fn main() { let future = hello_world(); println!(&amp;#34;start&amp;#34;); future.await; } Cargo.toml 文件中加入一行
[dependencies] tokio = { version = &amp;#34;1&amp;#34;, features = [&amp;#34;full&amp;#34;] } 运行上面的代码，会看到这样的输出
start hello, kitty! hello, world! 可以看出，future = hello_world(); 是创建一个异步执行的代码块， 并把它赋值给了 future 变量，这个代码块不会立刻执行，而是等到用户调用 await 的时候再去执行。</description>
    </item>
    <item>
      <title>DDIA 第九章 一致性和共识</title>
      <link>https://runzhen.github.io/posts/ddia-ch9-consistency-and-consensus/</link>
      <pubDate>Sun, 20 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/ddia-ch9-consistency-and-consensus/</guid>
      <description>一致性保证 本章主要包含了以下话题：
常用的强一致性模型：线性一致性，的优点和缺点。 分布式系统中的事件顺序，特别是因果关系和全局顺序的问题。 如何原子的提交分布式事务，也就是说如何解决共识问题。 线性一致性 哪些地方依赖于线性一致性呢 ？
Locking 服务和 leader election，比如加锁出错了会导致两个人同时写同一个文件。 账户余额，产品库存信息，比如产品超卖。 如何实现一个线性一致性系统 在分布式系统里面，产生不一致的根本原因是因为数据有多个副本，而更新这些副本不是原子操作。
以下是几种多副本系统，能否实现线性一致性的比较：
Single-leader replication (可能线性一致) 这里我猜作者说的是传统 MySQL 这样的主从复制技术。 Consensus Algorithms (线性一致) 这里应该就是常说的 Panox 和 Raft 了。 Multi-leader replication (非线性一致) Leaderless replication (也许不是线性一致的) Dynamo 风格 和 Cassandra 风格。 线性一致的代价 假设两个 datacenter 之间网络断了的情况下，
如果是 multi-leader 系统，那么每个 datacenter 仍然可以独立运行，datacenter 之间的数据是异步同步的，所以不会受到影响。 如果是 single-leader 系统，如果 client 连到了全是 follower 的 datacenter，那么所有 write 和 linerizable read 都受影响，如果 client 连到的是 leader 所在的 datacenter，则不受影响。 CAP Consistency, Availability, Partition Tolerance.</description>
    </item>
    <item>
      <title>Makefile 的几个语法坑</title>
      <link>https://runzhen.github.io/posts/makefile-syntax-pitfalls/</link>
      <pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/makefile-syntax-pitfalls/</guid>
      <description>Makefile 和 Bash script 在使用的过程中有很多奇奇怪怪的坑，本文做一下纪录。
首先，有两个文件，一个叫 envs，里面定义了一个环境变量，比如
$ cat envsexport GOPROXY=&amp;#34;test.local&amp;#34; 第二个文件就是 Makefile ，假如我这样写
test: source ./envs echo ${GOPROXY} 所以，总的目标是，我希望在 Makefile 中导入另一个文件中事先定义好的环境变量。 然而这样的写法有很多问题。
source 命令找不到 加入直接运行 make, 很有可能你会看到这样的错误
$ make source ./envs make: source: Command not found 可是在 terminal 里面明明可以用 source 命令啊？ 于是，第一个坑出现:
source is a (non-POSIX) shell builtin, not an executable program on any conventional UNIX-like system. If source is changed to ., make changes its strategy; instead of trying to find and execute a program, it just passes the rule body to the system shell.</description>
    </item>
    <item>
      <title>Gin HTTP 框架学习笔记</title>
      <link>https://runzhen.github.io/posts/gin-framework/</link>
      <pubDate>Fri, 03 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/gin-framework/</guid>
      <description>最近要做一个 REST API server，在网上搜索了一遍以后，发现常用的是 Gin 和 Echo，并且很多人都说 golang 本身提供的 http server 已经足够强大，gin 和 echo 也只是在外包了一层。
我看 Gin 的源码行数比 Echo 少很多，而且测试覆盖率也高很多，因此决定学习一下 Gin，本文目标有以下这些
学习如何设计一个 REST 风格的 server ？ 学习 Gin 在 go 自带的 http server 基础上做了哪些工作？ 启动 Gin http server 在使用 Gin 框架的时候，最后都会调用 gin.Run(&amp;quot;:8080&amp;quot;) ，这样你的 http server 就可以就收 client 请求了，
func (engine *Engine) Run(addr ...string) (err error) { defer func() { debugPrintError(err) }() address := resolveAddress(addr) debugPrint(&amp;#34;Listening and serving HTTP on %s\n&amp;#34;, address) err = http.</description>
    </item>
    <item>
      <title>错误使用 time.After() 导致内存泄漏</title>
      <link>https://runzhen.github.io/posts/golang-timer-leak/</link>
      <pubDate>Sun, 08 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-timer-leak/</guid>
      <description>今天看到了一篇有关 timer 泄露的文章，觉得很有意思，于是把它记录下来。
一般没有问题的写法 说道 time.After() 会导致内存泄露，很多人一定会觉得奇怪，因为代码里经常会用到它，也没见有内存泄漏啊？
是的，一般我们这样写的话是没有问题的
func main() { ch := make(chan int) go func() { ch &amp;lt;- 1 }() select { case _ = &amp;lt;-ch: case &amp;lt;-time.After(time.Second * 1): fmt.Println(&amp;#34;timeout&amp;#34;) } } 有问题的写法 那么，什么样的写法有问题呢？ 当使用 for loop 的时候，比如这样
for { select { case _ = &amp;lt;-ch: // do something... continue case &amp;lt;-time.After(300 * time.Millisecond): fmt.Printf(&amp;#34;time.After() fire！\n&amp;#34;) } } 很不幸的是，上面这样的写法也非常常见，我自己就写过这样的代码。那么它真的会造成内存泄露吗？试一下便知道
前一篇博客中已经介绍了如何使用 pprof 对 Go 程序进行 profiling，简单提一下步骤
在代码中引入 _ &amp;quot;net/http/pprof&amp;quot;, 并开启一个http server 导出 metrics 运行你的 binary 执行 go tool pprof -http=:8081 http://localhost:6060/debug/pprof/heap 浏览器就会自动打开 localhost:8081 显示结果了 测试代码如下：</description>
    </item>
    <item>
      <title>k8s 切换 namespace 以及命令补全</title>
      <link>https://runzhen.github.io/posts/k8s-set-namespace-tool/</link>
      <pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/k8s-set-namespace-tool/</guid>
      <description>本文可以学到
.kube/config 文件中有哪些内容 如何实现 bash 的命令补全功能 起因 在使用 kubectl 命令的过程中，经常需要查看不同 namespace 下的资源，因此命令经常需要带上 -n name。
如果不想每次都多打这些字符，也可以设置一个默认的 namespace，
kubectl config set-context --current --namespace=xxxx 这样是方便了不少，但是一旦切换了 namespace 之后，又要重复上面的命令，而且经常还不记得。
有没有更好的办法呢？ 有人开发了一个小工具，kubectx 专门用于方便的切换 ctx 和 namespace。 ctx 是什么呢？ 其实就是哪个 k8s 集群。 说白了就是让你方便的在多个集群和 namespace 之间切换。
kubectx 有两种实现，一开始用的是最简单的 bash shell 脚本，新的版本开始用 k8s client API 开发。 下文的分析仅仅关注 namespace 的切换。
shell 版本的实现 这个实现非常简单，本质上就是调用几个 kubectl 命令实现 ns 切换。
首先需要知道的是，在 ~/.kube/config 路径下的 config 记录了你配置 kubectl 的信息，比如你用 kubectl 操作过几个 k8s 都会纪录在里面。
apiVersion: v1 clusters: - cluster: certificate-authority-data: DATA+OMITTED server: https://10.</description>
    </item>
    <item>
      <title>Golang pprof 的使用姿势</title>
      <link>https://runzhen.github.io/posts/golang-pprof-usage/</link>
      <pubDate>Sun, 18 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-pprof-usage/</guid>
      <description>首先，在代码中引入 pprof 的方式非常简单，只要把下面这段代码放到 main 函数中即可
_ &amp;#34;net/http/pprof&amp;#34; go func() { if err := http.ListenAndServe(&amp;#34;:9090&amp;#34;, nil); err != nil { panic(err) } os.Exit(0) }() 然后启动你的程序，再用以下这些命令去对应的端口做 profiling
// cpu profile 默认从当前开始收集 30s 的 cpu 使用情况，需要等待 30s go tool pprof http://47.93.238.9:9090/debug/pprof/profile # wait 120s go tool pprof http://47.93.238.9:9090/debug/pprof/profile?seconds=120 // 以下 second 参数不起作用，因为采样是一瞬间完成的 go tool pprof http://47.93.238.9:9090/debug/pprof/heap go tool pprof http://47.93.238.9:9090/debug/pprof/goroutine go tool pprof http://47.93.238.9:9090/debug/pprof/block go tool pprof http://47.93.238.9:9090/debug/pprof/mutex 还有一种是 import &amp;quot;runtime/pprof“的方式，这种不太常用，不在本文范围。
运行了 go tool pprof 命令以后，会进入到一个交互界面，</description>
    </item>
    <item>
      <title>Implement LRU Cache in Rust</title>
      <link>https://runzhen.github.io/posts/rust-lru-cache/</link>
      <pubDate>Sun, 11 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/rust-lru-cache/</guid>
      <description>lru 算法的原理简而言之就是一个 hash ，一个 double linked list
Linked List 提供 O(1) 的复杂度对元素进行插入和删除 hash 提供 O(1) 的复杂度进行查找 本文主要是通过阅读一个 rust 实现的 lru 学习相关语法。
如何在结构体里面使用指针？ rust 是否有 raw pointer 直接指向内存地址，如果能用该怎么用？ Linked List 节点结构体 上面提到，真正的 key/value 是存在双链表的 Node 里，所以需要先定义这个 Node 长什么样，lru-rs 中 LruEntry 表示的就是 node：
K V 代表的是泛型的类型， struct LruEntry&amp;lt;K, V&amp;gt; { key: mem::MaybeUninit&amp;lt;K&amp;gt;, val: mem::MaybeUninit&amp;lt;V&amp;gt;, prev: *mut LruEntry&amp;lt;K, V&amp;gt;, next: *mut LruEntry&amp;lt;K, V&amp;gt;, } 下面是如何初始化一个 Node，
impl&amp;lt;K, V&amp;gt; LruEntry&amp;lt;K, V&amp;gt; { fn new(key: K, val: V) -&amp;gt; Self { LruEntry { key: mem::MaybeUninit::new(key), val: mem::MaybeUninit::new(val), prev: ptr::null_mut(), next: ptr::null_mut(), } } fn new_sigil() -&amp;gt; Self { LruEntry { key: mem::MaybeUninit::uninit(), val: mem::MaybeUninit::uninit(), prev: ptr::null_mut(), next: ptr::null_mut(), } } } key value 用 mem::MaybeUninit::new(key)进行初始化 prev next 指针用 ptr::null_mut() 初始化 LRU cache 结构体 链表的 node 定义好以后，双链表结构也自然而然就有了。接下来还缺一个 map 结构体，这个可以用 rust 原生的 hash 函数库，然后就可以定义出 LRU 结构体</description>
    </item>
    <item>
      <title>bbolt 的设计与实现</title>
      <link>https://runzhen.github.io/posts/bbolt/</link>
      <pubDate>Wed, 07 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/bbolt/</guid>
      <description>关于 bbolt 的分析，网上已经有很多资料，本文只是对资料和源码的整理，主要是自己的学习笔记，文章最后的参考资料中有更多链接。
bbolt DB 整体组织 首先，bbolt 的一个文件是一个 DB，DB 中可以有多个 table， 每一个 table 是一个 B+ 树。而这个 table 在源码中就是 bucket， 整个 DB 就是一个大 bucket，它的子节点有多个 bucket。整体结构如图所示：
顶层 B+ 树，比较特殊，称为 root bucket，其所有叶子节点保存的都是子 bucket B+ 树根的 page id 其他 B+ 树，不妨称之为 data bucket，其叶子节点可能是正常用户数据，也可能是子 bucket B+ 树根的 page id。 这样，就清楚的知道了 bbolt 中 DB，table，和 data 是如何组织的了。
bbolt 的源码很简洁，主要功能分布在以下几个文件：
bucket.go：对 bucket 操作的高层封装。包括 kv 的增删改查、子 bucket 的增删改查以及 B+ 树拆分和合并。 node.go：对 node 所存元素和 node 间关系的相关操作。节点内所存元素的增删、加载和落盘，访问孩子兄弟元素、拆分与合并的详细逻辑。 cursor.go：实现了类似迭代器的功能，可以在 B+ 树上的叶子节点上进行随意游走。 page.go: page 是磁盘上一个 4kb 页的表示，注意，相比 page，第二行提到的 node 表示的是内存里的结构。 db.</description>
    </item>
    <item>
      <title>Bigtable 论文阅读笔记</title>
      <link>https://runzhen.github.io/posts/bigtable-sstable/</link>
      <pubDate>Sun, 20 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/bigtable-sstable/</guid>
      <description>最近因为工作需要用到 Bigtable，而设计一个好的数据库 Schema 对于性能至关重要，因此想找一些资料看看别人是如何根据自身业务特点设计 schema 的。
在网上找到了一篇 GCP 自己的官方文档 , 里面提到了一些 best practice，也提到了哪些坑需要避免，然而还是看的云里雾里。 比如，
Row keys to avoid
Row keys that start with a timestamp. This will cause sequential writes to be pushed onto a single node, creating a hotspot. If you put a timestamp in a row key, you need to precede it with a high-cardinality value like a user ID to avoid hotspotting.
Row keys that cause related data to not be grouped together.</description>
    </item>
    <item>
      <title>Implement Bloom Filter in Rust Language</title>
      <link>https://runzhen.github.io/posts/rust-examples-1/</link>
      <pubDate>Sat, 19 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/rust-examples-1/</guid>
      <description>关于 Bloom Filter 的原理不做介绍，网上各种资料满天飞，其中参考资料 1 已经讲解的很详细。
我重点关注如何用 Rust 实现一个简单的 Bloom Filter，并学习一些语法，源码在参考资料 2 。
BloomFilter 结构体 pub struct BloomFilter&amp;lt;T&amp;gt; { hasher: T, k: u32, bit_vec: BitVec, insert_count: u64, } 尖括号中的 T 代表泛型，这样我们就可以使用不同的 hash 函数实现 （hasher） k 表示使用几个 hash 函数，根据 BF 的原理，使用多个 hash 能减少 False Positive bit vec 表示使用一个多大的 bit 数组，这个关系到 BF 的命中率和 FP 率 BitVec 的作用等于是实现了 bloom filter 的 bit array，直接用这个库省略了作者重复实现一个。
定义 BloomHasher 定义这个 trait 的目的是让所有的 hash 函数库都有 hash() 这个函数，方便在上面的 hasher 中调用。</description>
    </item>
    <item>
      <title>docker exec 是如何实现交互的</title>
      <link>https://runzhen.github.io/posts/docker-exec-io-stream/</link>
      <pubDate>Mon, 17 May 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/docker-exec-io-stream/</guid>
      <description>docker exec 命令的作用是进入到“容器内部”，并执行一些命令，那么它是如何实现把“容器内部”的 io 重定向到我们的终端(bash) 的呢？
基本原理 首先，要明白容器所依赖的内核 namespace 的概念，其实不存在“容器内部”，只要两个进程在相同的 namespace，那它们就相互可见，从用户的角度来说，也就是进入了容器內部。
nsenter nsenter 是一个命令行工具，它可以运行一个 binary，并且把它加入到指定的 namespace 中。
用法如下,
nsenter -h nsenter -a -t &amp;lt;pid&amp;gt; &amp;lt;command&amp;gt; nsenter -m -u -i -n -p -t &amp;lt;pid&amp;gt; &amp;lt;command&amp;gt; 假设有一个 redis container 正在运行，通过 docker inspect --format {{.State.Pid}} 获取 pid, 假设为 2929。
然后运行 nsenter 命令：
# nsenter -a -t 2929 ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND redis 1 0.0 0.</description>
    </item>
    <item>
      <title>Golang Channel 用法总结</title>
      <link>https://runzhen.github.io/posts/golang-channel-usage/</link>
      <pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-channel-usage/</guid>
      <description>之前的博客中已经粗略探究了一下 golang channel 的实现原理，本文总结一下使用 channel 的各种姿势。
先看一下对不同状态的 channel 的读，写，关闭操作的结果
1. 使用 for range 读取 channel 场景： 当需要不断从 channel 里读数据时
这是最常用的方式，又安全又便利，当channel 被关闭时，for 循环自动退出。 用法不再赘述。
2. 使用 _, ok 判断 channel 是否关闭 场景: 读 channel，但需要判断 channel 是否已关闭。
读 channel 的操作 &amp;lt;- chan 既可以返回一个值，也可以返回两个值，这里就是用的两个返回值的方式。
举例：
if v, ok := &amp;lt;- ch; ok { // can read channel fmt.Println(v) } 读到数据，并且通道没有关闭时，ok 的值为 true。 通道关闭，无数据读到时，ok 的值为 false。 3. 与 select 搭配使用 场景: 需要对多个通道进行处理，或者设置超时
举例：
func (h *Handler) handle(job *Job) { select { case h.</description>
    </item>
    <item>
      <title>如何设计一个连接池</title>
      <link>https://runzhen.github.io/posts/go-redis-conn-pool/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/go-redis-conn-pool/</guid>
      <description>事情的起因是我在 k8s 中部署了一个 redis，然后 service A 使用 go-redis 库连接 redis。
这个时候我想到： service Pod 和 redis Pod 启动的顺序是不一定的，可能是 service Pod 先启动，此时 redis pod 还没有启动；又或者 redis pod 中途 restart 了。 go-redis 库能正确的处理重连吗？
简单的用 kubectl 命令删除、 重启了 redis，发现 service Pod 能自动恢复连接，说明 go-redis 正确进行了处理，那么它是怎么做的呢 ？
在寻找答案之前，先来想想如果是我自己实现，需要哪些功能？ 该怎么实现？
conn pool 需要自动删除已经断开的、坏掉的连接。 (开一个 goroutine 定期检查即可) 能自动新建连接，补齐一定数量的 conn。 (也不难，goroutine 即可) 如何检测一个 conn 是不是出错了？ 对外的接口是 Get 和 Put，除了正常的用 mutext 控制并发以外，还有什么特殊的操作吗？ go-redis 源码位于 redis/v8/internal/pool/pool.go , 首先看 pool.Options 数据结构
type Options struct { PoolSize int // 连接池数量 MinIdleConns int // 最小空闲连接数 MaxConnAge time.</description>
    </item>
    <item>
      <title>如何实现一个 kubectl-debug</title>
      <link>https://runzhen.github.io/posts/how-to-implement-kubectl-debug/</link>
      <pubDate>Sat, 24 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/how-to-implement-kubectl-debug/</guid>
      <description>借助 k8s client-go 连接 API Server，并进行简单的 list 操作，创建 deployment 如何在 pod 内创建 container 如何让 container 加入到某个 Pod 中，并共享 namespace 如何让 pod 内部的 tty 操作结果显示在用户端 docker exec 和 直接 nsenter 还是不太一样的：
docker exec， OCI-O 的实现是启动一个 grpc server，重定向 IO，等于是把 container 的 IO stream 重定向到 用户 cli nsenter 则是启动一个 进程，然后加入到 container 的 ns 所以 前者不需要知道 container 的 pid，而后者需要知道，所以后者需要执行 docker insepect 命令。 观察上图，分析原理，不难发现，容器内部的进程关系已然不是树。然而，为什么总是强调“树状”关系呢？答案是：树状的继承关系，有利于容器管理。以上文《docker logs 实现剖析》中卖的关子「docker exec的标准输出不会作为容器日志」为例，Docker Daemon 创建容器主进程时，负责接管主进程的标准输出，从而保证容器主进程下所有进程的标准输出被接管，然而 Docker Daemon 在新创建 docker exec 所需执行的进程时，后者的标准输出并未与容器主进程作关联，也并未被 Docker Daemon 特殊处理，故 docker exec 所执行进程的标准输出不会进入容器的日志文件中。</description>
    </item>
    <item>
      <title>Goroutine 的 PMG 模型</title>
      <link>https://runzhen.github.io/posts/golang-runtime-pmg-1/</link>
      <pubDate>Mon, 19 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-runtime-pmg-1/</guid>
      <description>稍微了解过 Go runtime 的人想必都听过 goroutine 的 PMG 模型，哪么它到底代表什么意思呢？ Golang 源码中又是如何实现的？
前言 关于 PMG 的解释网上有很对，随便 copy 一个：
M 代表 Machine，系统线程，它由操作系统管理的，goroutine就是跑在M之上的；M 是一个很大的结构，里面维护小对象内存cache（mcache）、当前执行的goroutine、随机数发生器等等非常多的信息。 P 是 Processor，处理器，它的主要用途就是用来执行goroutine的，它维护了一个goroutine队列，即runqueue。Processor是让我们从N:1调度到M:N调度的重要部分。 G 代表 goroutine 它包含了栈，指令指针，以及其他对调度goroutine很重要的信息，例如其阻塞的channel。 通常 go 程序中可以用 GOMAXPROCS 设置 Processor 的个数； 而 M 则是 clone系统调用创建的，或者用linux pthread 库创建出来的线程实体。 M 与 P 是一对一的关系。
基本结构体 打开 src/runtime/runtime2.go 文件，p,m,g 三个结构体的定义是按顺序在一起的，除此之外还有一个 schedt，与 goroutine 的调度相关。
g 结构体 G 就是 goroutine 的意思，每个 Goroutine 对应一个 g 结构体，它有自己的栈内存, G 存储 Goroutine 的运行堆栈、状态以及任务函数。 当一个 goroutine 退出时，g 会被放到一个空闲的对象池中以用于后续的 goroutine 的使用， 以减少内存分配开销。</description>
    </item>
    <item>
      <title>Kubernetes Scheduler 设计与实现 (一)</title>
      <link>https://runzhen.github.io/posts/k8s-scheduler-beginning/</link>
      <pubDate>Sat, 17 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/k8s-scheduler-beginning/</guid>
      <description>Scheduler 的工作就是决定让一个 pod 在哪个 node 上运行。 scheduler 从 API Server 获得 pod 和 node 的信息，然后把它的决策信息写会 API Server, 它自己不参与具体的调度，而是运行在每个 node 上的 kubelet 主动获取更新，然后启动 pod。
scheduler 的入口函数在 cmd/kube-schduler/server.go，但实际工作都是在 pkg/scheduler/scheduler.go 里面的 Run 函数开始的。
打开 scheduler.go 文件找到结构体 Scheduler ，会发现它有很多私有函数，但只有唯一一个公开的 Run() 函数。
先从 Scheduler 结构体来说一下调度器的整体思路，其中最重要的三个成员如下：
type Scheduler struct { Algorithm core.ScheduleAlgorithm NextPod func() *framework.QueuedPodInfo SchedulingQueue internalqueue.SchedulingQueue } Algorithm 就是具体调度的算法 SchedulingQueue 是等待调度的队列，它本身是一个接口，它的实现是 PriorityQueue ，位于 pkg/scheduler/internal/queue/scheduling_queue.go NextPod 获取等待调度的 pod 另外顺便提一下，kubernetes 中的调度队列是由三个队列组成，分别是：
activeQueue：待调度的 pod 队列，scheduler 会监听这个队列 backoffQueue：在 kubernetes 中，如果调度失败了，就相当于一次 backoff。 backoffQueue 专门用来存放 backoff 的 pod。 unschedulableQueue：调度过程被终止的 pod 存放的队列。 然后来看 Scheduler 的 Run 函数:</description>
    </item>
    <item>
      <title>Golang Channel 的实现</title>
      <link>https://runzhen.github.io/posts/golang-channel/</link>
      <pubDate>Sat, 10 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-channel/</guid>
      <description>Channel 可以说是 Go 语言最具特色的设计了，我们经常会看到一些老鸟这样教育菜鸟：
Do not communicate by sharing memory; instead, share memory by communicating.
那么熟练使用 Golang 就离不开 channel，有必要了解一下 channel 是怎么实现的。
channel 的源代码在 Golang 的 src/runtime/chan.go 目录下，先看结构体:
type hchan struct { qcount uint // 循环列表元素个数 dataqsiz uint // 循环队列的大小 buf unsafe.Pointer // 循环队列的指针 elemsize uint16 // channel 中元素的大小 closed uint32 // 是否已close elemtype *_type // channel 中元素类型 sendx uint // send 在buffer中的索引 recvx uint // recv 在buffer中的索引 recvq waitq // receiver 的等待队列 sendq waitq // sender 的等待队列 lock mutex } type waitq struct { first *sudog last *sudog } 其中</description>
    </item>
    <item>
      <title>Golang WaitGroup 的实现</title>
      <link>https://runzhen.github.io/posts/golang-waitgroup/</link>
      <pubDate>Mon, 05 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-waitgroup/</guid>
      <description>sync.WaitGroup 的作用就是让主函数等待所有 goroutine 都执行完毕，再退出。
一个最简单的例子如下，如果没有 wg，那么 main 会在 goroutine 执行之前就退出，从而不会看到任何 output。
func main() { wg := sync.WaitGroup{} for i := 0; i &amp;lt; 3; i++ { wg.Add(1) go func(i int) { fmt.Println(i) wg.Done() }(i) } wg.Wait() } 那么 WaitGroup 是如何实现的呢？
万变不离其宗，其底层还是基于 go runtime 提供的信号量机制，也就是 runtime_Semrelease() 和 runtime_Semacquire()， 在之前的文章 Golang RWMutex 的实现 和 netpoll 的实现 中都有它们的影子存在。
runtime_Semacquire(s *uint32) 此函数会阻塞直到信号量*s的值大于0，原子减这个值。 runtime_Semrelease(s *uint32, lifo bool, skipframes int) 此函数执行原子增信号量的值，然后通知被runtime_Semacquire阻塞的协程 说到底，就是用 信号量 和 gopark 来控制 goroutine 是运行还是挂起，wg.</description>
    </item>
    <item>
      <title>Golang reflect 的使用</title>
      <link>https://runzhen.github.io/posts/golang-reflect/</link>
      <pubDate>Sat, 03 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-reflect/</guid>
      <description>所谓反射 (refection) 是指程序在运行过程中获取变量的类型、属性。 在 Golang 中，有时我们会看到 reflect.ValueOf() 或者 reflect.TypeOf() 这两个函数，这就是反射出一个变量的值和类型。 gPRC 的实现中也大量运用了反射。
本文主要介绍如何使用 reflect 包，关于 Go 内部是如何实现的将在下一篇文章中介绍。
TypeOf 和 ValueOf 先看一个最简单的例子
type User struct { Name string Age int } func main() { u := User{&amp;#34;Dick&amp;#34;, 18} t := reflect.TypeOf(u) v := reflect.ValueOf(u) fmt.Printf(&amp;#34;u type = %T, %v\n&amp;#34;, u, u) fmt.Printf(&amp;#34;t type = %T, %v\n&amp;#34;, t, t) fmt.Printf(&amp;#34;v type = %T, %v\n&amp;#34;, v, v) // 获取 v 的值 // v.Age , 错误，因为 v 是 reflect.</description>
    </item>
    <item>
      <title>Golang io 包的实现</title>
      <link>https://runzhen.github.io/posts/golang-io-package/</link>
      <pubDate>Fri, 02 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-io-package/</guid>
      <description>Golang 的 io package 包含 3 个文件 io.go, multi.go, pipe.go, 其中最主要的时 io.go。
当我们打开 io.go 的源码后，发现这个文件里面定义了大量的接口，实际上，io 包的作用就是如此 - 定义基本的 Read / Write inteface，而把具体的实现交给其他 package，比如 strings package 中就专门实现了 reader/writer，在后面的文章中再分析 strings 包。
接下来就看看 io 包中到底包含了哪些东西。
io.go 首先时定义了 4 个基础操作，读，写，关闭，seek
type Reader interface { Read(p []byte) (n int, err error) } type Writer interface { Write(p []byte) (n int, err error) } type Closer interface { Close() error } type Seeker interface { Seek(offset int64, whence int) (int64, error) } 基于这 4 个基础 interface，两两组合，有扩展了下面几个 interface</description>
    </item>
    <item>
      <title>Golang Context 的实现</title>
      <link>https://runzhen.github.io/posts/golang-context/</link>
      <pubDate>Sun, 28 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-context/</guid>
      <description>有这样一个场景： 父 goroutine 创建了多个子 goroutine 来处理某个请求，当这些子 goroutine 中任何一个出错的时候，我们希望所有的 goroutine 都停止。 该如何实现呢？
熟悉 Go 语言的可能首先想到用 context，而 context 主要是依靠 channel 来实现以上功能。
看了一下具体的实现，主要思想是:
每种类型的 ctx 都实现了 context.Context 接口的 Done() 函数 Done() &amp;lt;-chan struct{} 函数返回一个只读的 channel 而且没有地方向这个channel里写数据。所以直接调用这个只读channel会被阻塞。 一般通过搭配 select 来使用。一旦 channel 关闭，就会立即读出零值。 谁来关闭这个 channel 呢？ 用户主动调用返回的 CancelFunc，或者 timeout 超时 另外，在使用上配合 select 语句阻塞处理 Done() 才能起到预期的效果。
下面举两个如何使用 context 的例子，第一个例子如下
func main() { ctx, cancel := context.WithTimeout(context.Background(), 1*time.Second) defer cancel() go handle(ctx) // 等待3秒再结束（只是为了让 main 不提前 exit，与本文无关） select { case &amp;lt;- time.</description>
    </item>
    <item>
      <title>Kubernetes 中的 DNS </title>
      <link>https://runzhen.github.io/posts/k8s-kubedns-coredns/</link>
      <pubDate>Wed, 10 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/k8s-kubedns-coredns/</guid>
      <description>该文件指定如何解析主机名
cat /etc/host.conf order hosts, bind multi on order bind,hosts 指定主机名查询顺序，这里规定先使用 DNS 来解析域名，然后再查询 /etc/hosts 文件(也可以相反) multi on 指 /etc/hosts 文件中的主机可以有多个地址 nospoof on 指不允许对该服务器进行IP地址欺骗 </description>
    </item>
    <item>
      <title>Rust Ownership and Lifetime</title>
      <link>https://runzhen.github.io/posts/rust-ownership-lifetime/</link>
      <pubDate>Sun, 28 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/rust-ownership-lifetime/</guid>
      <description>所有权 在 Rust 中，heap 上的一块内存区域是一块 “值”，与之绑定的是一个变量，也就是说变量和值是绑定的，要注意这种绑定关系。 在任何时候，一个值只有一个对应的变量作为所有者。
理解了这些概念之后，再来看所有权和它的基本特性：
Rust中的每个值都有一个对应的变量作为它的所有者； 在同一时间内，只有且仅有一个所有者； 当所有者离开自己的作用域时，它持有的值就会被释放掉。 所有权的转移 赋值即转移。 如下面的示例，
fn test() { let v: Vec&amp;lt;u8&amp;gt; = vec![0;20]; let u = v } 在第二行，u 成为了内存中这个数组数据的所有者，当函数返回时，u 的作用域结束，这块内存随即被释放。
要想让 v 和 u 各自都拥有独立的数据，可以使用 v.clone() 函数，
注意，int, char 等基本类型，在赋值的时候等于自动调用了 clone，所以对于这些基本类型可以放心的像 C/C++ 语言那样使用。
所有权的借用 &amp;amp; 是一个在 C/C++ 和 Golang 中常见的符号，在 Rust 中，用在一个变量上是借用的意思，也就是说所有权不变。
官方文档用这样一个例子来说明借用
fn main() { let s1 = String::from(&amp;#34;hello&amp;#34;); let len = calculate_length(&amp;amp;s1); println!(&amp;#34;The length of &amp;#39;{}&amp;#39; is {}.&amp;#34;, s1, len); } fn calculate_length(s: &amp;amp;String) -&amp;gt; usize { s.</description>
    </item>
    <item>
      <title>Golang 读写锁的实现</title>
      <link>https://runzhen.github.io/posts/golang-rw-lock/</link>
      <pubDate>Thu, 18 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-rw-lock/</guid>
      <description>type RWMutex struct { w Mutex // held if there are pending writers writerSem uint32 // semaphore for writers to wait for completing readers readerSem uint32 // semaphore for readers to wait for completing writers readerCount int32 // number of pending readers readerWait int32 // number of departing readers } writerSem 是写入操作的信号量 readerSem 是读操作的信号量 readerCount 是当前读操作的个数 readerWait 当前写入操作需要等待读操作解锁的个数 其中 semaphore 就是操作系统课程里面学到的信号量的概念。
读写锁的实现非常简单，源码在 /usr/local/go/src/sync/rwmutex.go 下，我们可以逐一分析它的各个函数
读者加读锁 首先是读锁，读者进入临界区之前，把 readerCount 加一，
如果这个值小于 0，则调用runtime_SemacquireMutex 把自己所在的 goroutine 挂起。 如果大于等于 0， 则加读锁成功 func (rw *RWMutex) RLock() { if atomic.</description>
    </item>
    <item>
      <title>epoll 在 Golang net 库的使用</title>
      <link>https://runzhen.github.io/posts/golang-net-epoll/</link>
      <pubDate>Sun, 31 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-net-epoll/</guid>
      <description>本文主要关注以下几个问题:
Golang runtime 是怎么调用 epoll 的系统调用的 ？ Golang net 库如何封装 epoll，使得开发者几乎不用直接操作 epoll ? C 如何调用 epoll 首先回顾一下用 C 语言怎么使用 epoll
int s = socket(AF_INET, SOCK_STREAM, 0); bind(s...) listen(s...) int epfd = epoll_create(128); //创建eventpoll对象 ev.events = _EPOLLIN | _EPOLLOUT | _EPOLLRDHUP | _EPOLLET epoll_ctl(epfd, EPOLL_CTL_ADD, s, &amp;amp;ev);//注册事件 //轮询就绪事件 while(true){ //返回值n为就绪的事件数,events为事件列表 int n = epoll_wait(epfd, &amp;amp;events[0], len(events), 1000) for( i := 0; i &amp;lt; n; i++ ) { ev := &amp;amp;events[i] //处理事件 } } C 语言中调用 epoll 的方式比较底层，总的来说分下面三个步骤</description>
    </item>
    <item>
      <title>gRPC-go Server 端实现</title>
      <link>https://runzhen.github.io/posts/grpc-go-server-code/</link>
      <pubDate>Mon, 18 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/grpc-go-server-code/</guid>
      <description>在上一篇文章中，介绍了 grpc 建立 TCP 连接的过程，侧重点在 Client 端，而关于 Server 端建立 TCP 的过程相对是比较简单的。
Server端 listen on 本地端口，并且接收来自 client 的连接请求，一旦建立 TCP 连接后，接下来的步骤是什么呢？ 建立 HTTP2 server，并收发数据。
本文尝试回答一下几个问题：
Server 怎么利用 http2 的 stream 传输数据？ 从 stream 里读的数据存放在哪？ Stream 读到的数据如何传给用户 Server 要发送的数据又是从哪发送的？ 创建 http2Server 首先从用户的代码入手，用户的代码最后会调用 grpcServer.Serve(lis), 稍微追踪几个函数就能发现调用链是 handleRawConn() 到 serveStreams()。
从 handleRawConn() 中我们发现 newHTTP2Transport 会创建一个新的 http2Server。
serveStreams() 中的 HandleStreams() 是 type ServerTransport interface 的一个函数，而 type http2Server struct 实现了这个接口。
值得注意的是，有两个结构体实现了 ServerTransport，分别是
transport/handler_server.go 的 serverHandlerTransport transport/http2_server.go 的 http2Server 一般我们在 main 函数中调用 grpcServer.</description>
    </item>
    <item>
      <title>Bittorrent 协议及工作原理</title>
      <link>https://runzhen.github.io/posts/how-bt-torrent-works/</link>
      <pubDate>Sat, 24 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/how-bt-torrent-works/</guid>
      <description>在 2000 年左右开始接触互联网的同学都应该记得用 BT 种子下载电影和小电影那段的时光。之前只是大概知道 BT 的工作原理，但并没有仔细研究过，所以一直很好奇。
随便在网上搜索下，可以知道 BT 大概是这样工作的:
BitTorrent 协议把提供下载的文件虚拟分成大小相等的块，块大小必须为 2k 的整数次方，并把每个块的索引信息和 Hash 验证码 写入 .torrent 文件（即种子文件，也简称为“种子”）中，作为被下载文件的“索引”。 下载者要下载文件内容，需要先得到相应的 .torrent 文件，然后使用 BT 客户端软件进行下载。
下载时，BT 客户端首先解析 .torrent 文件得到 Tracker 地址，然后连接 Tracker 服务器。Tracker 服务器回应下载者的请求，提供下载者其他下载者（包括发布者）的 IP。或者，BT客户端也可解析 .torrent 文件得到 nodes 路由表，然后连接路由表中的有效节点，由网络节点提供下载者其他下载者的 IP。
torrent 文件包含了什么 根据 bittorrent.org官方文档，种子文件也被称为metainfo files, 主要包含以下信息：
announce, The URL of the tracker. info, This maps to a dictionary. 所以种子文件就是告诉你，去 announce 这个地址找文件，具体文件信息包含在 info 里面。
Info 结构体有以下基本内容：
name key maps to a UTF-8 encoded string.</description>
    </item>
    <item>
      <title>Docker 的 privileged 模式</title>
      <link>https://runzhen.github.io/posts/docker-privileged/</link>
      <pubDate>Sun, 18 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/docker-privileged/</guid>
      <description>无论是 docker 启动一个 container 还是在 k8s 中 deploy 一个 Pod 都可以指定 privileged 参数，之前在 Pod 的 spec YAML file 也里曾经用过，但是一直没有仔细想过加上这个参数后有什么不一样，今天就来研究一下。
首先来看一个最直观的对比，先运行一个没有 privileged 的容器：
$ docker run --rm -it ubuntu:18.04 bash root@e6f5f42c5b7e:/# ls /dev/ console core fd full mqueue null ptmx pts root@e6f5f42c5b7e:/# fdisk -l 再来看看如果加上了 privileged 会有什么不一样：
$ docker run --rm -it --privileged ubuntu:18.04 bash root@8e28f79eec9e:/# ls /dev/ tty11 tty2 tty28 tty36 tty44 tty52 tty60 ... ... root@8e28f79eec9e:/# fdisk -l Disk /dev/loop0: 97.</description>
    </item>
    <item>
      <title>Goroutine Pool 实现高并发</title>
      <link>https://runzhen.github.io/posts/goroutine-pool/</link>
      <pubDate>Tue, 13 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/goroutine-pool/</guid>
      <description>本文是读完 Handling 1 Million Requests per Minute with Go 之后，根据自己的理解，对文中提到的并发模型和实现再梳理一遍。
前言 假设有一个 http server 接收 client 发来的 request，如果用下面的这样的代码，会有什么问题呢？
func payloadHandler(w http.ResponseWriter, r *http.Request) { // Go through each payload and queue items individually to be posted to S3 for _, payload := range content.Payloads { go payload.UploadToS3() // &amp;lt;----- DON&amp;#39;T DO THIS } } 显而易见，有 2 个问题：
接收一个 request 就开启一个 goroutine 处理，当 request 数量在短时间内暴增的话，光是 goroutine 的数量都足以让 server 崩溃。 每个 goroutine 都会与后端建立 TCP 连接，既耗费三次握手的时间，也会造成后端有大量 TCP 连接 所以，我们的目标是 没有蛀牙</description>
    </item>
    <item>
      <title>How gRPC-go set up the TCP connection</title>
      <link>https://runzhen.github.io/posts/grpc-client-tcp-connection/</link>
      <pubDate>Sun, 11 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/grpc-client-tcp-connection/</guid>
      <description>首先看一个最简单的建立 client server 之间 gRPC 连接的代码，以这个代码为例，分析一下 TCP 是在何时建立的。
Server 端的代码相对来说很容易，一个最简单的 server 代码如下：
func main() { lis, _ := net.Listen(&amp;#34;tcp&amp;#34;, fmt.Sprintf(&amp;#34;:%d&amp;#34;, 8080)) grpcServer := grpc.NewServer() protobuf.RegisterTestServer(grpcServer, &amp;amp;server{}) grpcServer.Serve(lis) } 在 grpc/server.go 中的 Serve() 函数调用了 lis.Accept() 并阻塞，当 client 端发来 TCP 请求时，Accept() 返回 Conn 结构，并开启 goroutine handleRawConn() 进行后续的处理。
就 TCP 来说，server 端的代码简单易懂，相比之下 client 端则不一样，一个基本的 Client 代码如下：
func main() { conn, err := grpc.Dial(&amp;#34;localhost:8080&amp;#34;, grpc.WithInsecure()) defer conn.Close() cli := protobuf.NewTestClient(conn) } 而要弄清楚 Client 端如何建立 TCP 却不容易，这是因为 grpc client 有 resolve DNS 以及做 load balancer 的功能，因此代码复杂很多。</description>
    </item>
    <item>
      <title>Spanner Distributed Database 阅读</title>
      <link>https://runzhen.github.io/posts/spanner-paper-reading/</link>
      <pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/spanner-paper-reading/</guid>
      <description>Introduction Spanner 数据库中的数据是分片(Shard)分散存储在多个数据中心的，数据是用 Paxos 算法(状态机)保证一致性。如果数据中心发生变化，Spanner 自动做 reshard。
Spanner 中的数据是存储在 schematized semi-relational table 上的。 在 commit 的时候把 timestamp 作为数据的 version。老版本的数据可以被垃圾回收，client 也可以读老数据。
Spanner 有两个特性是一般的分布式系统非常难实现的，
读和写操作的外部一致性。 在一个时间戳下，读操作是全局一致的。 能实现以上两点，是因为 Spanner 可以分配全球范围内保持一致的 commit timestamp。Spanner 的 timestamp 靠 TrueTime API 实现，甚至可以用 GPS 和原子钟来提高 TrueTime API 的精度。
Implementation 一个 Spanner 集群被称为一个 universe，如下图所示
Spanner 被组织成许多个 zone 的集合，zone 是管理部署的基本单元。一个数据中心可能会有多个 zone，zone 也是物理隔离的单元，例如，两个不同应用的数据就会被分散在两个 zone 上。
Zonemaster 把数据分配给 spanserver，spanserver 是真正存数据的地方 Client 从 location proxy 定位数据在哪个 spanserver 上 Universe master 主要是一个管理界面 Placement driver 周期性的与 spanserver 交互，进行负载均衡 Spanserver Software Stack 每一台 spanserver 上会存 100 到 1000 张表，每一张表上都有一个 Paxos 状态机，每一个 Paxos 状态机都会把自己的 metadata 和 log 存在 tablet 上。</description>
    </item>
    <item>
      <title>Kubernetes 的 Volume 和 StorageClass</title>
      <link>https://runzhen.github.io/posts/kubernetes-volume-pv-pvc/</link>
      <pubDate>Mon, 28 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/kubernetes-volume-pv-pvc/</guid>
      <description>Kubernetes 的 Pod 可以 mount 很多种 Volume，常见的 volume 有
emptyDir hostPath configMap, secret persistentVolumeClaim nfs, gitRepo, cephfs, iscsi, cinder 等 其中，emptyDir 是最简单的一种，用于挂载一些临时文件，比如同一个 Pod 中两个 container 需要通过 unix socket 通信，那么把 socket 放在 emptyDir 中是最简单的方法。
甚至可以指定把这个抽象的目录放在内存，从而加快速度。
volumes: - name: html emptyDir: medium: Memory hostPath 是把数据直接存在 kubernetes 某个 worker node 上，这种方法一般不推荐使用，因为当 Pod 被调度到其他节点上后，数据就丢失了。
那么什么样的情况适合挂载 hostPath 呢？一些系统级的组件，需要挂载 node 上系统本身自带的一些文件时，比如需要读取 host 的 cert 目录，或者 etc 目录。常见的有 kube-system 空间下的 coreDNS 组件等。
当 Pod 中的程序需要把数据持久化到外部存储时，最推荐的用法是先在系统中定义 StorageClass，然后配合 persistentVolumeClaim (PVC) 和 persistentVolume (PV) 一起动态的分配空间。</description>
    </item>
    <item>
      <title>Kubernetes Headless Service</title>
      <link>https://runzhen.github.io/posts/kubernetes-headless-service/</link>
      <pubDate>Sun, 20 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/kubernetes-headless-service/</guid>
      <description>问题起源于我用 envoy 对 grpc 做 Layer 7 负载均衡的时候，发现 traffic 永远被转发到了一个特定的 Pod，显然是配置出错了。
环境如下：
同一个 namespace 下部署了 2 个 grpc server，一个 envoy
$ kubectl get pod NAME READY STATUS RESTARTS AGE grpc-envoy-7684f49cb-9fv4h 1/1 Running 0 4h49m grpc-server-668bdd6576-2bvkz 1/1 Running 0 4h51m grpc-server-668bdd6576-tqzj4 1/1 Running 0 4h51m envoy 的 service 配置如下
apiVersion: v1 kind: Service metadata: name: grpc-envoy namespace: default labels: app: grpc-envoy spec: type: NodePort ports: - name: grpc port: 8080 targetPort: grpc nodePort: 30061 protocol: TCP selector: app: grpc-envoy grpc-server 的 service 配置如下</description>
    </item>
    <item>
      <title>Kubernetes Pod 中的 Pause 容器</title>
      <link>https://runzhen.github.io/posts/k8s-pod-pause-container/</link>
      <pubDate>Sun, 13 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/k8s-pod-pause-container/</guid>
      <description>在一个运行 kubernetes 的节点上，我们能看到很多名叫 “pause” 的 container。比如
$ sudo docker ps | grep pause a4218d1d379b k8s.gcr.io/pause:3.1 &amp;#34;/pause&amp;#34; a2109bf3f0db k8s.gcr.io/pause:3.1 &amp;#34;/pause&amp;#34; 57cfa42e95d3 k8s.gcr.io/pause:3.1 &amp;#34;/pause&amp;#34; 仔细观察一下不难发现，每一个 Pod 都会对应一个 pause container。
在查阅了网上的一些资料以后，我总结了一下它大概有两个作用，
它是 Pod 中第一个启动的 container ，由它创建新的 linux namespace，其他 container 启动后再加入到这些 namespace 中。 在 Pod 的环境中充当 init process 的角色，它的 PID 是 1，负责回收所有僵尸进程。 说个题外话，在 docker 中，一个 container 启动时，Dockerfile 的 ENTRYPOINT 中指定的命令会成为这个 container 的 init process，PID 为 1.
顺便来看一下 pause 容器的实现，一共只有几十行 C 语言代码
static void sigdown(int signo) { psignal(signo, &amp;#34;Shutting down, got signal&amp;#34;); exit(0); } static void sigreap(int signo) { while (waitpid(-1, NULL, WNOHANG) &amp;gt; 0) ; } int main(int argc, char **argv) { if (getpid() !</description>
    </item>
    <item>
      <title>Unicode 字符编码</title>
      <link>https://runzhen.github.io/posts/unicode-utf8-encode/</link>
      <pubDate>Sun, 16 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/unicode-utf8-encode/</guid>
      <description>ASCII 我们熟悉的 ASCII 码可以说是字符编码的始祖了。它规定了常用的数字、符号、英文字母与二进制之间的对应关系。
ASCII 的缺点是字符集太少了，只能表示英文和数字，无法表示像中文，日文这样的符号。因此人们就设计出了 Unicode 字符集，囊括了几乎所有人类语言文字的符号。
Unicode Unicode 是一个字符集，而不是一种编码方式。 Unicode 相当于是给人类所有的符号一个独一无二的 ID，只要大家都是用这个 ID 表示字符，就不会出现乱码的问题。
因为 Unicode 是一个字符集，因此它不存在所谓的 “用几个字节表示 unicode” 这样的问题，这是具体的编码方式需要处理的事。
Unicode 把 ID 划分成了 17 组 (Plane)，每组有 65536 个字符，编号可以用 U+[XX]YYYY 这样的形式表示，每一位是一个十六进制数字，其中 XX 代表组编号，从 0 到 0x10，一共17个，YYYY 代表这一组中的字符编号，一共 65536 个。
其中第 0 组叫 Basic Multilingual Plane，简称 BMP，它是 Unicode 中最基础和最常用的一部分，码点范围是U+0000 ~ U+FFFF，包含了我们常用的英文和汉字。
UFT-8 UTF-8 是 Unicode 具体的编码方式，除此之外还要 UTF-16, UTF-32 等等。
为什么需要编码方式呢？ 直接用 Unicode 的 ID 不就行了吗？ 因为我们需要节省存储空间。
UTF-8 是一种变长的编码方式，它可以使用 1-4 个字节表示一个符号，编码规则如下</description>
    </item>
    <item>
      <title>Kubernetes Dashboard 添加 Auth</title>
      <link>https://runzhen.github.io/posts/k8s-dashboard-auth/</link>
      <pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/k8s-dashboard-auth/</guid>
      <description>组里的 k8s cluster Dashboard 一直没有设置登录，导致所有可以 ping IP 的人都可以登录管理界面，这样显然是不合理的，应该要设置几个不同权限的账户，并且开启 Dashboard 的 Basic Auth。
关于开启 Dashboard Basic Auth 网上有不少资料，但是在实际操作中还是遇到几个莫名其妙的坑。
创建用户文件 根据实际需求，并不需要使用 LDAP 等等复杂的登录方式，我们只需要一个 admin 账户，再加上低权限的只读 view 账户，以及有修改权限的 edit 账户就足够。
因此，只要添加 /etc/kubernetes/pki/basic_auth_file 文件即可。
vi /etc/kubernetes/pki/basic_auth_file password,username,1 需要注意的一个坑是用户名密码的顺序是反过来的（如上面所示），否则在 dashboard 上怎么输入都提示不对。
修改 API Server 配置 修改 /etc/kubernetes/manifests/kube-apiserver.yaml 加入一个启动参数
vim /etc/kubernetes/manifests/kube-apiserver.yaml - --basic-auth-file=/etc/kubernetes/pki/basic_auth_file 这里又遇到一个坑，在我们的 k8s Master 节点这个目录下有两个文件，第一个叫 kube-apiserver.yaml， 另一个是 kube-apiserver_xxx.yaml。
本来我以为第一个是实际的配置文件，第二个应该是其他人配置时 copy 的一个备份。
然而实际并不是，真正被使用的是第二个文件，这点让人匪夷所思，我花了好久才发现这个坑，但是我始终没找到哪里指定了让 api server 读取 kube-apiserver_xxx.yaml 而不是 kube-apiserver.yaml 。
重启 API Server 很多资料上都会把重启 api server 一笔带过，但是都不写具体怎么操作，k8s 上并不是简单删除一个 pod 就算重启的。</description>
    </item>
    <item>
      <title>在虚拟机中使用 GPU 计算</title>
      <link>https://runzhen.github.io/posts/nvidia-gpu-pass-through/</link>
      <pubDate>Sat, 13 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/nvidia-gpu-pass-through/</guid>
      <description>本文介绍如何在 Linux 虚拟机中直接使用 GPU 做科学计算，要达到这个目的，需要满足下面几个条件：
物理主机使用 VMWare ESXi 作为虚拟化的 VMM，并且版本最好大于等于 6.5 使用的是 Nvidia GPU 的显卡 Linux 虚拟机 OS 没有限制，我使用的是 ubuntu ESXi 开启显卡直通 假设已经安装好了 ESXi，通过 WebUI 进入 Host 的 Manage 界面，点击 Hardware，如图
把 nVidia 开头的这几个全部选中，然后 “Active”， 表示开启 PCI 设备的直通 (passthrough)。
重启物理主机。
配置虚拟机 创建一个新虚拟机，或者修改已有的虚拟机，
点击 Edit，VM Options ，在 Advanced 里面点击 Edit configuration 。
增加一条配置参数 hypervisor.cpuid.v0, 对应的值为 FALSE，这一步的目的是让驱动把虚拟机当做物理机来处理。
另一需要修改的地方让虚拟机硬件配置内存大小下面勾选 “Reserve all guest memory (All locked)”，让虚拟机启动时一次性获取物理主机内存，而不是按需获取。
到这里，主机和虚拟机的配置就全部完成了，接下来是驱动软件的安装
虚拟机安装驱动 重启并进入虚拟机 CLI，首先可以确认一下 GPU 已经被直通给了虚拟机，这一步不是必须要做，但检查一下没坏处。
$ lshw | grep display $ sudo apt install ubuntu-drivers-common $ ubuntu-drivers devices 接下来，禁用系统自带的开源 nouveau 驱动</description>
    </item>
    <item>
      <title>Kubeflow 部署 MNIST</title>
      <link>https://runzhen.github.io/posts/mnist_step_by_step/</link>
      <pubDate>Sun, 22 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/mnist_step_by_step/</guid>
      <description>在阅读本文之前，假设已经在 GCP 上安装好了 Kubeflow。
首先进入 Kubeflow，点击 Notebook Server，新建一个 Jupyter Notebook。
新建的时候会让你输入 Name 和 Namespace，在 Kubeflow 中，每个用户都在 k8s 集群上有自己的 Namespace。
这里输入的 Name 对应的 Notebook Pod 最后会在自己的 Namespace 下。
新的 Notebook 里面是空的，我们需要下载一些例子。打开 terminal
然后输入 git clone 命令：
git clone https://github.com/kubeflow/examples.git
回到默认界面会看到刚刚 clone 的项目，打开 mnist 目录下的 mnist_gcp.ipynb
开始 首先第一个问题，当我打开这个 Jupyter Notebook 的 WebUI 时，它运行在哪里？
Notebook 是在哪个 Pod $ kubectl -n Your-namespace get pod NAME READY STATUS RESTARTS AGE fairing-builder-chvkq-6s4cn 0/1 Completed 0 3d23h mnist-model-7886dcbb5b-t2kk8 1/1 Running 0 3d22h mnist-tensorboard-774c585b7c-65766 2/2 Running 0 21h mnist-train-2596-chief-0 0/1 Completed 0 3d22h mnist-train-2596-worker-0 0/1 Completed 0 3d22h mnist-ui-7f95c8498b-xqsfs 2/2 Running 0 3d22h test1-0 2/2 Running 0 3d23h test1-0 是之前在 UI里面创建 Notebook server 时定下的名字，于是test1-0</description>
    </item>
    <item>
      <title>[Istio] 使用 istio 控制转发流量</title>
      <link>https://runzhen.github.io/posts/use-istio/</link>
      <pubDate>Sun, 15 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/use-istio/</guid>
      <description>概念 首先介绍几个概念，Ingress 指的是进入到 k8s 集群中的 traffic，比如一个 client 发起的 HTTP 请求，经过层层网络最终到达了 k8s cluster 的外部，那么让不让它进入到 cluster 内部就是 ingress controller 做的事。
Kubernetes 原生提供了自己的 Ingress Controller，此外还有很多第三方的 Ingress Controller，istio 就是其中之一。需要注意的是，本文所有部署都是基于 GKE，其他的云平台可能略有不同。
在 k8s 中安装了 istio 之后，就可以用 istio 来控制所有进入 cluster 的流量。如何安装 istio 不在本文的范围，读者可以参考 istio 官方文档。
安装 istio 完成之后，kubectl get namespace 命令可以看到有个名叫 istio-system 的空间，所有 istio 组件的 pod 都在这个空间中。
然后用如下命令查看 istio ingressgateway
$ kubectl -n istio-system get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT istio-ingressgateway LoadBalancer 10.0.23.180 35.222.xxx.xxx 15020:32011/TCP,80:30444/TCP 我们会看到这个名叫 istio-ingressgateway 的 LoadBalancer 它有公网 IP 地址 35.</description>
    </item>
    <item>
      <title>用 Grafana 展示监控状态</title>
      <link>https://runzhen.github.io/posts/grafana-dashboard/</link>
      <pubDate>Sun, 02 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/grafana-dashboard/</guid>
      <description>运维或者 SRE 部门经常会弄一个大屏幕展示各种系统状态，看上去很好玩，于是我也用类似的开源软件监控一下家里的主机。
整个过程非常简单，主要是安装三个软件 Node exporter，Prometheus，Grafana。
Node exporter 既然要展示系统状态，那么第一步就是要获得系统的状态数据，比如 CPU 使用率，内存使用率，网络流量等。
Prometheus 官方提供了一个使用 go 语言编写的程序 node_exporter，直接下载项目主页上 release 里的二进制即可。node_exporter 最好直接安装在物理主机上，因为这样才能采集到最准确的数据。
运行 node_exporter 以后，会自动启动一个 http server 并且监听 9100 端口，如果有 client 过来访问， server 返回主机的监控信息。比如：
$ curl http://localhost:9100/metrics node_network_transmit_packets_total{device=&amp;#34;veth126cb08&amp;#34;} 28859 node_network_transmit_packets_total{device=&amp;#34;veth1276a16&amp;#34;} 1383 node_network_transmit_packets_total{device=&amp;#34;veth749c501&amp;#34;} 1.108492e+06 返回信息的格式是符合 Prometheus 定义的标准的，因此 Prometheus 能够处理并以简单的图标的形式展现这些数据。
看到这里大家应该不难想到，如果我自己写一个程序 HelloWorld，并且把程序的状态按照一定的格式导出，那么同样可以通过 Prometheus + Grafana 展现。
Prometheus Prometheus 是一个功能齐全的数据库，还提供了 PromSQL 语言方便用户查询，以及一个简单的网页前端。
最简单快捷的方式当然是启动一个容器，唯一需要注意的是把配置文件 prometheus.yml 挂载到容器的 /etc/prometheus/ 目录下。
$ docker run -d -p 9090:9090 \ -v /home/prometheus/:/etc/prometheus/ prom/prometheus 配置文件中需要在 scrape_configs 部分添加 noder exporter 的 IP 地址和端口。</description>
    </item>
    <item>
      <title>Kubernetes 部署 Tensorflow Serving</title>
      <link>https://runzhen.github.io/posts/kubernetes-tensorflow-serving/</link>
      <pubDate>Mon, 20 Jan 2020 20:22:33 -0800</pubDate>
      <guid>https://runzhen.github.io/posts/kubernetes-tensorflow-serving/</guid>
      <description>本文记录如何把 TensorFlow ResNet 模型部署在本地 Kubernetes 集群上，并提供一个 grpc 端口供集群外部访问。
本文不牵涉 ResNet（Deep residual networks）模型的实现细节，只讨论部署。
本文来源于 TensorFlow 官网上的一个例子，但正如大多数项目的文档一样，文档落后于项目的发展，因此有一些小坑，这里记录一下。
下载 ResNet 模型数据 这一步没什么好说的，按照步骤下载就行了
mkdir /tmp/resnet curl -s http://download.tensorflow.org/models/official/20181001_resnet/savedmodels/resnet_v2_fp32_savedmodel_NHWC_jpg.tar.gz | \ tar --strip-components=2 -C /tmp/resnet -xvz 制作并启动 ResNet serving 因为我们要把这个 serving 部署到 k8s，所以制作 docker 镜像是必须的。
先启动运行一个空 serving 镜像：
docker run -d --name serving_base tensorflow/serving 然后把刚刚下载的 /tmp/resnet 文件夹下的所有内容拷贝到容器中：
docker cp /tmp/resnet serving_base:/models/resnet 最后，commit 生成一个自己的 image
docker commit --change &amp;#34;ENV MODEL_NAME resnet&amp;#34; serving_base resnet_serving docker kill serving_base docker rm serving_base 然后我们试着运行一下这个镜像，要是看到类似如下输出，证明启动正常。</description>
    </item>
    <item>
      <title>Kubernetes DNS</title>
      <link>https://runzhen.github.io/posts/kubernetes-dns/</link>
      <pubDate>Sun, 19 Jan 2020 21:47:47 -0800</pubDate>
      <guid>https://runzhen.github.io/posts/kubernetes-dns/</guid>
      <description>在介绍 Kubernetes 中的 DNS 之前，我们先来看看 Kubernetes 中的另一个概念 Service，以及为什么需要 Service。
什么是 Service 我们知道 k8s 集群中应用的部署是以 Pod 为单位的，在 Pod 内执行 ifconfig 可以看到每个 Pod 都有自己的 IP，这个 IP 在集群内部是唯一的，其他 Pod 都能 ping 这个地址。
这样的设计使得 Pod 里的应用程序可以直接交互。
另一方面，Pod 的生命周期是短暂的，因此 Pod IP 也是不断变化的，而且 Pod 也会有多个副本。
那么问题来了： 其他程序访问这个 Pod 时该用哪个 IP 地址呢 ？
这个时候就需要 Service 出场了， Service 对外只会提供一个 IP，一个请求到来时 Service 决定该转发到后面哪一个 Pod 上。
我们可以理解为 Service 加上的一组 Pod 可以看做是一个微服务。
Service 对外提供 ClusterIP, NodePort 等访问方式。
Kubernetes DNS 上面提到 Service 对外提供一个唯一的 IP，但这个 IP 偶尔也会随着 service 的更新改变，所以还需要一个 k8s 集群内部的 DNS 把服务对应到 IP 上。</description>
    </item>
    <item>
      <title>安装 Kubernetes Dashboard</title>
      <link>https://runzhen.github.io/posts/kubernetes-dashboard/</link>
      <pubDate>Sat, 18 Jan 2020 23:07:24 -0800</pubDate>
      <guid>https://runzhen.github.io/posts/kubernetes-dashboard/</guid>
      <description>Kubernetes Dashboard 是一个 Web UI 的集群管理工具。项目主页在这里
首先根据它的主页上 README 里面的内容直接 kubectl apply -f recommended.yaml，这样集群中就会创建并运行 dashboard 的 POD。
接下来的问题是如何从外界访问到这个 UI。
我的 k8s 集群环境是一台物理主机上的三台虚拟机，每个虚拟机都是 Headless 启动，也就是说纯命令行没有桌面环境，无法打开浏览器，因此项目主页上说的 kubectl proxy 访问 http://localhost:8001 的方式不适用。
我希望最终能从物理主机上访问到这个 WebUI。
使用 NodePort 我们查看一下 kubernetes-dashboard 使用 recommended.yaml 部署之后 service 的类型是 ClusterIP
$ kubectl get svc --all-namespaces NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.96.0.1 &amp;lt;none&amp;gt; 443/TCP 17d default redis-nodeport NodePort 10.96.39.42 &amp;lt;none&amp;gt; 6379:31250/TCP 15d kube-system kube-dns ClusterIP 10.96.0.10 &amp;lt;none&amp;gt; 53/UDP,53/TCP,9153/TCP 17d kubernetes-dashboard dashboard-metrics-scraper ClusterIP 10.</description>
    </item>
    <item>
      <title>本地搭建三节点 Kubenetes</title>
      <link>https://runzhen.github.io/posts/kubenetes-cluster-from-scratch/</link>
      <pubDate>Sat, 28 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/kubenetes-cluster-from-scratch/</guid>
      <description>赶在 2019 年快结束之际，写一篇博客作为本年度的收官之作吧。
前言 虽然现在各大云平台厂商都提供了一键搭建 kubenetes 的服务，但缺点是费用太贵了，如果仅仅把它作为自己没事折腾的小玩具非常不划算，另外虽然也可以用公司的账号，但并不想把自己的折腾的东西跟工作混为一谈。
所以决定自己在主机上手动搭建一个。
准备工作 一般 kubenetes 至少要三台 linux 主机组建成一个 cluster，因为手头没有三台 linux 物理主机，所以要用虚拟机代替。
kubenetes 各个虚拟机节点的规划如下：
主机名 主机 IP OS 集群角色 192-168-56-10.master 192.168.56.10 Ubuntu 18.04 master 192-168-56-11.node 192.168.56.11 Ubuntu 18.04 node1 192-168-56-12.node 192.168.56.12 Ubuntu 18.04 node2 准备工作主要为下面几步：
物理主机内存 16G，操作系统为 Ubuntu 18.04 安装 Virtualbox 6.1 创建三台虚拟机，每台内存 4G 对三台虚拟机做基本配置 VirtualBox 虚拟机配置 默认只有一个 NAT 适配器，我们需要添加一个 Host-Only Adapter。NAT 适配器是虚拟机用来访问互联网的，Host-Only 适配器是用来虚拟机之间通信的。上面表格所指的 主机 IP 也是这个 Host only IP。
vbox 上配置一个 host only adapter， 默认名字 vboxnet0，网络地址 192.</description>
    </item>
    <item>
      <title>CPU affinity</title>
      <link>https://runzhen.github.io/posts/taskset/</link>
      <pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/taskset/</guid>
      <description>CPU affinity &amp;ndash; CPU 亲和性，指进程更希望运行在哪个 CPU core 上。
指定 core 有什么好处呢？
比如，可以自己决定哪些程序可以独占 CPU 资源，保证这个程序性能的最大化； 指定 CPU 以后可以提高 Cache 的命中率，常用于一些对性能非常高要求的程序，例如 nginx。 命令行指令 taskset 在 Linux 系统中，我们可以用 taskset 命令指定一个进程运行在哪个核心上。
比如我们写一个程序用 while(1) 制造死循环，那么运行这个程序的时候 CPU 会飙到 100%
用以下这条命令运行这个程序
taskset -c 3 ./a.out 意思是把 a.out 运行在从 0 开始数起的第 3 个核心上。
于是，用 htop 命令查看，会看到第 4 个核 CPU 使用率是 100%。
编程的 API 那么在程序的代码里怎么用呢？ 先来看看 glibc 提供的系统 API
#include &amp;lt;sched.h&amp;gt; int sched_setaffinity(pid_t pid, unsigned int cpusetsize, cpu_set_t *mask); int sched_getaffinity(pid_t pid, unsigned int cpusetsize, cpu_set_t *mask); void CPU_CLR(int cpu, cpu_set_t *set); int CPU_ISSET(int cpu, cpu_set_t *set); void CPU_SET(int cpu, cpu_set_t *set); void CPU_ZERO(cpu_set_t *set); nginx 的 config 文件中，可以为每个工作进程绑定CPU</description>
    </item>
    <item>
      <title>从 Kubernetes 中访问 Memorystore</title>
      <link>https://runzhen.github.io/posts/gke-access-memorystore/</link>
      <pubDate>Sun, 25 Aug 2019 14:49:42 -0700</pubDate>
      <guid>https://runzhen.github.io/posts/gke-access-memorystore/</guid>
      <description>Memorystore 是 Google Cloud 在 2018 年推出的托管 Redis 服务，让用户一键生成 Redis 实例，必要的时候再一键 scale，省去了维护 Redis 的烦恼。
本文在 k8s 中部署一个简单的小程序访问 Memorystore 数据库，获取 counter 值，并开启一个 http server 对外提供这个值。
准备 GCP 提供了一个命令行工具 gcloud，几乎所有的 web 操作都有对应的 CLI，非常方便。不同操作系统对应的安装包可以在这里下载
我的笔记本就叫它 “local host”，安装好 gcloud 之后，以下所有的操作都在 local 进行，命令执行的结果直接部署到 cloud 中。
现在开始前期准备工作。首先，在 GCP web 界面一键创建 Memorystore，之后我们能在 MemoryStore 的 Instances 里面看到这个实例，它的 IP 地址是 10.0.16.3 端口 6379。
很显然，10.0.16.3 这个 IP 是无法直接访问的，而如果你在相同的 GCP Project 里面创建了一个 VM instance，GCP 会自动创建一条路由，让你的 VM 可以 telnet 10.0.16.3 6379。
然后，创建一个 k8s 集群，这一步也同样可以在 web 界面里做，如果要用 GCP 提供的 gcloud 命令行的话如下：</description>
    </item>
    <item>
      <title>Docker 中编译 vim8.0</title>
      <link>https://runzhen.github.io/posts/build-your-vim8/</link>
      <pubDate>Fri, 16 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/build-your-vim8/</guid>
      <description>vim 的最新版 vim8.0 提供了很多新的特性，而且一些流行的 vim 插件很多功能也依赖于 8.0 版本，如果你要使用 vim8.0，那么最好的办法当然是使用操作系提供的软件包管理器一键安装，省时省力。
但是总有那么一些蛋疼的情况 ——你需要自己编译 vim。
本文就是记录下具体的步骤，并且把编译源码时需要安装的依赖软件全部做成 docker 镜像。
事情起因 某台服务器上我要用 vim8.0 的新特性，但是在服务器上我没有任何超级权限，只能读写我自己的 home 目录。
所以没法直接安装vim，只能从源码编译。
制作编译 vim 的docker image 编译 vim 需要系统中安装很多依赖软件，比如 vim 最基本的要包括 python2.7，luajit 等。
都 2019 年了，最好的方式当然是制作一个 docker 镜像，具体的步骤就不一一解释，贴上 Dockerfile 以示诚意。
FROM ubuntu:18.04 RUN apt-get update &amp;amp;&amp;amp; apt-get install -y \ liblua5.1-dev \ luajit \ libluajit-5.1 \ python-dev \ ruby-dev \ libperl-dev \ libncurses5-dev \ libatk1.0-dev \ libx11-dev \ libxpm-dev \ libxt-dev \ gnupg2 \ curl \ &amp;amp;&amp;amp; gpg2 --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDB \ &amp;amp;&amp;amp; curl -sSL https://get.</description>
    </item>
    <item>
      <title>How TCP backlog works in Linux</title>
      <link>https://runzhen.github.io/posts/tcp-backlog/</link>
      <pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/tcp-backlog/</guid>
      <description>原文： http://veithen.io/2014/01/01/how-tcp-backlog-works-in-linux.html
当一个应用程序使用 listen() 系统调用把一个 socket fd 设置成 LISTEN 状态时，也需要指定一个 backlog 值。通常我们可以认为这个 backlog 代表这个 socket fd 可以接受最大的连接请求数。
#include &amp;lt;sys/types.h&amp;gt; #include &amp;lt;sys/socket.h&amp;gt; int listen(int sockfd, int backlog); 因为 TCP 的三次握手，在 server 端 accept() 系统调用返回，并且tcp 状态在变成 ESTABLISHED 之前，会有一个短暂的 SYN RECEIVED 状态。那么这个状态的 tcp 链接应该放在哪个 queue 里面呢？
单个 queue，其大小就是 listen() 参数 backlog。当一个 SYN 包到达时，server 返回一个 SYN/ACK 给 client，并且把这个链接放入 queue。当 client 的 ACK 到达时，TCP 的状态变成 ESTABLISHED。这就意味着这一个 queue 有两种不同的状态：SYN RECEIVED 和 ESTABLISHED。只有在 ESTABLISHED 状态的链接才能被 accept() 返回给用户程序。</description>
    </item>
    <item>
      <title>minikube, 单机版 kubernetes</title>
      <link>https://runzhen.github.io/posts/ubuntu-install-k8s/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/ubuntu-install-k8s/</guid>
      <description>本来想在我的 linux 主机上创建 3 个虚拟机，然后手工搭建一个拥有 3 个节点的 k8s 集群。
但是翻了翻网上的各种教程，发现每个教程都是巨复杂，给我一种 “即使我跟着教程千辛万苦敲完所有命令，也不一定能运行” 的感觉。最后，我发现了 minikube 这个东西，可以方便的搭建一个单机版 k8s。
麻雀虽小五脏俱全，即便是这样一个简单的 k8s，目前也足够我学习一些基本知识了。
本文记录一下安装 minikube 的具体步骤，并在 k8s 中部署一个简单的服务。
安装 minikube 开局一张图，先展示一下 minikube 的整个架构。
首先是准备工作，更新系统，安装必要组件。
sudo apt-get update sudo apt-get install apt-transport-https sudo apt-get upgrade 然后安装 virtualbox，
sudo apt install virtualbox virtualbox-ext-pack 安装 minikube
wget https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 chmod +x minikube-linux-amd64 sudo mv minikube-linux-amd64 /usr/local/bin/minikube 添加 kubectl 源
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - 值得注意的是，我的主机是 ubuntu 18.04，代号 bionic，而安装的源却是 xenial，对应 ubuntu 16.</description>
    </item>
    <item>
      <title>Golang 操作共享内存</title>
      <link>https://runzhen.github.io/posts/share-memory-golang/</link>
      <pubDate>Fri, 02 Aug 2019 00:33:33 -0700</pubDate>
      <guid>https://runzhen.github.io/posts/share-memory-golang/</guid>
      <description>前言 进程间通信的方式有很多种，如果两个进程分别在不同的机器上，那么使用 socket 通信；如果在同一台机器上，共享内存机制是一种快速高效的方式。
本文实现一个 go 语言二进制程序和 C 语言二进制程序通过共享内存交换数据。
提到共享内存主要有两种：
System V 标准的 shmget/shmdt 等接口 POSIX 标准的 shm_open 等接口 另外 Linux 下 mmap() 匿名映射也是最常用的进程间共享内存方法。
创建了共享内存以后，一般会显示在系统的 /dev/shm 目录下。Linux 默认 /dev/shm 为实际物理内存的1/2, 比如我的机器上物理内存为 16G，运行 df 命令后可以看到 /dev/shm 的大小为 7.8G 。
$ df -h Filesystem Size Used Avail Use% Mounted on tmpfs 1.6G 3.2M 1.6G 1% /run tmpfs 7.8G 4.0K 7.8G 1% /dev/shm tmpfs, ramfs 和 ramdisk
tmpfs是一个虚拟内存文件系统，在Linux内核中，虚拟内存资源由物理内存(RAM)和交换分区组成，Tmpfs可以使用物理内存，也可以使用交换分区。
ramdisk 是一个块设备，只不过它是存在于内存上的。
ramfs 也是文件系统，不过已经被 tmpfs 替代了。</description>
    </item>
    <item>
      <title>ASLR 内核虚拟地址随机化</title>
      <link>https://runzhen.github.io/posts/address-space-layout-randomize/</link>
      <pubDate>Sun, 07 Jul 2019 14:01:40 -0700</pubDate>
      <guid>https://runzhen.github.io/posts/address-space-layout-randomize/</guid>
      <description>ASLR 全称 Address Space Layout Randomization，是一项 Linux 内核的安全措施，使应用程序每次加载到内存后，函数地址都不同。
试用一下 先来直观的感受下什么是 ASLR。目前大多数 linux 系统都默认开启了这个选项，可以用一下两个命令确认一下系统是否支持 ASLR。
$ cat /proc/sys/kernel/randomize_va_space 2 $ sysctl kernel.randomize_va_space kernel.randomize_va_space = 2 其中 0 表示关闭，1 表示有约束的随机，2 表示完全随机化。
然后随便找一个可执行程序，用 ldd 命令显示它加载的动态链接库，可以看到两次运行 ldd 结果各个库的地址不一样。
$ ldd /bin/sleep linux-vdso.so.1 (0x00007ffd49764000) libc.so.6 =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f02783ae000) /lib64/ld-linux-x86-64.so.2 (0x00007f02789a8000) $ ldd /bin/sleep linux-vdso.so.1 (0x00007ffc10996000) libc.so.6 =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f12c3534000) /lib64/ld-linux-x86-64.so.2 (0x00007f12c3b2e000) 应用程序如何使用 ASLR 在这篇文章中提到，除了 kernel 开启以外，应用程序在编译的时候也必须添加编译选项 gcc -fPIE -pie test.c 。
但是在我的实际测试中，似乎并不需要额外添加编译选项，看来 gcc 默认开启了 ASLR。</description>
    </item>
    <item>
      <title>Orphan, Zombie and Docker</title>
      <link>https://runzhen.github.io/posts/orphan-zombie-and-docker/</link>
      <pubDate>Sun, 16 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/orphan-zombie-and-docker/</guid>
      <description>孤儿进程的产生 孤儿进程： 父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。通常，孤儿进程将被进程号为1的进程(进程号为 1 的是 init 进程)所收养，并由该进程调用 wait 对孤儿进程收尸。
#include &amp;lt;stdio.h&amp;gt; #include &amp;lt;stdlib.h&amp;gt; #include &amp;lt;errno.h&amp;gt; #include &amp;lt;unistd.h&amp;gt; int main() { pid_t pid; pid = fork(); if (pid == 0) { printf(&amp;#34;I&amp;#39;m child process, pid:%d ppid:%d\n&amp;#34;, getpid(), getppid()); sleep(5); printf(&amp;#34;I&amp;#39;m child process, pid:%d ppid:%d\n&amp;#34;, getpid(), getppid()); } else { printf(&amp;#34;I&amp;#39;m father process, pid:%d ppid:%d\n&amp;#34;, getpid(), getppid()); sleep(1); printf(&amp;#34;father process is exited.\n&amp;#34;); } return 0; } 运行结果如下所示：
I&amp;#39;m father process, pid:25354 ppid:13981I&amp;#39;m child process, pid:25355 ppid:25354father process is exited.</description>
    </item>
    <item>
      <title>LSM Tree 简介</title>
      <link>https://runzhen.github.io/posts/lsm-tree-basic/</link>
      <pubDate>Sun, 09 Jun 2019 00:33:33 -0700</pubDate>
      <guid>https://runzhen.github.io/posts/lsm-tree-basic/</guid>
      <description>LSM Tree 是 Log Structured Merge Tree 的缩写，这种 Tree 数据结构的特点就是”Log Structured“ 和 ”Merge“。LSM Tree 主要用在各种新兴的数据库，作为底层数据结构。 提供了比 B+ 树/ISAM 更好的写性能。
本文是一篇读书笔记，作为以后再次阅读的提纲，此外，原文旁征博引有不少参考资料，值得一读。 原文传送门。
Some Background 核心： 硬盘（无论是磁盘，SSD 甚至是内存）随机读写性能太差，但是顺序读写性能非常高，所以要充分利用一点。
这篇文章指出，磁盘的顺序访问甚至比内存的随机访问还快！
所以，如果我们对写性能要求高，怎么办？ 一种方法是写数据的时候只 append（添加在文件尾部）。通常我们把这种叫做写日志，logging。
但是，简单的 log 结构无法满足复杂的需求，为了满足类似搜索、kv 之类的场景，需要在 logging 基础上加上额外的数据结构，比如 binary search, hash, B+ or external。
Search Sorted File: save data to a file, sorted by key. If data has defined widths use Binary search. If not use a page index + scan. Hash: split the data into buckets using a hash function, which can later be used to direct reads.</description>
    </item>
    <item>
      <title>Call Rust Function from C Code</title>
      <link>https://runzhen.github.io/posts/call-rust-functions-from-c/</link>
      <pubDate>Sat, 15 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/call-rust-functions-from-c/</guid>
      <description>看到 rust 可以编译成动态链接库（.so），想到是不是可以用 C 语言链接到这个库呢？答案是肯定的。Rust 提供了 FFI 接口，即 Foreign Function Interface，目的就是和其他语言交互。
废话不多说，开始干。我们要实现三个例子：
C 调用 Rust 动态库 C 调用 Rust 静态库 Rust 调用 C 函数 (不是库) C 调用 Rust 动态库 Rust 部分 首先是用 cargo new NAME --lib 创建一个新项目，然后编辑 src/lib.rs
#![crate_type = &amp;#34;dylib&amp;#34;] #[no_mangle] pub extern fn double_input(input: i32) -&amp;gt; i32 { println!(&amp;#34;hello --from rust shared library&amp;#34;); input * 2 } crate_type = &amp;ldquo;dylib&amp;rdquo; 代表编译成动态链接库。
no_mangle 告诉 rust 编译器，不要擅自改变下面这个函数的函数名。一些高级语言比如 c++ 之类，为了防止不同库中的函数名冲突，都会在编译时给每个函数生成独一无二的函数名，比如 func::h485dee。</description>
    </item>
    <item>
      <title>Lua 语法知识点记录</title>
      <link>https://runzhen.github.io/posts/lua-programming-basic/</link>
      <pubDate>Sun, 09 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/lua-programming-basic/</guid>
      <description>table {:toc} Lua 是一门小巧的编程语言，但麻雀虽小五脏俱全，而且与 C 语言的交互非常友好，所以有人称它是 “胶水语言”。最近在研究 nginx，另一个广泛应用的、基于 nginx 的开源项目 OpenResty 就是把 lua 嵌入到了 nginx 中，很有意思。于是就来学习一下 lua。
基本语法 单行注释用 --，多行注释用
--[[多行注释--]] 数据类型 nil 表示一个无效值（在条件表达式中相当于false）。 boolean 包含两个值：false和true。 number 表示双精度类型的实浮点数 string 字符串由一对双引号或单引号来表示 function 由 C 或 Lua 编写的函数 userdata 表示任意存储在变量中的C数据结构 thread 执行协同程序 table 表或者数组 table 类型 在 Lua 里，table 的创建是通过&amp;quot;构造表达式&amp;quot;来完成，最简单构造表达式是{}，用来创建一个空表。
local tbl1 = {}local tbl2 = {&amp;#34;apple&amp;#34;, &amp;#34;pear&amp;#34;, &amp;#34;orange&amp;#34;, &amp;#34;grape&amp;#34;} 另外，数组的索引可以是数字或者是字符串。比如有以下代码：
a = {}a[&amp;#34;key&amp;#34;] = &amp;#34;value&amp;#34;key = 10a[key] = 22a[key] = a[key] + 11 最后 table a 中的内容是 (10, 33) 和 (key, value)</description>
    </item>
    <item>
      <title>nginx HTTP 的 11 个阶段</title>
      <link>https://runzhen.github.io/posts/nginx-http-11-phases/</link>
      <pubDate>Sun, 09 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/nginx-http-11-phases/</guid>
      <description>nginx 源码的特点是用了很多回调函数，阅读起来非常麻烦，因为不知道当前这个 hanlder 到底对应哪个函数。
在正式开始研究这 11 个阶段之前，我们先看几个结构体，然后再看 ngx_http_core_run_phases() 函数，希望能更快的理解这些 phase 是怎么 run 的。
ngx_http_core_main_conf_t 回顾一下 ngx_http_core_main_conf_t，在前面的博客中已经介绍过，它还有两个兄弟 ngx_http_core_srv_conf_t 和 ngx_http_core_loc_conf_t。
ngx_http_core_main_conf_t 中有两个成员是本文比较关心的： phase_engine 和 phases。
typedef struct { ngx_array_t handlers; } ngx_http_phase_t; typedef struct { // 所有的http请求都要使用这个引擎处理 ngx_http_phase_engine_t phase_engine; // http handler模块需要向这个数组添加元素 ngx_http_phase_t phases[NGX_HTTP_LOG_PHASE + 1]; } ngx_http_core_main_conf_t; 配置解析后的 postconfiguration 里向cmcf-&amp;gt;phases数组添加元素，phases数组存放了所有的phase，其中每个元素是ngx_http_phase_t类型的，表示的就是对应的phase handler的数组。ngx_http_core_main_conf_t-&amp;gt;phases数组主要用于handler的注册。
ngx_http_phase_engine_t typedef struct { ngx_http_phase_handler_t *handlers; ngx_uint_t server_rewrite_index; ngx_uint_t location_rewrite_index; } ngx_http_phase_engine_t; ngx_http_phase_handler_t struct ngx_http_phase_handler_s { ngx_http_phase_handler_pt checker; ngx_http_handler_pt handler; ngx_uint_t next; }; 看完了相关数据结构，特别是看到 checker、handler 的时候，是不是突然觉得熟悉了？没错，这就是上一篇博客 http 请求处理流程中，最后的 run core phase。</description>
    </item>
    <item>
      <title>nginx HTTP 链接建立过程</title>
      <link>https://runzhen.github.io/posts/nginx-http-init-connection/</link>
      <pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/nginx-http-init-connection/</guid>
      <description>ngx_http_init_connection() 负责建立 http 链接的是 ngx_http_init_connection()， 我们先来看一下谁会调用这个函数。
ngx_http_optimize_servers() 是一个负责合并配置项的函数，在有关 http 配置项解析的博客中曾经提到过，它会调用 ngx_http_add_listening()。
继续看 ngx_http_add_listening(), 看到 ls-&amp;gt;handler = ngx_http_init_connection;
即 ngx_http_init_connection() 作为一个回调函数，被设置在了 listening 结构体的 handler 中。
那么它是从哪儿被调用的呢？ 最简单的方法就是打印一下backtrace
#0 ngx_http_init_connection (c=0x7ffff7fa90f8) #1 ngx_event_accept (ev=0x73c3a0) #2 ngx_epoll_process_events (cycle, timer, flags) #3 ngx_process_events_and_timers (cycle=cycle@entry=0x716860) #4 ngx_single_process_cycle (cycle=cycle@entry=0x716860) #5 main (argc=&amp;lt;optimized out&amp;gt;, argv=&amp;lt;optimized out&amp;gt;) 原来是在 epoll 中由事件触发的，正所谓事件驱动。现在知道了它的调用者，那么继续看 init connection 具体做了哪些事。
我们先忽略对 ipv6、ssl 的处理，直接看最简单的情况，ssl 相关后续博客再分析。精简后的代码如下：
void ngx_http_init_connection() {c-&amp;gt;data = hc; // data 是专门存数据的地方，之后使用// 拿到以前提到很多次的 ngx_http_conf_ctx_t hc-&amp;gt;conf_ctx = hc-&amp;gt;addr_conf-&amp;gt;default_server-&amp;gt;ctx;// 连接的读事件，rev 是 ngx_event_t 类型rev = c-&amp;gt;read;// 处理读事件，读取请求头// 设置了读事件的 handler，可读时就会调用 handlerrev-&amp;gt;handler = ngx_http_wait_request_handler;// 前面把读事件加入epoll，当socket有数据可读时就调用 // ngx_http_wait_request_handler// 同时因为事件也加入了定时器，超时时也会调用 handlerngx_handle_read_event(rev, 0);} ngx_http_wait_request_handler() 前面我们把 read 事件加入了 epoll，当 socket 有数据可读是就会调用本函数。又因为是事件触发，可能会被多次调用，即重入。</description>
    </item>
    <item>
      <title>nginx 启动流程之 ngx_init_cycle()</title>
      <link>https://runzhen.github.io/posts/nginx-init-cycle/</link>
      <pubDate>Sun, 02 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/nginx-init-cycle/</guid>
      <description>从 main() 函数开始之后，很快就调用到 ngx_init_cycle()，这是 nginx 源码中一个非常重要的函数，它负责调用所有模块的init_module函数指针，初始化模块，并且解析 nginx.conf 文件中的各种参数。所以在分析 nginx 启动流程的时候，必须搞清楚这个函数做了哪些工作。
首先函数的传入参数只有一个，ngx_cycle_t *old_cycle。
当 main() 函数调用 ngx_init_cycle() 时，因为是第一次启动 nginx，给的参数是一个刚刚初始化的变量，只填写了一些必要的信息； 另一个会调用ngx_init_cycle()是 ngx_master_process_cycle()。因为 nginx 支持动态加载 nginx.conf 文件，所以此时的传入参数就是当前的配置。 分析 ngx_master_process_cycle() 就会了解 nginx master 进程是如何等待、处理信号，并且启动新的 worker 进程的。
先来看看 ngx_cycle_t 结构体里的成员，这里只列出本文关心的几个:
struct ngx_cycle_s {// 存储所有模块的配置结构体，是个二维数组// 0 = ngx_core_module// 1 = ngx_errlog_module// 3 = ngx_event_module// 4 = ngx_event_core_module// 5 = ngx_epoll_module// 7 = ngx_http_module// 8 = ngx_http_core_modulevoid ****conf_ctx;// 保存模块数组，可以加载动态模块// 可以容纳所有的模块，大小是ngx_max_module + 1// ngx_cycle_modules()初始化ngx_module_t **modules;// 拷贝模块序号计数器到本cycle// ngx_cycle_modules()初始化ngx_uint_t modules_n;// 标志位，cycle已经完成模块的初始化，不能再添加模块// 在ngx_load_module里检查，不允许加载动态模块ngx_uint_t modules_used;} 其中最最重要的莫过于 conf_ctx 了，回顾上一篇博客里的图，重点关注一下其中 ngx_http_module，需要搞清楚这些配置是怎么解析到 conf_ctx 中去的。</description>
    </item>
    <item>
      <title>nginx HTTP 配置项的解析</title>
      <link>https://runzhen.github.io/posts/nginx-http-config/</link>
      <pubDate>Sun, 25 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/nginx-http-config/</guid>
      <description>在前面的两篇博客中我们看到，无论是实现一个 http 模块，或者是 http filter 模块，都需要实现模块自己的 ngx_http_module_t 结构体。
typedef struct {ngx_int_t (*preconfiguration)(ngx_conf_t *cf);ngx_int_t (*postconfiguration)(ngx_conf_t *cf);void *(*create_main_conf)(ngx_conf_t *cf);char *(*init_main_conf)(ngx_conf_t *cf, void *conf);void *(*create_srv_conf)(ngx_conf_t *cf);char *(*merge_srv_conf)(ngx_conf_t *cf, void *prev, void *conf);void *(*create_loc_conf)(ngx_conf_t *cf);char *(*merge_loc_conf)(ngx_conf_t *cf, void *prev, void *conf);} ngx_http_module_t; 其中 main、srv、loc 分别对应 nginx.conf 中的 http，server，location 配置块，本文就来关注一下这些配置项是如何被解析和使用的。
解析 http 不同级别配置项 一个简单的 nginx.conf 配置如下：
http {test_cmd;server {listen 80;test_cmd;location / {test_cmd;root html;index index.</description>
    </item>
    <item>
      <title>Rust Beginner&#39;s Notes</title>
      <link>https://runzhen.github.io/posts/rust-programming-language/</link>
      <pubDate>Wed, 21 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/rust-programming-language/</guid>
      <description>Rust 编程语言知识点笔记。
trait 关键字 Rust没有继承，它和Golang不约而同的选择了trait(Golang叫Interface)作为其实现多态的基础。
使用trait定义一个特征：
trait HasArea { fn area(&amp;amp;self) -&amp;gt; f64; } trait里面的函数可以没有函数体，实现代码交给具体实现它的类型去补充：
struct Circle { x: f64, y: f64, radius: f64, } impl HasArea for Circle { fn area(&amp;amp;self) -&amp;gt; f64 { std::f64::consts::PI * (self.radius * self.radius) } } fn main() { let c = Circle { x: 0.0f64, y: 0.0f64, radius: 1.0f64, }; println!(&amp;#34;circle c has an area of {}&amp;#34;, c.area()); } derive 属性 Rust提供了一个属性derive来自动实现一些trait，这样可以避免重复繁琐地实现他们，能被derive使用的trait包括：Clone, Copy, Debug, Default, Eq, Hash, Ord, PartialEq, PartialOrd。常用的例子是：</description>
    </item>
    <item>
      <title>nginx HTTP Filter 模块</title>
      <link>https://runzhen.github.io/posts/nginx-http-filter-module/</link>
      <pubDate>Mon, 19 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/nginx-http-filter-module/</guid>
      <description>过滤模块基本概念 普通的 HTTP 模块和 HTTP filter 模块有很大的不同。普通模块，例如上篇博客提到的 hello world 模块，可以介入 nginx http 框架的 7 个处理阶段，绝大多数情况下介入 NGX_HTTP_CONTENT_PHASE 阶段，特点是一旦介入了，那么一个 http 请求在这个阶段将只有这个模块处理。
http filter 模块则不同，一个请求可以被任意个 http 过滤模块处理，而且过滤模块 仅处理服务器发出的 HTTP 响应 header 和 body，不处理客户端发来的请求。
过滤链表的顺序 编译 nginx 的第一步是执行 configure 脚本生成 objs/ngx_modules.c 文件，这个文件中的 ngx_modules 数组会保存所有的 nginx 模块，包括普通的 http 模块和本文介绍的 http filter 模块。
nginx 在启动时初始化模块的顺序就是 nginx_modules 数组成员的顺序。因此，只要看 configure 命令生成的 ngx_modules.c 文件就可以知道所有 http 过滤模块的顺序。
对于 http 过滤模块来说，在 ngx_modules 数组中的位置越靠后，实际执行请求时就越先执行。因为在初始化 http 过滤模块时，每个过滤模块都是将自己插入到整个链表的首部。
开发一个 HTTP 过滤模块 一个简单的过滤模块实现这样的功能：在返回的 http response 中，先检查 header，如果是 200 OK，则在 body 中插入一段字符。如下所示：</description>
    </item>
    <item>
      <title>nginx 模块开发入门</title>
      <link>https://runzhen.github.io/posts/nginx-hello-world-module/</link>
      <pubDate>Sun, 18 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/nginx-hello-world-module/</guid>
      <description>编写模块 想要学习如何开发一个 nginx 模块，最快速简单的方法莫过于写一个 Hello World 模块，没错，还真有这么一个 nginx-hello-world-module，而且 nginx.org 官网还介绍了这个模块。
首先，对于所有的 nginx 模块来说，都需要实现一个 ngx_module_t 结构体，如下所示，需要特别注意的是，如果去看 module 结构体的定义，它与下面的代码并不是一一对应的，这是因为 NGX_MODULE_V1 宏把其他变量都赋值了，帮我们屏蔽了一些细节。 总而言之，开发一个 nginx 模块，我们跟着这个套路走就行了。
ngx_module_t ngx_http_hello_world_module = {NGX_MODULE_V1,&amp;amp;ngx_http_hello_world_module_ctx, /* module context */ngx_http_hello_world_commands, /* module directives */NGX_HTTP_MODULE, /* module type */NULL, /* init master */NULL, /* init module */NULL, /* init process */NULL, /* init thread */NULL, /* exit thread */NULL, /* exit process */NULL, /* exit master */NGX_MODULE_V1_PADDING}; 其中第一个变量和最后一个变量都是固定的，我们不需要关心。如果开发的是 HTTP 模块，那么 module type 那儿写上 HTTP 的宏就行了，都是固定死的操作。</description>
    </item>
    <item>
      <title>Wireshark/tcpdump 抓到的数据包可信吗？</title>
      <link>https://runzhen.github.io/posts/can-we-trust-wireshark/</link>
      <pubDate>Wed, 24 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/can-we-trust-wireshark/</guid>
      <description>table {:toc} 问题的表面现象 问题的背景是这样的：
一个应用程序监听某端口的 UDP 包，发送者发送的 UDP 包比较大，有 65535 个字节。显然，发送者的 UDP 包在经过 IP 层时，会被拆分层多个 1460 字节长度的 IP 分片，经过网络传输之后到达接收方，接收方的网卡收到包后，在内核协议栈的 IP 层又将分片重新组合，生成大的 UDP 包给应用程序。
正常情况下，应用程序能准确无误的收到大 UDP 包，偶尔系统网络流量十分巨大的时候，会丢个别 UDP 包 ——这都在允许范围内。
突然有一天，即便网络流量不是很大的时候，UDP 包的丢包十分严重，应用程序几乎收不到任何数据包。
开始 debug 遇到这样的问题，第一反应有两种可能：
网络不通，数据包没送到网卡。 个别 IP 分片在传输中丢失了，导致接收方无法重组成完整的 UDP 包。 于是用 tcpdump 在 interface eth0 上抓包，出乎意料的是，抓到的 pcap 有完整的 IP 分片。用 wireshark 打开 pcap，wireshark 会自动把 IP 分片重组成 UDP 数据包，检查这个 UDP 包，数据完整无误。
那么现在能断定是应用程序自己出了问题吗？
因为一直以来一个根深蒂固的想法是：既然抓到的 pcap 准确无误，所以数据包已经送到了接收方了，linux 内核只要把分片重组一下交给应用层就可以了，这个过程一般不会出错，所以应用程序没收到只能怪它自己咯？
实际导致问题的原因 最终查明的原因是这台 linux 系统上有两个参数被修改了，当 IP 分片数量过大时，内核中分配给重组的缓冲区已满，导致之后的分片都被丢弃了。</description>
    </item>
    <item>
      <title>nginx 文件锁、自旋锁的实现</title>
      <link>https://runzhen.github.io/posts/nginx-lock/</link>
      <pubDate>Tue, 18 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/nginx-lock/</guid>
      <description>在上一篇博客 Linux 共享内存以及 nginx 中的实现的示例中，我们看到每次多个进程同时对共享内存中的 count 加一，导致每次运行结果都不一样，那么解决的方法就是对临界区加锁了，所以本文就来研究一下 nginx 中的几种加锁方式。
文件锁 文件锁的原理就是在磁盘上创建一个文件（操作系统创建一个文件描述符），然后多个进程竞争去获取这个文件的访问权限，因此同一时刻只有一个进程能够访问临界区。
可以看出，进程并不会真正在这个文件中写什么东西，我们只是想要一个文件描述符 FD 而已，因此 nginx 会在创建了文件后把这个文件删除，只留下文件描述符。
多个进程打开同一个文件，各个进程看到的文件描述 FD 值可能会不一样。例如文件 test.txt 在 进程1 中是 101， 而在进程2中是 201
使用文件锁举例 使用文件锁主要用到两个 libc 提供的结构体和函数。
struct flock; 提供一些锁的基本信息，比如读锁 F_RDLCK, 还是写锁 F_WRLCK fcntl(): 对文件描述符进行操作的库函数。 那么如何用这个函数来实现锁的功能呢？
先看一个加锁的代码：
void mtx_file_lock(struct fdmutex *m) { struct flock fl; memset(&amp;amp;fl, 0, sizeof(struct flock)); fl.l_type = F_WRLCK; fl.l_whence = SEEK_SET; if (fcntl(m-&amp;gt;fd, F_SETLKW, &amp;amp;fl) == -1) { printf(&amp;#34;[-] PID %d, lock failed (%s).</description>
    </item>
    <item>
      <title>Linux 共享内存以及 nginx 中的实现</title>
      <link>https://runzhen.github.io/posts/share-memory/</link>
      <pubDate>Tue, 04 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/share-memory/</guid>
      <description>共享内存方法简介 Linux/Unix系统中，共享内存可以通过两个系统调用来获得，mmap 和 shmget/shm_open，其中 shmget 和 shm_open 分别属于不同的标准：
POSIX 共享内存（shm_open()、shm_unlink()） System V 共享内存（shmget()、shmat()、shmdt()） shmget 和 shm_open 类似的地方在于都是创建一个共享内存，挂载到 /dev/shm 目录下，并且返回一个文件描述符，fd。
区别是 POSIX 没有提供将 fd 映射到进程地址空间的方法，而 System V 方式则直接提供了 shmat()，之后再 nginx 的实现中会再次看到。
mmap 语义上比 shmget 更通用，因为它最一般的做法，是将一个打开的实体文件，映射到一段连续的内存中，各个进程可以根据各自的权限对该段内存进行相应的读写操作，其他进程则可以看到其他进程写入的结果。
而 shmget 在语义上相当于是匿名的 mmap，即不关注实体文件，直接在内存中开辟这块共享区域，mmap 通过设置调用时的参数，也可达到这种效果，一种方法是映射/dev/zero 设备,另一种是使用MAP_ANON选项。
mmap() 的函数原型如下，具体参数含义在最后的参考资料中给出。
void *mmap(void *addr, size_t len, int prot, int flags, int fd, off_t offset);
nginx 中的实现 nginx 中是怎么实现的呢？ 我们看一下源码 src/os/unix/ngx_shmem.c。
一目了然，简单粗暴有木有！ 分三种情况
如果mmap系统调用支持 MAP_ANON选项，则使用 MAP_ANON 如果1不满足，如果mmap系统调用支持映射/dev/zero设备，则映射/dev/zero来实现。 如果1和2都不满足，且如果支持shmget的话，则使用该shmget来实现。 #if (NGX_HAVE_MAP_ANON) ngx_int_t ngx_shm_alloc(ngx_shm_t *shm) { shm-&amp;gt;addr = (u_char *) mmap(NULL, shm-&amp;gt;size, PROT_READ|PROT_WRITE, MAP_ANON|MAP_SHARED, -1, 0); return NGX_OK; } #elif (NGX_HAVE_MAP_DEVZERO) ngx_int_t ngx_shm_alloc(ngx_shm_t *shm) { fd = open(&amp;#34;/dev/zero&amp;#34;, O_RDWR); shm-&amp;gt;addr = (u_char *) mmap(NULL, shm-&amp;gt;size, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0); return (shm-&amp;gt;addr == MAP_FAILED) ?</description>
    </item>
    <item>
      <title>使用 socketpair 实现进程间通信</title>
      <link>https://runzhen.github.io/posts/socketpair/</link>
      <pubDate>Tue, 28 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/socketpair/</guid>
      <description>socketpair 牛刀小试 int socketpair(int d, int type, int protocol, int sv[2]);第1个参数d，表示协议族，只能为 AF_LOCAL 或者 AF_UNIX；第2个参数 type，表示类型，只能为0。第3个参数 protocol，表示协议，可以是 SOCK_STREAM 或者 SOCK_DGRAM AF_UNIX 指的就是 Unix Domain socket，那么它与通常网络编程里面的 TCP socket 有什么区别呢？ 查阅了资料后发现：
Unix Domain socket 是同一台机器上不同进程间的通信机制。 IP(TCP/IP) socket 是网络上不同主机之间进程的通讯机制。 socketpair() 只支持 AF_LOCAL 或者 AF_UNIX，不支持 TCP/IP，也就是 AF_INET， 所以用 socketpair() 的话无法进行跨主机的进程间通信。
先看一个简单的示例：
int main() { int fd[2], retpid; int pid , status; char input[MAX_LEN]; if (socketpair(AF_UNIX, SOCK_STREAM, 0, fd) &amp;lt; 0) { printf(&amp;#34;call socketpair() failed, exit\n&amp;#34;); return -1; } pid = fork(); if (pid) { /* parent */ printf(&amp;#34;Parent process, pid = %d\n&amp;#34;, getpid()); while (1) { fgets(input, MAX_LEN, stdin); write(fd[0], input, MAX_LEN); } } else { /* child */ printf(&amp;#34;Child process, pid = %d\n&amp;#34;, getpid()); int nread = 0; while (1) { nread = read(fd[1], input, MAX_LEN); input[nread] = &amp;#39;\0&amp;#39;; printf(&amp;#34;Child: nread = %d, data = %s\n&amp;#34;, nread, input); } } retpid = wait(&amp;amp;status); if (retpid) { printf(&amp;#34;Parent: reap child process pid = %d\n&amp;#34;, retpid); } return 0; } 编译后运行，可以看到每次在终端输入信息，子进程都会回显到屏幕上。</description>
    </item>
    <item>
      <title>BPF -- Linux 中的 DTrace</title>
      <link>https://runzhen.github.io/posts/linux-bpf/</link>
      <pubDate>Thu, 09 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/linux-bpf/</guid>
      <description>记得 5 年前刚接触 perf 的时候，还特意调研了一下不同系统上的动态和静态追踪工具，知道了 Linux 上的 SystemTap，perf。Solaris 上的 DTrace。看到绝大多数资料都说 DTrace 多么的强大好用，但是 Linux 却没有与之相提并论的工具。
最近看到 BPF 这三个字被提及的很频繁，搜索了一下发现它号称 “Linux 中的 DTrace”， 于是试着玩了一下。
BPF 全称是 &amp;ldquo;Berkeley Packet Filter&amp;rdquo;，字面意思是包过滤器，那么问题来了：我一个包过滤器，怎么就成了追踪调试工具呢？ 这主要是因为一些历史的进程：原先开发 BPF 的目的是在内核重定向数据包，接着增加了对事件的追踪功能，然后又增加了基于时间的采样，于是久而久之 BPF 就成了一个功能强大的调试工具。
安装 首先，内核版本最好大于 4.9 ， 可以用 uname -a 命令查看。
其次，查看一下内核在编译的时候是否开启了 BPF 选项，一般在 /boot/ 目录下有对应内核版本的 config 文件，比如在我的机器上是 /boot/config-4.15.0-30-generic。 如果看到 CONFIG_BPF_SYSCALL=y 说明可以用 BPF 的基本功能。
前面提到 BPF 号称 Linux 中的 DTrace，为什么呢？ 因为 DTrace 包含了一个类似脚本语言的 D 语言，用户可以用简单的几句 D 语言完成复杂的调试追踪任务，这一点是 perf 做不到，而 BPF 做到了。
确认了内核支持 BPF 之后，我们可以安装一个叫做 bcc 的工具，通过它可以方便的使用 BPF。</description>
    </item>
    <item>
      <title>Aho–Corasick 算法，AC 自动机</title>
      <link>https://runzhen.github.io/posts/aho-ac-automaton/</link>
      <pubDate>Sat, 14 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/aho-ac-automaton/</guid>
      <description>AHO 算法，或者叫 AC 自动机、又或者叫 Aho–Corasick string matching algorithm，是一个高效的多模式匹配算法，它的特点是可以同时匹配多个模式串。
最常见的应用就是病毒扫描 : 把所有病毒的特征码（类似一段字符串）构造成一个 AC 自动机，把用户的文件或者网络数据流作为输入文本，只要在输入文本中找到了任何一个特征码，那么就表示有病毒存在。
对于这样的一个应用场景，我们最希望的功能就是只扫描一遍输入文本，找出所有的病毒，而 AC 自动机恰恰就有这样的能力。
一般使用 AC 自动机需要以下三步：
根据待匹配的字符串 P1, P2, Pn 建立 Trie 给 Trie 添加失败路径，实际上是生成了一个自动机。 将输入文本 str 的字符逐个通过自动机，在 O(n) 的时间复杂度内找出 P1, P2 &amp;hellip; Pn 是否存在于 str 内。 举例 假设我们有模式字符串 { fat, fare, hat, are }， 输入的文本为 “fatehatfare”。
显然，所有的模式串都出现在了我们的输入文本中。我们的目标就是只扫描一遍文本串，找出所有的模式串。
建立 Trie 首先根据模式串建立起一个 trie，一般来说每个节点的结构大概是这样：
Node {Node * children[26]; /* 指向子节点的指针 */Node * parent; /* 指向父节点的指针 */ Node * fail; /* 失败指针 */bool terminate; /* 是否是一个终结节点，即一个串的最后一个字符 */} Trie 建立好之后大概是这个样子：</description>
    </item>
    <item>
      <title>SO_REUSEPORT 和 epoll 的 Thundering Herd</title>
      <link>https://runzhen.github.io/posts/port-reuse/</link>
      <pubDate>Sat, 23 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/port-reuse/</guid>
      <description>SO_REUSEPORT 顾名思义就是重用端口，是指不同的 socket 可以 bind 到同一个端口上。 Linux 内核 3.9 版本引入了这个新特性，有兴趣的同学可以移步到这个链接查看更加详细的内容。 https://lwn.net/Articles/542629/
Reuse Port 我们先通过一段简单的代码来看看怎么使用这个选项（完整的代码在这里下载）。
int serv_sock = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP); // 一定要在 bind() 函数之前设定好 SO_REUSEPORT setsockopt(serv_sock, SOL_SOCKET, SO_REUSEPORT, &amp;amp;enable, sizeof(int)); bind(serv_sock, (struct sockaddr*)&amp;amp;serv_addr, sizeof(serv_addr)); listen(serv_sock, 20); accept(serv_sock, (struct sockaddr*)&amp;amp;clnt_addr, &amp;amp;clnt_addr_size); 将上面的代码编译生成两个可执行文件，分别启动运行，并监听相同的端口。
./port_reuse1 127.0.0.1 1234
再用 telnet/nc 等工具发送请求到 1234 端口上，多重复几次，会看到两个进程轮流的处理客户端发来的请求。
这里说一个题外话，上面的例子是手动启动两个进程。而我发现如果是进程自动 fork() 生成 2 个进程的话，似乎不用设置 SO_REUSEPORT 也能自动监听同一个端口。这是为什么？
Thundering Herd / 惊群现象 The thundering herd problem occurs when a large number of processes waiting for an event are awoken when that event occurs, but only one process is able to proceed at a time.</description>
    </item>
    <item>
      <title>如何拦截库函数调用 ? </title>
      <link>https://runzhen.github.io/posts/hook-syscall/</link>
      <pubDate>Sun, 03 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/hook-syscall/</guid>
      <description>LD_PRELOAD 环境变量 : 直接作用在可执行文件上 (准确的说是拦截库函数) 。 ptrace() : 拦截子进程的系统调用。 1. LD_PRELOAD LD_PRELOAD 的优势:
使用简单。 不需要修改被拦截程序的源码。 例如我想拦截程序 A 所有调用 malloc() 的地方，那么程序 A 不需要任何修改，只要准备好自己的 malloc() 函数，编译成动态链接库 .so 文件，然后在运行 A 之前先用 LD_PRELOAD 设定好环境变量就可以了。
LD_PRELOAD 的原理就是链接器在动态链接的时刻，优先链接 LD_PRELOAD 指定的函数。准确的说 LD_PRELOAD 拦截的是动态库中的函数，但是一般我们写的应用程序都是通过库函数来调用系统调用 API，所以 LD_PRELOAD 也间接的拦截了系统调用。
说到这里，LD_PRELOAD 的缺点也非常明显，它只能作用于动态链接库，要是静态链接的就没戏了。
腾讯的 C++ 协程库 libco，以及 tcmalloc 的 TC_MALLOC 都用到了这种方式。
2. ptrace() ptrace 是 linux 内核原生提供的一个功能，因此功能比 LD_PRELOAD 强大的多。它最初的目的是用来 debug，例如大名鼎鼎的 gdb 就是依赖于 ptrace。
要使用 ptrace 拦截程序 A 的系统调用，有两种方法：
ptrace 一个新进程：在代码中 fork 一个子进程，子进程执行 ptrace(PTRACE_TRACEME, 0, 0, 0)函数，然后通过 execv() 调用程序 A。 attach 到已运行的程序 A ：执行ptrace(PTRACE_ATTACH, pid, 0, 0)。 以上两种方式，ptrace 都会拦截发送到 A 进程的所有信号（除 SIGKILL 外），然后我们需要自己选择哪些系统调用需要拦截，并在拦截后转到我们自己的处理函数。</description>
    </item>
    <item>
      <title>容器化博客的编译环境</title>
      <link>https://runzhen.github.io/posts/dockerize-blog/</link>
      <pubDate>Wed, 16 May 2018 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/dockerize-blog/</guid>
      <description>前言 在上一篇博客 从 Wordpress 到 Jekyll 中，提到了把博客从原来的 Workpress 迁移到了 Jekyll, 开始用 Markdown 语法写博客正文，用 jekyll build 生成博客内容并部署到 Web 服务器。
最近这两天在考虑把 &amp;ldquo;jekyll build&amp;rdquo; 这个过程容器化，达到传说中 一次部署到处运行 的终极目标。 :-)
目标 废话不多说，开始折腾。 首先要制定一下具体的目标：
jekyll build 是将 Markdown 语法写成的纯文本文件生成 html 文件，这也是我希望将它容器化的部分，因为我并不希望再次搭建 jekyll 的环境，只要有一个稳定能用的编译环境就可以了。
生成了 html 文件以后，拷贝到 nginx 的目录下，这样就可以通过浏览器访问博客内容了，一般来说也可以将 nginx 运行在容器里面。
但是我目前在学习 nginx 源码，希望时不时的能把我修改过的 nginx 部署到博客上，因此，不容器化 nginx 。
步骤 首先去 docker hub 上找了一个可用的镜像 jekyll/jekyll，这个镜像已经部署好了一个基本的 jekyll 环境。
docker pull jekyll/jekyll:latest
然后启动这个镜像并进入到容器中。
这一步如果非常清楚需要安装哪些软件的话可以用 Dockerfile 代替完成。我在这里直接进入到容器中是因为并不清楚需要安装哪些依赖软件，需要一步一步 debug。
docker run --rm --volume=/LOCAL_DIR:/srv/jekyll -it jekyll/jekyll /bin/bash</description>
    </item>
    <item>
      <title>RFC 793 传输控制协议 TCP</title>
      <link>https://runzhen.github.io/posts/tcp-rfc793/</link>
      <pubDate>Mon, 12 Mar 2018 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/tcp-rfc793/</guid>
      <description>RFC 793 是 TCP 正式成为标准时的文档，虽然距今已有 30 多年的历史，并且已经多次被更新，但是要学习 TCP 这份文档仍然值得一读。
文本算是一个读书笔记，把阅读过程中我认为需要注意的部分记录下来，方便自己以后查漏补缺。后续也会有阅读其他相关 RFC 的读书笔记。
RFC 的第一二章节是按照惯例的声明，正式内容从 第三章 FUNCTIONAL SPECIFICATION 开始。
3.1 头部格式 IP 头部带有源地址和目地址等信息，这两者也同样会被 TCP 头部的某些字段使用（比如计算 checksum 的时候）。
TCP 头部格式如下：
0 1 2 30 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Source Port | Destination Port |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Sequence Number |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Acknowledgment Number |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Data | |U|A|P|R|S|F| || Offset| Reserved |R|C|S|S|Y|I| Window || | |G|K|H|T|N|N| |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Checksum | Urgent Pointer |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| Options | Padding |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+| data |+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+TCP Header Format Source Port，16位，源端口号</description>
    </item>
    <item>
      <title>博客十年的变迁: 从 Wordpress 到 Jekyll</title>
      <link>https://runzhen.github.io/posts/wordpress-to-jekyll/</link>
      <pubDate>Sun, 04 Mar 2018 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/wordpress-to-jekyll/</guid>
      <description>前言 印象中我的第一个博客是在大约2007年左右创建的，那个时候是个人博客非常流行的几年，各个网站都推出了自己的博客服务，如今已经是2018年，转眼间10年过去了，个人博客不如以前那么盛行了，现在流行的是微博自媒体和微信公众号。但是作为一个程序员，总觉得还是自己的博客比较 “geek”，所以一直还在折腾。
一开始的博客是建立在《电脑爱好者》网站推出的“博墅”服务上的，跟所有提供博客服务的网站一样，用户只要在网上点几下鼠标博客就建立好了。
然后就折腾更加高级的：自己买域名，买主机空间，自己建数据库，自己安装博客程序（也就是当时最流行的博客程序 WordPress）。
Wordpress 是我目前用的最长的博客系统了，从2009年左右一直到现在2018年，当初买的域名，买的主机空间，一直续费到现在，算一算有近10年了。
现在，准备把 wordpress 博客停用了，换成更 “geek” 的方式，用 Markdown 语法写内容，用 Jekyll 程序生成博客。生成的博客可以托管在 github pages 上，也可以放在云主机上。
博客源代码 博客的源码已经放在了我的 github 上，通过 github pages 服务这个博客已经可以通过浏览器访问了。
但是还是想在自己的云主机上也创建一个（太喜欢折腾了&amp;hellip;），因此同样的一个博客内容在 xxx.github.io 上和云主机上各有一份，我的博客域名 blog.nlogn.cn 随意指向其中一个（看我心情咯）。
Ubuntu 部署 Jekyll 1. 安装 ruby sudo apt install ruby-full ruby-bundler
2. 安装 jekyll sudo gem install jekyll bundler minima
3. git clone github.io 上博客源码。
4. jekyll build 博客 jekyll build --destination /usr/local/nginx/html/myblog
自动将生成的博客部署到指定目录下。至此，一个静态博客就部署完成了。
云主机上我用的是自己编译的 nginx 服务器，通过设置 nginx 的配置文件，指定一个针对域名 &amp;ldquo;blog.</description>
    </item>
    <item>
      <title>TCP 的 FIN_WAIT1 状态</title>
      <link>https://runzhen.github.io/posts/tcp-fin-wait/</link>
      <pubDate>Wed, 09 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/tcp-fin-wait/</guid>
      <description>最近看到了一篇有关 TCP 关闭连接时 FIN-WAIT1 状态的文章（见参考资料），觉得很有意思，于是也在自己的电脑上验证了一下。
首先，开局一张图：
{:height=&amp;ldquo;300&amp;rdquo; width=&amp;ldquo;300&amp;rdquo;}
FIN-WAIT1 状态出现在 主动关闭链接方发出 FIN 报文后，收到对应 ACK 之前。通常 Server 在收到 FIN 报文之后，会在很短的时间内回复 ACK（这个 ACK 可能携带数据，也可能只是一个纯 ACK），所以 FIN-WAIT1 状态存在的时间非常短暂，很难被观察到。
于是准备两台虚拟机，我们可以设计这样一个实验：
1）服务端监听 1234 端口：nc -l 1234
2）客户端连接服务端：nc 192.168.122.183 1234， 此时 TCP 的状态为 ESTABLISHED
$ sudo netstat -anp | grep tcptcp 0 0 192.168.122.167:60482 192.168.122.183:1234 ESTABLISHED 3712/nc 3）服务端配置 iptables，拦截从服务端发送到客户端的任何报文：iptables -A OUTPUT -d 192.168.122.167 -j DROP
4）客户端按下 ctrl + c 断开连接，这一步的目的是让操作系统自动发送 FIN 报文给服务端。
在完成第 4 步之后，客户端就会进入 FIN-WAIT-1 状态，服务端也会收到 FIN 报文，并且马上会发出一个 ACK，但是因为配置了 iptables，因此客户端会一直等待服务端的 ACK。</description>
    </item>
    <item>
      <title>TLS Perfect Forward Secrecy 之 DH/ECDHE</title>
      <link>https://runzhen.github.io/posts/pfs-ecdhe/</link>
      <pubDate>Tue, 21 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/pfs-ecdhe/</guid>
      <description>接着上一篇 TLS Perfect Forward Secrecy 之 RSA 缺陷 继续来看看 DH/ECDHE 如何解决这个问题。
前面提到 RSA 做密钥协商过程中，最关键的缺陷是客户端用公钥加密了 PreMaster Key，服务器用私钥解密 PreMaster Key。理想中更好的方法是 公钥/私钥只用来对证书签名，不参与到密钥协商的过程中来，换句话说，希望通信的双方能独立计算出对称加密的密钥。于是密码学家找到了尘封已久的，几乎与 RSA 同时出现的 DH 算法。
其实我一直有一个疑问：既然问题出在传送 PreMaster Key，那么客户端不发送 PreMaster Key 不就行了？握手的过程中已经有了 2 个随机数了，难道一定要 3 个随机数才能生成 master key 吗？
首先来看一下 DH 算法的数学基础。
+-------------------------------------------------------------------+| Global Pulic Elements || || p prime number || a prime number, a &amp;lt; p |+-------------------------------------------------------------------++-------------------------------------------------------------------+| User A Key Generation || || Select private Xa Xa &amp;lt; p || Calculate public Ya Ya = a^Xa mod p |+-------------------------------------------------------------------++-------------------------------------------------------------------+| User B Key Generation || || Select private Xb Xb &amp;lt; p || Calculate public Yb Yb = a^Xb mod p |+-------------------------------------------------------------------++-------------------------------------------------------------------+| Calculation of Secret Key by User A || || Secret Key K K = Yb^Xa mod p |+-------------------------------------------------------------------++-------------------------------------------------------------------+| Calculation of Secret Key by User B || || Secret Key K K = Ya^Xb mod p |+-------------------------------------------------------------------+ 上面一共出现了 a, p, Xa, Ya, Xb, Yb, K 共 7 个数，其中：</description>
    </item>
    <item>
      <title>TLS Perfect Forward Secrecy 之 RSA 的缺陷</title>
      <link>https://runzhen.github.io/posts/ssl-perfect-forward-secrecy/</link>
      <pubDate>Tue, 21 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/ssl-perfect-forward-secrecy/</guid>
      <description>HTTPS 这样的加密传输，最关键的是要解决两个问题，
一个是通信双方如何协商出一个对称加密的密钥（密钥交换） 二是自己如何确认对方的服务器就是我想访问的，即认证。 第一个问题可以由非对称加密解决，第二个问题是证书的合法性校验，“签发数字签名”，指的是用hash函数，对证书中的发行者，有效期，证书名等信息计算得到一个摘要（digest），然后用私钥进行加密，得到签名A。而对应的“校验数字签名”，则是先用相同的hash函数得到digest, 再利用相应的公钥解密A，然后对比自己hash出的和解密出的A是否相同，以此来认证对方是不是我们想要通信的人。
写到这里，正好最近有一个有趣的新闻：SHA1签名算法被破解，也就是说，安全研究人员根据理论上的SHA1破解算法，运用大量的计算资源，终于生成了两个不同的 PDF文件，但是它们的SHA1 值是一样的，所以以后我们应该使用更高级的签名算法来避免可能的安全漏洞。有兴趣的可以移步到上面的链接查看。
|算法 | 密钥交换 | 认证 ||--------|---------|---------||基于 RSA | RSA | RSA ||基于 DH | DH | RSA/DSA| 相比传统的 RSA 握手，ECDHE 能支持 forward secrecy(DH 算法本身没有forward secrecy，要 ECDHE 才行)。DH 算法让 client 和 server 分别独立的计算出同步加密的密钥，注意：是独立计算出，而不是通过一方传递给另一方。
加密套件，类似于 “ECDHE-ECDSA-AES256-SHA384” 这样的一串，主要包含这些信息：
密钥协商的算法，要么 RSA，要么 DH 认证方法，比如 SHA 同步加密的方法，即 session key 所属的类型 hash 函数，保证传输的用户数据的完整性 在探讨 Perfect Forward Secrecy 之前，先来回顾一下旧的 TLS 握手过程会有什么缺陷。
没有 PFS 的 RSA 握手过程 第一步，ClientHello 这一步，客户端向服务器提供以下信息：</description>
    </item>
    <item>
      <title>抓包分析 TLS 1.2 连接过程</title>
      <link>https://runzhen.github.io/posts/tls2-packets/</link>
      <pubDate>Sun, 19 Jun 2016 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/tls2-packets/</guid>
      <description>本文是《Traffic Analysis of an SSL/TLS Session》的笔记，原文见此处。
在 TLS 协议格式 中详细分析了协议数据的各个字段， 现在就来实战 ——用 Wireshark 抓包并观察数据。
第一发（Client -&amp;gt; Server） Full contents of the packet0000 02 00 00 00 45 00 00 98 13 ed 40 00 40 06 00 00 ....E.....@.@...0010 7f 00 00 01 7f 00 00 01 ec 26 01 bb 43 7c ee 74 .........&amp;amp;..C|.t0020 60 b5 50 0a 80 18 31 d7 fe 8c 00 00 01 01 08 0a `.</description>
    </item>
    <item>
      <title>TLS 1.2 协议格式</title>
      <link>https://runzhen.github.io/posts/tls-basic/</link>
      <pubDate>Sat, 18 Jun 2016 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/tls-basic/</guid>
      <description>本文内容来自《Traffic Analysis of an SSL/TLS Session》的阅读笔记，原文见此处。
TLS 全称为 Transport Layer Security，它本身也分为了 Lower 和 Higher 两层协议。
Lower Layer Lower Layer 协议在 TCP 协议之上，提供面向连接的可靠传输，在这一层的协议主要是记录协议（TLS Record Protocol）。 记录协议首先把上层协议传来的数据分成小于2^14字节的数据块，如果设置了压缩选项，则第二步为压缩数据，然后在数据块的末尾增加 MAC（Message Authentication Code），第三步将数据块加密，最后增加记录头。该过程如下所示。
each block is packed into a structure that does not preserve client message boundaries, meaning that mulitiple message of the same type maybe coalesced into a single structure
-----------+ data --+--------------&amp;gt; 1. Fragment data -----------+ +------------------------+ | | | | +------------------------+ 2. Compress data (generally no compression applied) +------------------------+----+ | |MAC | Add a Message Authentication Code | | | +------------------------+----+ 3.</description>
    </item>
    <item>
      <title>Linux 内核在 x86-64 上的内存分区</title>
      <link>https://runzhen.github.io/posts/zone-highmem/</link>
      <pubDate>Thu, 02 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/zone-highmem/</guid>
      <description>如果稍微了解过 Linux 内核的内存管理，那么对内存分区的概念一定不陌生，Linux内核把物理内存分成了3个区，
0 – 16M 为ZONE_DMA区, 16M – 896M 为ZONE_NORMAL区， 高于896M 为ZONE_HIGHMEM区 我没有去考证过为什么要取896这个数字，但是可以肯定的是这样的划分在当时看来是合理的，然而计算机发展今非昔比，现在4G的物理内存已经成为PC的标配了，CPU也进入了64位时代，很多事情都发生着改变。
在CPU还是32位的时代，CPU最大的物理寻址范围是0-4G， 在这里为了方便讨论，我们不考虑物理地址扩展（PAE）。进程的虚拟地址空间也是 4G，Linux内核把 0-3G虚拟地址空间作为用户空间，3G-4G虚拟地址空间作为内核空间。
目前几乎所有介绍Linux内存管理的书籍还是停留在32位寻址的时代，所以大家对下面这张图一定很熟悉！ （这个图画得非常详细，本篇文章我们关注的重点是 3个分区 以及最右边的线性地址空间，也就是虚拟地址空间之间的关系，另外，应该是ZONE_DMA， ZONE_NORMAL, ZONE_HIGHMEM, 图中把ZONE 写成了ZUNE）
然而，现在是64位的时代了, 64位CPU的寻址空间是多大呢？ 16EB， 1EB = 1024 TB = 1024 * 1024 GB，我想很多人这辈子还没见过大于1TB的内存吧，事实上也是这样，几乎没有哪个服务器能有16EB的内存，实现64位长的地址只会增加系统的复杂度和地址转换的成本，所以目前的x86_64架构CPU都遵循AMD的 Canonical Form, 即只有虚拟地址的最低48位才会在地址转换时被使用, 且任何虚拟地址的48位至63位必须与47位一致, 也就是说总的虚拟地址空间为256TB。
那么在64位架构下，如何分配虚拟地址空间的呢？
0000000000000000 – 00007fffffffffff(128TB)为用户空间, ffff800000000000 – ffffffffffffffff(128TB)为内核空间。 而且内核空间中有很多空洞, 越过第一个空洞后, ffff880000000000 – ffffc7ffffffffff(64TB) 才是直接映射物理内存的区域, 也就是说默认的PAGE_OFFSET为 ffff880000000000.
请关注下图的最左边，这就是目前64位的虚拟地址布局。
在本文的一开头提到的物理内存分区 ZONE_DMA， ZONE_NORMAL， ZONE_HIGHMEM 就是与内核虚拟地址的直接映射有关的，如果读者不了解 内核直接映射物理地址这个概念的话，建议你去google一下，这个很简单的一一映射的概念。
既然现在内核直接映射的物理内存区域有64TB， 而且一般情况下，极少有计算机的内存能达到64TB（别说64TB了，1TB内存的也很少很少），所以整个内核虚拟地址空间都能够一一映射到计算机的物理内存上，因此，不再需要 ZONE_HIGHMEM这个分区了，现在对物理内存的划分，只有ZONE_DMA， ZONE_NORMAL。
如果你想有个更加直观的了解的话，请打开 /boot/config*** 文件，例如我的是 /boot/config-3.</description>
    </item>
    <item>
      <title>一个简单的跳表(SkipList)实现</title>
      <link>https://runzhen.github.io/posts/skiplist/</link>
      <pubDate>Fri, 22 Feb 2013 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/skiplist/</guid>
      <description>跳表（skiplist） 是一个非常有趣的、简单的数据结构， 应用也非常广泛， 著名的NoSQL内存数据库Redis， 就用到了skiplist作为排序集合的基础数据结构。 跳表最大的特点就是插入、删除操作的性能均为O(logn) 。
关于它的原理网上有一大堆，如果不了解的话，可以先看看文章末尾的参考资料， 或者动手google一下。
需要指出的是，网上搜的一些的文章原理介绍的不错， 但是代码写的有点乱， 排版的时候也没有语法高亮， 所以我自己也参照着写了一份， 并且在这里简单的注释一下。
首先， 一个“跳表” 基本上长的是这个样子的：
再来看下抽象的数据结构：
#define object int typedef struct _node { int key; object *obj; struct _node *forward[1]; } node; typedef struct _skiplist { int level; struct _node *head; } skiplist; 值得注意的是， 结构体 node 的 forward 指针数组长度为1， 而我们从 skiplist 的长相和定义来看， forward 数组大小是随机的， 在 1 – MAX_LEVEL 之间， 因此， 一个技巧就是这样：
node *nd = (node *)malloc(sizeof(node) + level * sizeof(node *)); 每次创建新的节点的时候， 都必须用到上面这条语句。</description>
    </item>
    <item>
      <title>KVM 入门 (一) </title>
      <link>https://runzhen.github.io/posts/kvm-basic/</link>
      <pubDate>Sat, 15 Dec 2012 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/kvm-basic/</guid>
      <description>最近用一些零碎的时间学习KVM，算算大概也快有一个月了吧，进度还是很缓慢的，感觉该写一些类似读书笔记的东西了。欢迎大家来讨论。
KVM 即 Kernel Based Virtual Machine, 是一个内核模块，使用它需要CPU支持虚拟化。加载KVM模块后，系统中会有一个 /dev/kvm 设备，这个设备提供 ioctl 和 mmap操作。
目前，KVM还必须和修改过的Qemu配合起来使用（这样说是不准确的，因为已经有一个叫 native kvm tool的东东了），KVM 使用Qemu做I/O模拟，虽然Qemu也提供CPU的模拟，但是KVM没有使用，也许这正是KVM+Qemu比单纯用Qemu进行虚拟化性能高的原因吧。
运行在Qemu中的虚拟机被称为Guest，运行Qemu的物理机称为Host， 每一个guest是host上的一个进程，guest的每一个cpu对应进程中的一个线程。
Qemu和KVM之间通过ioctl进行交互，KVM和guest之间通过VM Entry和VM Exit进行切换。
那么什么时候会发生VM Entry和VM Exit呢？
先从虚拟化这个概念说起，其实通常的进程/线程也是一种虚拟化。Intel把CPU的优先级分了4层，ring0完整的暴露了CPU的各个接口，运行在ring0的操作系统可以任意使用CPU提供的所有功能，相比之下，运行在ring3的应用程序只能看到较少的CPU接口。
在这种情况下，操作系统担任的角色就是类似VMM（Virtual Machine Monitor）。运行在ring3的应用程序，自以为自己掌握所有的计算机资源，比如CPU资源，4GB的内存等等。众多傻乎乎的应用程序（用户态进程）在自己的虚拟世界里玩的不亦乐乎，而操作系统默默的在背后处理各个进程CPU相关的数据的保存和恢复，比如修改cr3寄存器，修改页表等等。
所以，操作系统完成了对ring3环境下的CPU的虚拟化，而KVM则是实现了一个完整的CPU虚拟化（所以这种类型的虚拟化，能运行不经任过何修改操作系统）。
kvm是一个内核模块，用kvm虚拟化技术创建的虚拟机，如果guest OS 执行的是与访问全局资源无关的指令，那么这些指令是直接运行在物理CPU上的；
当执行到了访问全局资源的指令，比如产生缺页错误，或者设备I/O，则guest OS产生VM Exit退出到KVM中，然后KVM再根据退出的原因，决定是由自己来处理还是交给Qemu。
当发生VM Entry和VM Exit 时必须要有一个结构体来保存当前CPU的上下文，这个结构就是VMCS (Virtual Machine Control Structure)，而前面提到的，使用KVM必须要有CPU硬件上的支持，指的就是CPU硬件必须提供一种功能，能自动根据VMCS的内容完成VM和VMM之间的状态切换。
从原博客拷贝的评论 Davelv：
写的不错，让我这种没研究KVM技术的人都看懂了。 不过有一点比较容易让人疑惑，在这里没有说清楚qemu是什么。 qemu在这里就是qemu-kvm这个修改版，他是一个单独的程序，运行起来是一个进程。 就是你说的使用KVM技术的qemu，这个进程的一些IO操作是像原始的qemu一样做模拟，但是和处理机相关模拟则会调用的kvm内核模块的功能（而不是像旧的qemu一样全部自己模拟）。kvm内核模块是一个运行在ring0的系统模块（驱动）本身没有独立的进程，依附于系统内核存在，作为操作系统(ring0层）的一部分 给应用程序（ring3层）如qemu-kvm相应支持。</description>
    </item>
    <item>
      <title>用GDB追踪glibc代码执行过程</title>
      <link>https://runzhen.github.io/posts/trace-glibc-by-using-gdb/</link>
      <pubDate>Tue, 27 Mar 2012 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/trace-glibc-by-using-gdb/</guid>
      <description>首先需要安装一下额外的工具包，一个是 libc6-dbg，这是带有debug symbol信息的 libc.so；另一个是libc6-dev，这是glibc的源代码，获取之后我们就可以在gdb中查看代码了。
在Ubuntu/Debian 系统上，我们可以通过以下2条命令获得：
$sudo apt-get install libc6-dbg
$sudo apt-get source libc6-dev
在Fedora/Red Hat 系的OS上，需要安装的软件包的名字不叫 libc6-dbg，libc6-dev，貌似应该是glibc-debuginfo。
之后，以一段小程序为例来演示整个过程，小程序包含了一个系统调用fork()，一个库函数printf()
int main(){ pid_t son; if((son=fork())==0) printf(&amp;#34;I am son\n&amp;#34;); else printf(&amp;#34;I am farther\n&amp;#34;); return 0; } 接着，编译产生带有调试信息的可执行文件 $gcc -g -o f fork.c 然后开启gdb调试 $gdb fork
在开始调试之前，需要指定一下刚刚获得的带有libc6-dev源码文件夹的路径，比如我把这些源码放在了 ~/glibc/lib 文件夹下，通常一般程序需要的是stdio-common这个目录内的文件，
于是输入 (gdb) directory ~/glibc/lib/stdio-common
注意看其中几条命令的用法。
程序在调用fork函数后，其实执行的是glibc包装过的__libc_fork ，并且我们可以查看其源代码。 这里有几个常用命令：
s 单步执行； list 查看源代码； start 程序开始执行，并在main函数处停下，相当于在main处加断点。 但是在执行了几步之后出现了这样的错误：
_IO_list_lock () at genops.c:1299 1299 genops.c: No such file or directory.</description>
    </item>
    <item>
      <title>从书上的一个错误说 Buffer Overflow</title>
      <link>https://runzhen.github.io/posts/buffer-overflow/</link>
      <pubDate>Sat, 01 Oct 2011 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/buffer-overflow/</guid>
      <description>时间倒回到2011年5月的一天，大学的最后一门课《计算机信息安全技术》，讲到《缓冲区溢出》这一章，并且给出了一段示例代码来演示缓冲区溢出，回到宿舍后出于好奇我运行了一下这段代码，发现结果并不是书上所说的那样，当时在人人网也发过一篇吐槽的日志，但是一直拖到现在都没有仔细的去研究过，正好现在十一放假没事，就花点时间搞搞啦。
书第136页-137页。代码如下，出于简单考虑（其实书上的C++代码格式也是错的），我除去了头文件和cout函数，这样就跟纯C语言代码是一样了。
void function(int a){char buffer[5];char *ret;ret = buffer + 12;*ret += 8;}int main(){int x;x = 10;function(7);x = 1;return 0;} 书上说最后x的值是10，不是1，而我的结果恰恰相反。 接着用gcc产生汇编代码，在这里用 gcc -O0 -S命令告诉编译器不采用任何优化措施，产生最原始的汇编代码，这样有利于我们分析，即使是采用-O1级优化的时候，汇编代码已经很难读了，大家可以试一试。
function:pushl %ebpmovl %esp, %ebpsubl $16, %espleal -9(%ebp), %eaxaddl $12, %eaxmovl %eax, -4(%ebp)movl -4(%ebp), %eaxmovzbl (%eax), %eaxaddl $8, %eaxmovl %eax, %edxmovl -4(%ebp), %eaxmovb %dl, (%eax)leavemain:pushl %ebpmovl %esp, %ebpsubl $20, %espmovl $10, -4(%ebp)movl $7, (%esp)call functionmovl $1, -4(%ebp)leaveret 书上详细的解释了为什么结果是10，下面我来逐条分析。首先画一张内存图，同样处于简洁考虑，只画function函数附近的内存分布，不影响分析。</description>
    </item>
    <item>
      <title>《编程珠玑》Maximum Subarray </title>
      <link>https://runzhen.github.io/posts/maximum-subarray/</link>
      <pubDate>Tue, 19 Jul 2011 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/maximum-subarray/</guid>
      <description>Table {:toc} 问题 有一个数组 31,-41,59,26,-53,58,97,-93,-23,84 。现在要求出它的连续子串的最大值。 比如，31,-41,59,26是它的一个连续的子串，他们的和为75。但是75并不是最大值，有一个子串 59,26,-53,58,97它们的和187才是最大的。
解答 《Programming Pearls》第77页开始一共给出了4种解法，前两种非常简单，是大多数人思考几分钟就能想出的方法，但是复杂度却很高，分别为O(n^3)和O(n^2)。后两种解法则非常巧妙，更神奇的是第四种方法居然只有线性复杂度O(n)
解法1、解法2略。
解法 3：分治法 复杂度为O(nlogn)。 分治法在结构上是递归的，在保证不改变原问题的条件下，将问题的规模减小，生成多个子问题，并多次递归调用自身来解决子问题，之后再将子问题的求解结果合并成原问题的解。
对于case1，我们只要比较 ma 和 mb 的大小就可以得出原数组的最大子串的和了。 对于case2, 只要把ma和mb相加即可。以上只是将问题一次分解的过程，我们还需要将问题再分解直到不能在分解或是能直接得出结果为止。
什么时候能直接得出结果？当子数组只有一个元素的时候，此时ma就是它本身（为负数时我们让它为0）。 因此，原数组的最大和 = 2个子数组中最大和的较大者，或者，包括中间分界线的一段连续区域的和。
即，maxsum(orignial)=max(mc，maxsum(a)，maxsum(b))
递归结束的条件是，子数组只有一个元素，如果是正返回它本身，为负返回0
代码如下。
int maxSubArray(std::vector&amp;lt;int&amp;gt;&amp;amp; nums) {return maxsum(nums, 0, nums.size()-1);}int maxsum(std::vector&amp;lt;int&amp;gt; &amp;amp;nums, int left, int right) {if (left &amp;gt; right) {return 0;}if (left == right) {return nums[left];}int mid = (left + right)/2;int left_max = INT_MIN, right_max = INT_MIN;int tmp_max = 0;for (int i = mid; i &amp;gt;= left; i--) {tmp_max += nums[i];left_max = std::max(left_max, tmp_max);}tmp_max = 0;for (int i = mid+1; i &amp;lt;= right; i++) {tmp_max += nums[i];right_max = std::max(right_max, tmp_max);}return std::max(left_max+right_max,std::max(maxsum(nums, left, mid),maxsum(nums, mid+1, right)));} 解法4：扫描法 一次扫描数组即可得出答案，复杂度O(n)。这种方法用文字描述不容易说清楚，下面用每一步运算的图示来表达。伪代码如下：</description>
    </item>
    <item>
      <title>iPhone 3G 越狱 (内置卡贴版)</title>
      <link>https://runzhen.github.io/posts/jailbreak-iphone/</link>
      <pubDate>Sun, 13 Sep 2009 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/jailbreak-iphone/</guid>
      <description>泡了N久论坛，看了N多帖子以后终于成功的把 iPhone 3G 有内置卡贴的手机成功的破解，并且抽出了卡贴，越狱成功，可以用任何移动运营商的手机卡了！
哦耶！散花庆祝！（如果不懂什么是“越狱”请自行补习相关知识 ）
说一下我的经验心得：
1、我主要是根据这个教程做的 http://bbs.weiphone.com/read.php?tid=401812，当然，之前也学习过其他的帖子，如果你也准备动手的话，请先泡泡论坛，多学习学习，也要注意一下那些失败的家伙是怎么折腾的，等功力大成之后，再动手不迟~（不要草率，先多学点注意事项，毕竟你要整的是贵重脆弱的东西啊）
2、下面说下我这次破解跟别人的不同之处。如果你的iPhone也非常不幸的是内置卡贴的话，可以参考我的方法，不用拆开手机。我是用了一根绣花针（一定要细小，但是要结实），在放SIM卡的开口处挑的，先用很薄的小刀沿着SIM卡开口把卡贴（一般是一根屎黄色的带子）划断，很容易划断的，然后用针一点一点把它的挑出来，你挑不出来的部分就让他在里面，没事的，关键是要把挡在SIM下面的卡贴去掉，下图就有这个万恶的卡贴！！（这个过程如有疑问欢迎通过邮件联系我^_^）
下面放图，分享一下～～
先来一张万恶的卡贴：</description>
    </item>
    <item>
      <title>收到了 Ubuntu 寄来的免费CD</title>
      <link>https://runzhen.github.io/posts/ubuntu-cd/</link>
      <pubDate>Sat, 12 Sep 2009 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/ubuntu-cd/</guid>
      <description>话说，我是在8月13号左右申请的，9月10号收到光盘的，花了4个多星期，比官方说的6—10周快了一点，毕竟，人家是从荷兰不远万里的寄来的光盘，哈哈。
如果你也想申请，点这个链接https://shipit.ubuntu.com/ 任何人都可以申请哦，不分国界，不分性别….这就是“Ubuntu精神”。
“Ubuntu”is an ancient African word that means “humanity to otherts”.This Linux distribution brings the spirit of Ubuntu to the software world.
下面贴几张图吧~（点击图片放大）
这是邮件的包裹，没拆开之前</description>
    </item>
    <item>
      <title>Dune: User-level Access to Privileged CPU Features 笔记</title>
      <link>https://runzhen.github.io/posts/2018-06-03-dune/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/2018-06-03-dune/</guid>
      <description>论文的原文在这里 Dune: Safe User-level Access to Privileged CPU Features
1. Introduction Dune 的目标是让用户态程序直接使用硬件特性（例如，获得更好的硬件加速等），不过所谓的 “直接” 还是在虚拟化的环境中。Dune 利用现代 CPU 的虚拟化功能，提供给用户一个 进程的抽象， 而不是一个 虚拟机的抽象。
Dune 是一个内核模块，可以在需要的时候加载到标准的 linux 内核中。 于是一个普通进程可以进入 “Dune Mode”，在这个模式下的进程可以直接访问页表、中断、系统调用表等等。
特点：
Dune 进程是一个普通的 linux 进程，唯一的区别是它用 VMCALL 指令进入系统调用。 Dune 内核模块只提供进程级别的抽象，因此相对于 KVM 来说要简单得多。 3. Kernel Support for Dune Dune mode 下的进程运行在 VMX non-root 模式，可以安全的访问特权级硬件。然后看一下 Dune 的整个架构图。
{:height=&amp;ldquo;300&amp;rdquo; width=&amp;ldquo;300&amp;rdquo;}
如果说前面 1、2 小节读下来仍然云里雾里的话，一看到这个图立刻觉得特别清晰了，因为太像 KVM 的架构了！ 同样也是利用 CPU 提供的 VT-x 技术，Dune 更像是一个轻量级的 KVM。
在后续的章节中，作者也提到了 Dune 项目的原型就是在 KVM 的基础上修改的，特别是对 VT-x 操作的部分。</description>
    </item>
    <item>
      <title>Raft 一致性算法实现 1</title>
      <link>https://runzhen.github.io/posts/2009-09-01-raft-implementation-leader/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/2009-09-01-raft-implementation-leader/</guid>
      <description>在实现的时候发现这张状态转换图非常重要。整个 raft 的运行就是围绕着这张图发生一系列状态转换。 因此，第一步实现一个状态机就成了关键。
参考资料 Go 并发、管道</description>
    </item>
    <item>
      <title>Raft 一致性算法读书笔记</title>
      <link>https://runzhen.github.io/posts/2018-02-24-raft-paper/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/2018-02-24-raft-paper/</guid>
      <description>论文原文链接 In Search of an Understandable Consensus Algorithm
Raft 保证一致性的 4 个重要组成部分： 领导人的选举，日志复制，安全性，集群成员的变化。
Raft 是一个强领导者算法，意思就是所有的数据以当前领导者（Leader）上的为准，领导者上有的就是正确的，领导者上没有的就是错误的。
5.1 Raft 的基本术语和概念 任期的概念：任期(term) 用来区分时间轴上不同的领导者，当领导者换了，日期也也立刻变更。raft 算法用 term 来表示一个 逻辑时钟，而不是用传统的几月几日几点几分几秒，term 是一个递增的整数，当领导者变了，term 也 +1。
集群中的每一个服务器都存储了一个 currentTerm，任意两个 server 要相互通信时都需要包含各自的 currentTerm。如果一个 server 的 currentTerm 值小于对方，那么它需要更新自己的值。如果 candidate 或者 leader 发现他们自己的 currentTerm 比其他人的要小，那个立马变成 follower。如果收到的请求包含一个旧的 currentTerm，直接忽略。
原论文的图 2 包含了大量的信息，我自己制作了下面四个图片。
5.2 领导人的选举 领导人不断的向集群中其他人发送心跳包，当集群中一个 server 在一段时间里没有收到领导人的心跳包，那么他就要起义，自己竞选当领导人。
开始一次选举 Follower 把自己的 term+1，身份转变成 Candidate，向集群中其他 server 发起请求投票 RPC，当他获得大多数选票时，就成功当选。
问题：他怎么知道 “大多数” 是多少？（etcd 的实现似乎是每个 server 知道集群里面有多少台服务器）
在竞选的过程中， candidate 一直保持着 candidate 的身份直到下面三个事件中的任何一个发生：</description>
    </item>
    <item>
      <title>Raft 算法实现 2：日志的复制</title>
      <link>https://runzhen.github.io/posts/2009-09-01-raft-implementation-log-replication/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/2009-09-01-raft-implementation-log-replication/</guid>
      <description>投票的过程 日志的复制 一旦选举成功，所有的 Client 请求最终都会交给 Leader 处理。
当 Client 请求到 Leader后，Leader 首先将该请求转化成 LogEntry，然后添加到自己的 log[] 中，并且把对应的 index 值存到 LogEntry 的 index 中，这样 LogEntry 中就包含了当前 Leader 的 term 信息和在 log[] 中的index信息，这两个信息在发给 Follower 以后会用到。
所以一旦一个节点成为 Leader 以后，那么它的 log[] 保存的这一组 LogEntry 就代表了整个集群中的最终一致的数据。用 raft 论文的话来说就是，节点是一个状态机，LogEntry 是指令集，任何一个节点，只要逐个执行这一串指令，最后状态机的状态都一样。
先看看与日志复制相关的几个数据结构，首先是 raft 结构，其中相关的有以下几个变量：
type Raft struct {....log []LogEntrycommitIndex int // 所有机器lastApplied int nextIndex []int // 只在 leadermatchIndex []int .....} nextIndex 和 matchIndex，这两个数组只有当这个 raft 是 Leader 的时候才有效，否则这两个数组的内容无效。</description>
    </item>
  </channel>
</rss>
