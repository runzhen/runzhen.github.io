<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>K8s on Mind in the Wind</title>
    <link>https://runzhen.github.io/categories/k8s/</link>
    <description>Recent content in K8s on Mind in the Wind</description>
    <image>
      <title>Mind in the Wind</title>
      <url>https://runzhen.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://runzhen.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.125.1</generator>
    <language>en</language>
    <lastBuildDate>Sun, 03 Apr 2022 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://runzhen.github.io/categories/k8s/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Escape from k8s Pod to the Host using nsenter</title>
      <link>https://runzhen.github.io/posts/nsenter-escape-from-pod-to-host/</link>
      <pubDate>Sun, 03 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/nsenter-escape-from-pod-to-host/</guid>
      <description>本文是读完 Detecting a Container Escape with Cilium and eBPF 和 使用 Cilium 增强 Kubernetes 网络安全 的一个简单总结。
如何从 Pod 逃逸到 Host 通常为了安全起见，生产环境的 docker image 都要求不使用 root，一般都是在 Dockerfile 中指定 USER xxx，这样启动的 container/pod 是使用非特权的 user，这样的 user 是没法用 sudo 安装软件的。
有时为了能临时 debug，需要安装 vim, curl 之类的命令，又不想改动 Dockerfile 重新 build image，该怎么办呢？
一个 k8s 原生支持的方法是在 deployment 里面指定 securityContext，如下所示
$ cat privileged.yaml apiVersion: v1 kind: Pod metadata: name: privileged-the-pod spec: hostPID: true hostNetwork: true containers: - name: privileged-the-pod image: nginx:latest ports: - containerPort: 80 securityContext: privileged: true 对于 docker container，可以在指定 docker run 命令时，设置 --user 为 0 也能获得 root 的 container。</description>
    </item>
    <item>
      <title>k8s 切换 namespace 以及命令补全</title>
      <link>https://runzhen.github.io/posts/k8s-set-namespace-tool/</link>
      <pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/k8s-set-namespace-tool/</guid>
      <description>本文可以学到
.kube/config 文件中有哪些内容 如何实现 bash 的命令补全功能 起因 在使用 kubectl 命令的过程中，经常需要查看不同 namespace 下的资源，因此命令经常需要带上 -n name。
如果不想每次都多打这些字符，也可以设置一个默认的 namespace，
kubectl config set-context --current --namespace=xxxx 这样是方便了不少，但是一旦切换了 namespace 之后，又要重复上面的命令，而且经常还不记得。
有没有更好的办法呢？ 有人开发了一个小工具，kubectx 专门用于方便的切换 ctx 和 namespace。 ctx 是什么呢？ 其实就是哪个 k8s 集群。 说白了就是让你方便的在多个集群和 namespace 之间切换。
kubectx 有两种实现，一开始用的是最简单的 bash shell 脚本，新的版本开始用 k8s client API 开发。 下文的分析仅仅关注 namespace 的切换。
shell 版本的实现 这个实现非常简单，本质上就是调用几个 kubectl 命令实现 ns 切换。
首先需要知道的是，在 ~/.kube/config 路径下的 config 记录了你配置 kubectl 的信息，比如你用 kubectl 操作过几个 k8s 都会纪录在里面。
apiVersion: v1 clusters: - cluster: certificate-authority-data: DATA+OMITTED server: https://10.</description>
    </item>
    <item>
      <title>docker exec 是如何实现交互的</title>
      <link>https://runzhen.github.io/posts/docker-exec-io-stream/</link>
      <pubDate>Mon, 17 May 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/docker-exec-io-stream/</guid>
      <description>docker exec 命令的作用是进入到“容器内部”，并执行一些命令，那么它是如何实现把“容器内部”的 io 重定向到我们的终端(bash) 的呢？
基本原理 首先，要明白容器所依赖的内核 namespace 的概念，其实不存在“容器内部”，只要两个进程在相同的 namespace，那它们就相互可见，从用户的角度来说，也就是进入了容器內部。
nsenter nsenter 是一个命令行工具，它可以运行一个 binary，并且把它加入到指定的 namespace 中。
用法如下,
nsenter -h nsenter -a -t &amp;lt;pid&amp;gt; &amp;lt;command&amp;gt; nsenter -m -u -i -n -p -t &amp;lt;pid&amp;gt; &amp;lt;command&amp;gt; 假设有一个 redis container 正在运行，通过 docker inspect --format {{.State.Pid}} 获取 pid, 假设为 2929。
然后运行 nsenter 命令：
# nsenter -a -t 2929 ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND redis 1 0.0 0.</description>
    </item>
    <item>
      <title>如何实现一个 kubectl-debug</title>
      <link>https://runzhen.github.io/posts/how-to-implement-kubectl-debug/</link>
      <pubDate>Sat, 24 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/how-to-implement-kubectl-debug/</guid>
      <description>借助 k8s client-go 连接 API Server，并进行简单的 list 操作，创建 deployment 如何在 pod 内创建 container 如何让 container 加入到某个 Pod 中，并共享 namespace 如何让 pod 内部的 tty 操作结果显示在用户端 docker exec 和 直接 nsenter 还是不太一样的：
docker exec， OCI-O 的实现是启动一个 grpc server，重定向 IO，等于是把 container 的 IO stream 重定向到 用户 cli nsenter 则是启动一个 进程，然后加入到 container 的 ns 所以 前者不需要知道 container 的 pid，而后者需要知道，所以后者需要执行 docker insepect 命令。 观察上图，分析原理，不难发现，容器内部的进程关系已然不是树。然而，为什么总是强调“树状”关系呢？答案是：树状的继承关系，有利于容器管理。以上文《docker logs 实现剖析》中卖的关子「docker exec的标准输出不会作为容器日志」为例，Docker Daemon 创建容器主进程时，负责接管主进程的标准输出，从而保证容器主进程下所有进程的标准输出被接管，然而 Docker Daemon 在新创建 docker exec 所需执行的进程时，后者的标准输出并未与容器主进程作关联，也并未被 Docker Daemon 特殊处理，故 docker exec 所执行进程的标准输出不会进入容器的日志文件中。</description>
    </item>
    <item>
      <title>Kubernetes Scheduler 设计与实现 (一)</title>
      <link>https://runzhen.github.io/posts/k8s-scheduler-beginning/</link>
      <pubDate>Sat, 17 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/k8s-scheduler-beginning/</guid>
      <description>Scheduler 的工作就是决定让一个 pod 在哪个 node 上运行。 scheduler 从 API Server 获得 pod 和 node 的信息，然后把它的决策信息写会 API Server, 它自己不参与具体的调度，而是运行在每个 node 上的 kubelet 主动获取更新，然后启动 pod。
scheduler 的入口函数在 cmd/kube-schduler/server.go，但实际工作都是在 pkg/scheduler/scheduler.go 里面的 Run 函数开始的。
打开 scheduler.go 文件找到结构体 Scheduler ，会发现它有很多私有函数，但只有唯一一个公开的 Run() 函数。
先从 Scheduler 结构体来说一下调度器的整体思路，其中最重要的三个成员如下：
type Scheduler struct { Algorithm core.ScheduleAlgorithm NextPod func() *framework.QueuedPodInfo SchedulingQueue internalqueue.SchedulingQueue } Algorithm 就是具体调度的算法 SchedulingQueue 是等待调度的队列，它本身是一个接口，它的实现是 PriorityQueue ，位于 pkg/scheduler/internal/queue/scheduling_queue.go NextPod 获取等待调度的 pod 另外顺便提一下，kubernetes 中的调度队列是由三个队列组成，分别是：
activeQueue：待调度的 pod 队列，scheduler 会监听这个队列 backoffQueue：在 kubernetes 中，如果调度失败了，就相当于一次 backoff。 backoffQueue 专门用来存放 backoff 的 pod。 unschedulableQueue：调度过程被终止的 pod 存放的队列。 然后来看 Scheduler 的 Run 函数:</description>
    </item>
    <item>
      <title>Kubernetes 中的 DNS </title>
      <link>https://runzhen.github.io/posts/k8s-kubedns-coredns/</link>
      <pubDate>Wed, 10 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/k8s-kubedns-coredns/</guid>
      <description>该文件指定如何解析主机名
cat /etc/host.conf order hosts, bind multi on order bind,hosts 指定主机名查询顺序，这里规定先使用 DNS 来解析域名，然后再查询 /etc/hosts 文件(也可以相反) multi on 指 /etc/hosts 文件中的主机可以有多个地址 nospoof on 指不允许对该服务器进行IP地址欺骗 </description>
    </item>
    <item>
      <title>Kubernetes 的 Volume 和 StorageClass</title>
      <link>https://runzhen.github.io/posts/kubernetes-volume-pv-pvc/</link>
      <pubDate>Mon, 28 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/kubernetes-volume-pv-pvc/</guid>
      <description>Kubernetes 的 Pod 可以 mount 很多种 Volume，常见的 volume 有
emptyDir hostPath configMap, secret persistentVolumeClaim nfs, gitRepo, cephfs, iscsi, cinder 等 其中，emptyDir 是最简单的一种，用于挂载一些临时文件，比如同一个 Pod 中两个 container 需要通过 unix socket 通信，那么把 socket 放在 emptyDir 中是最简单的方法。
甚至可以指定把这个抽象的目录放在内存，从而加快速度。
volumes: - name: html emptyDir: medium: Memory hostPath 是把数据直接存在 kubernetes 某个 worker node 上，这种方法一般不推荐使用，因为当 Pod 被调度到其他节点上后，数据就丢失了。
那么什么样的情况适合挂载 hostPath 呢？一些系统级的组件，需要挂载 node 上系统本身自带的一些文件时，比如需要读取 host 的 cert 目录，或者 etc 目录。常见的有 kube-system 空间下的 coreDNS 组件等。
当 Pod 中的程序需要把数据持久化到外部存储时，最推荐的用法是先在系统中定义 StorageClass，然后配合 persistentVolumeClaim (PVC) 和 persistentVolume (PV) 一起动态的分配空间。</description>
    </item>
    <item>
      <title>Kubernetes Headless Service</title>
      <link>https://runzhen.github.io/posts/kubernetes-headless-service/</link>
      <pubDate>Sun, 20 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/kubernetes-headless-service/</guid>
      <description>问题起源于我用 envoy 对 grpc 做 Layer 7 负载均衡的时候，发现 traffic 永远被转发到了一个特定的 Pod，显然是配置出错了。
环境如下：
同一个 namespace 下部署了 2 个 grpc server，一个 envoy
$ kubectl get pod NAME READY STATUS RESTARTS AGE grpc-envoy-7684f49cb-9fv4h 1/1 Running 0 4h49m grpc-server-668bdd6576-2bvkz 1/1 Running 0 4h51m grpc-server-668bdd6576-tqzj4 1/1 Running 0 4h51m envoy 的 service 配置如下
apiVersion: v1 kind: Service metadata: name: grpc-envoy namespace: default labels: app: grpc-envoy spec: type: NodePort ports: - name: grpc port: 8080 targetPort: grpc nodePort: 30061 protocol: TCP selector: app: grpc-envoy grpc-server 的 service 配置如下</description>
    </item>
    <item>
      <title>Kubernetes Pod 中的 Pause 容器</title>
      <link>https://runzhen.github.io/posts/k8s-pod-pause-container/</link>
      <pubDate>Sun, 13 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/k8s-pod-pause-container/</guid>
      <description>在一个运行 kubernetes 的节点上，我们能看到很多名叫 “pause” 的 container。比如
$ sudo docker ps | grep pause a4218d1d379b k8s.gcr.io/pause:3.1 &amp;#34;/pause&amp;#34; a2109bf3f0db k8s.gcr.io/pause:3.1 &amp;#34;/pause&amp;#34; 57cfa42e95d3 k8s.gcr.io/pause:3.1 &amp;#34;/pause&amp;#34; 仔细观察一下不难发现，每一个 Pod 都会对应一个 pause container。
在查阅了网上的一些资料以后，我总结了一下它大概有两个作用，
它是 Pod 中第一个启动的 container ，由它创建新的 linux namespace，其他 container 启动后再加入到这些 namespace 中。 在 Pod 的环境中充当 init process 的角色，它的 PID 是 1，负责回收所有僵尸进程。 说个题外话，在 docker 中，一个 container 启动时，Dockerfile 的 ENTRYPOINT 中指定的命令会成为这个 container 的 init process，PID 为 1.
顺便来看一下 pause 容器的实现，一共只有几十行 C 语言代码
static void sigdown(int signo) { psignal(signo, &amp;#34;Shutting down, got signal&amp;#34;); exit(0); } static void sigreap(int signo) { while (waitpid(-1, NULL, WNOHANG) &amp;gt; 0) ; } int main(int argc, char **argv) { if (getpid() !</description>
    </item>
    <item>
      <title>Kubernetes Dashboard 添加 Auth</title>
      <link>https://runzhen.github.io/posts/k8s-dashboard-auth/</link>
      <pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/k8s-dashboard-auth/</guid>
      <description>组里的 k8s cluster Dashboard 一直没有设置登录，导致所有可以 ping IP 的人都可以登录管理界面，这样显然是不合理的，应该要设置几个不同权限的账户，并且开启 Dashboard 的 Basic Auth。
关于开启 Dashboard Basic Auth 网上有不少资料，但是在实际操作中还是遇到几个莫名其妙的坑。
创建用户文件 根据实际需求，并不需要使用 LDAP 等等复杂的登录方式，我们只需要一个 admin 账户，再加上低权限的只读 view 账户，以及有修改权限的 edit 账户就足够。
因此，只要添加 /etc/kubernetes/pki/basic_auth_file 文件即可。
vi /etc/kubernetes/pki/basic_auth_file password,username,1 需要注意的一个坑是用户名密码的顺序是反过来的（如上面所示），否则在 dashboard 上怎么输入都提示不对。
修改 API Server 配置 修改 /etc/kubernetes/manifests/kube-apiserver.yaml 加入一个启动参数
vim /etc/kubernetes/manifests/kube-apiserver.yaml - --basic-auth-file=/etc/kubernetes/pki/basic_auth_file 这里又遇到一个坑，在我们的 k8s Master 节点这个目录下有两个文件，第一个叫 kube-apiserver.yaml， 另一个是 kube-apiserver_xxx.yaml。
本来我以为第一个是实际的配置文件，第二个应该是其他人配置时 copy 的一个备份。
然而实际并不是，真正被使用的是第二个文件，这点让人匪夷所思，我花了好久才发现这个坑，但是我始终没找到哪里指定了让 api server 读取 kube-apiserver_xxx.yaml 而不是 kube-apiserver.yaml 。
重启 API Server 很多资料上都会把重启 api server 一笔带过，但是都不写具体怎么操作，k8s 上并不是简单删除一个 pod 就算重启的。</description>
    </item>
    <item>
      <title>在虚拟机中使用 GPU 计算</title>
      <link>https://runzhen.github.io/posts/nvidia-gpu-pass-through/</link>
      <pubDate>Sat, 13 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/nvidia-gpu-pass-through/</guid>
      <description>本文介绍如何在 Linux 虚拟机中直接使用 GPU 做科学计算，要达到这个目的，需要满足下面几个条件：
物理主机使用 VMWare ESXi 作为虚拟化的 VMM，并且版本最好大于等于 6.5 使用的是 Nvidia GPU 的显卡 Linux 虚拟机 OS 没有限制，我使用的是 ubuntu ESXi 开启显卡直通 假设已经安装好了 ESXi，通过 WebUI 进入 Host 的 Manage 界面，点击 Hardware，如图
把 nVidia 开头的这几个全部选中，然后 “Active”， 表示开启 PCI 设备的直通 (passthrough)。
重启物理主机。
配置虚拟机 创建一个新虚拟机，或者修改已有的虚拟机，
点击 Edit，VM Options ，在 Advanced 里面点击 Edit configuration 。
增加一条配置参数 hypervisor.cpuid.v0, 对应的值为 FALSE，这一步的目的是让驱动把虚拟机当做物理机来处理。
另一需要修改的地方让虚拟机硬件配置内存大小下面勾选 “Reserve all guest memory (All locked)”，让虚拟机启动时一次性获取物理主机内存，而不是按需获取。
到这里，主机和虚拟机的配置就全部完成了，接下来是驱动软件的安装
虚拟机安装驱动 重启并进入虚拟机 CLI，首先可以确认一下 GPU 已经被直通给了虚拟机，这一步不是必须要做，但检查一下没坏处。
$ lshw | grep display $ sudo apt install ubuntu-drivers-common $ ubuntu-drivers devices 接下来，禁用系统自带的开源 nouveau 驱动</description>
    </item>
    <item>
      <title>Kubeflow 部署 MNIST</title>
      <link>https://runzhen.github.io/posts/mnist_step_by_step/</link>
      <pubDate>Sun, 22 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/mnist_step_by_step/</guid>
      <description>在阅读本文之前，假设已经在 GCP 上安装好了 Kubeflow。
首先进入 Kubeflow，点击 Notebook Server，新建一个 Jupyter Notebook。
新建的时候会让你输入 Name 和 Namespace，在 Kubeflow 中，每个用户都在 k8s 集群上有自己的 Namespace。
这里输入的 Name 对应的 Notebook Pod 最后会在自己的 Namespace 下。
新的 Notebook 里面是空的，我们需要下载一些例子。打开 terminal
然后输入 git clone 命令：
git clone https://github.com/kubeflow/examples.git
回到默认界面会看到刚刚 clone 的项目，打开 mnist 目录下的 mnist_gcp.ipynb
开始 首先第一个问题，当我打开这个 Jupyter Notebook 的 WebUI 时，它运行在哪里？
Notebook 是在哪个 Pod $ kubectl -n Your-namespace get pod NAME READY STATUS RESTARTS AGE fairing-builder-chvkq-6s4cn 0/1 Completed 0 3d23h mnist-model-7886dcbb5b-t2kk8 1/1 Running 0 3d22h mnist-tensorboard-774c585b7c-65766 2/2 Running 0 21h mnist-train-2596-chief-0 0/1 Completed 0 3d22h mnist-train-2596-worker-0 0/1 Completed 0 3d22h mnist-ui-7f95c8498b-xqsfs 2/2 Running 0 3d22h test1-0 2/2 Running 0 3d23h test1-0 是之前在 UI里面创建 Notebook server 时定下的名字，于是test1-0</description>
    </item>
    <item>
      <title>[Istio] 使用 istio 控制转发流量</title>
      <link>https://runzhen.github.io/posts/use-istio/</link>
      <pubDate>Sun, 15 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/use-istio/</guid>
      <description>概念 首先介绍几个概念，Ingress 指的是进入到 k8s 集群中的 traffic，比如一个 client 发起的 HTTP 请求，经过层层网络最终到达了 k8s cluster 的外部，那么让不让它进入到 cluster 内部就是 ingress controller 做的事。
Kubernetes 原生提供了自己的 Ingress Controller，此外还有很多第三方的 Ingress Controller，istio 就是其中之一。需要注意的是，本文所有部署都是基于 GKE，其他的云平台可能略有不同。
在 k8s 中安装了 istio 之后，就可以用 istio 来控制所有进入 cluster 的流量。如何安装 istio 不在本文的范围，读者可以参考 istio 官方文档。
安装 istio 完成之后，kubectl get namespace 命令可以看到有个名叫 istio-system 的空间，所有 istio 组件的 pod 都在这个空间中。
然后用如下命令查看 istio ingressgateway
$ kubectl -n istio-system get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT istio-ingressgateway LoadBalancer 10.0.23.180 35.222.xxx.xxx 15020:32011/TCP,80:30444/TCP 我们会看到这个名叫 istio-ingressgateway 的 LoadBalancer 它有公网 IP 地址 35.</description>
    </item>
    <item>
      <title>minikube, 单机版 kubernetes</title>
      <link>https://runzhen.github.io/posts/ubuntu-install-k8s/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/ubuntu-install-k8s/</guid>
      <description>本来想在我的 linux 主机上创建 3 个虚拟机，然后手工搭建一个拥有 3 个节点的 k8s 集群。
但是翻了翻网上的各种教程，发现每个教程都是巨复杂，给我一种 “即使我跟着教程千辛万苦敲完所有命令，也不一定能运行” 的感觉。最后，我发现了 minikube 这个东西，可以方便的搭建一个单机版 k8s。
麻雀虽小五脏俱全，即便是这样一个简单的 k8s，目前也足够我学习一些基本知识了。
本文记录一下安装 minikube 的具体步骤，并在 k8s 中部署一个简单的服务。
安装 minikube 开局一张图，先展示一下 minikube 的整个架构。
首先是准备工作，更新系统，安装必要组件。
sudo apt-get update sudo apt-get install apt-transport-https sudo apt-get upgrade 然后安装 virtualbox，
sudo apt install virtualbox virtualbox-ext-pack 安装 minikube
wget https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 chmod +x minikube-linux-amd64 sudo mv minikube-linux-amd64 /usr/local/bin/minikube 添加 kubectl 源
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - 值得注意的是，我的主机是 ubuntu 18.04，代号 bionic，而安装的源却是 xenial，对应 ubuntu 16.</description>
    </item>
  </channel>
</rss>
