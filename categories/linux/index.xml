<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Linux on Mind in the Wind</title>
    <link>https://runzhen.github.io/categories/linux/</link>
    <description>Recent content in Linux on Mind in the Wind</description>
    <image>
      <title>Mind in the Wind</title>
      <url>https://runzhen.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://runzhen.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.125.1</generator>
    <language>en</language>
    <lastBuildDate>Sun, 26 Sep 2021 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://runzhen.github.io/categories/linux/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Makefile 的几个语法坑</title>
      <link>https://runzhen.github.io/posts/makefile-syntax-pitfalls/</link>
      <pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/makefile-syntax-pitfalls/</guid>
      <description>Makefile 和 Bash script 在使用的过程中有很多奇奇怪怪的坑，本文做一下纪录。
首先，有两个文件，一个叫 envs，里面定义了一个环境变量，比如
$ cat envsexport GOPROXY=&amp;#34;test.local&amp;#34; 第二个文件就是 Makefile ，假如我这样写
test: source ./envs echo ${GOPROXY} 所以，总的目标是，我希望在 Makefile 中导入另一个文件中事先定义好的环境变量。 然而这样的写法有很多问题。
source 命令找不到 加入直接运行 make, 很有可能你会看到这样的错误
$ make source ./envs make: source: Command not found 可是在 terminal 里面明明可以用 source 命令啊？ 于是，第一个坑出现:
source is a (non-POSIX) shell builtin, not an executable program on any conventional UNIX-like system. If source is changed to ., make changes its strategy; instead of trying to find and execute a program, it just passes the rule body to the system shell.</description>
    </item>
    <item>
      <title>Golang pprof 的使用姿势</title>
      <link>https://runzhen.github.io/posts/golang-pprof-usage/</link>
      <pubDate>Sun, 18 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-pprof-usage/</guid>
      <description>首先，在代码中引入 pprof 的方式非常简单，只要把下面这段代码放到 main 函数中即可
_ &amp;#34;net/http/pprof&amp;#34; go func() { if err := http.ListenAndServe(&amp;#34;:9090&amp;#34;, nil); err != nil { panic(err) } os.Exit(0) }() 然后启动你的程序，再用以下这些命令去对应的端口做 profiling
// cpu profile 默认从当前开始收集 30s 的 cpu 使用情况，需要等待 30s go tool pprof http://47.93.238.9:9090/debug/pprof/profile # wait 120s go tool pprof http://47.93.238.9:9090/debug/pprof/profile?seconds=120 // 以下 second 参数不起作用，因为采样是一瞬间完成的 go tool pprof http://47.93.238.9:9090/debug/pprof/heap go tool pprof http://47.93.238.9:9090/debug/pprof/goroutine go tool pprof http://47.93.238.9:9090/debug/pprof/block go tool pprof http://47.93.238.9:9090/debug/pprof/mutex 还有一种是 import &amp;quot;runtime/pprof“的方式，这种不太常用，不在本文范围。
运行了 go tool pprof 命令以后，会进入到一个交互界面，</description>
    </item>
    <item>
      <title>Golang Channel 用法总结</title>
      <link>https://runzhen.github.io/posts/golang-channel-usage/</link>
      <pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-channel-usage/</guid>
      <description>之前的博客中已经粗略探究了一下 golang channel 的实现原理，本文总结一下使用 channel 的各种姿势。
先看一下对不同状态的 channel 的读，写，关闭操作的结果
1. 使用 for range 读取 channel 场景： 当需要不断从 channel 里读数据时
这是最常用的方式，又安全又便利，当channel 被关闭时，for 循环自动退出。 用法不再赘述。
2. 使用 _, ok 判断 channel 是否关闭 场景: 读 channel，但需要判断 channel 是否已关闭。
读 channel 的操作 &amp;lt;- chan 既可以返回一个值，也可以返回两个值，这里就是用的两个返回值的方式。
举例：
if v, ok := &amp;lt;- ch; ok { // can read channel fmt.Println(v) } 读到数据，并且通道没有关闭时，ok 的值为 true。 通道关闭，无数据读到时，ok 的值为 false。 3. 与 select 搭配使用 场景: 需要对多个通道进行处理，或者设置超时
举例：
func (h *Handler) handle(job *Job) { select { case h.</description>
    </item>
    <item>
      <title>Goroutine 的 PMG 模型</title>
      <link>https://runzhen.github.io/posts/golang-runtime-pmg-1/</link>
      <pubDate>Mon, 19 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-runtime-pmg-1/</guid>
      <description>稍微了解过 Go runtime 的人想必都听过 goroutine 的 PMG 模型，哪么它到底代表什么意思呢？ Golang 源码中又是如何实现的？
前言 关于 PMG 的解释网上有很对，随便 copy 一个：
M 代表 Machine，系统线程，它由操作系统管理的，goroutine就是跑在M之上的；M 是一个很大的结构，里面维护小对象内存cache（mcache）、当前执行的goroutine、随机数发生器等等非常多的信息。 P 是 Processor，处理器，它的主要用途就是用来执行goroutine的，它维护了一个goroutine队列，即runqueue。Processor是让我们从N:1调度到M:N调度的重要部分。 G 代表 goroutine 它包含了栈，指令指针，以及其他对调度goroutine很重要的信息，例如其阻塞的channel。 通常 go 程序中可以用 GOMAXPROCS 设置 Processor 的个数； 而 M 则是 clone系统调用创建的，或者用linux pthread 库创建出来的线程实体。 M 与 P 是一对一的关系。
基本结构体 打开 src/runtime/runtime2.go 文件，p,m,g 三个结构体的定义是按顺序在一起的，除此之外还有一个 schedt，与 goroutine 的调度相关。
g 结构体 G 就是 goroutine 的意思，每个 Goroutine 对应一个 g 结构体，它有自己的栈内存, G 存储 Goroutine 的运行堆栈、状态以及任务函数。 当一个 goroutine 退出时，g 会被放到一个空闲的对象池中以用于后续的 goroutine 的使用， 以减少内存分配开销。</description>
    </item>
    <item>
      <title>Golang Channel 的实现</title>
      <link>https://runzhen.github.io/posts/golang-channel/</link>
      <pubDate>Sat, 10 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-channel/</guid>
      <description>Channel 可以说是 Go 语言最具特色的设计了，我们经常会看到一些老鸟这样教育菜鸟：
Do not communicate by sharing memory; instead, share memory by communicating.
那么熟练使用 Golang 就离不开 channel，有必要了解一下 channel 是怎么实现的。
channel 的源代码在 Golang 的 src/runtime/chan.go 目录下，先看结构体:
type hchan struct { qcount uint // 循环列表元素个数 dataqsiz uint // 循环队列的大小 buf unsafe.Pointer // 循环队列的指针 elemsize uint16 // channel 中元素的大小 closed uint32 // 是否已close elemtype *_type // channel 中元素类型 sendx uint // send 在buffer中的索引 recvx uint // recv 在buffer中的索引 recvq waitq // receiver 的等待队列 sendq waitq // sender 的等待队列 lock mutex } type waitq struct { first *sudog last *sudog } 其中</description>
    </item>
    <item>
      <title>Golang WaitGroup 的实现</title>
      <link>https://runzhen.github.io/posts/golang-waitgroup/</link>
      <pubDate>Mon, 05 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-waitgroup/</guid>
      <description>sync.WaitGroup 的作用就是让主函数等待所有 goroutine 都执行完毕，再退出。
一个最简单的例子如下，如果没有 wg，那么 main 会在 goroutine 执行之前就退出，从而不会看到任何 output。
func main() { wg := sync.WaitGroup{} for i := 0; i &amp;lt; 3; i++ { wg.Add(1) go func(i int) { fmt.Println(i) wg.Done() }(i) } wg.Wait() } 那么 WaitGroup 是如何实现的呢？
万变不离其宗，其底层还是基于 go runtime 提供的信号量机制，也就是 runtime_Semrelease() 和 runtime_Semacquire()， 在之前的文章 Golang RWMutex 的实现 和 netpoll 的实现 中都有它们的影子存在。
runtime_Semacquire(s *uint32) 此函数会阻塞直到信号量*s的值大于0，原子减这个值。 runtime_Semrelease(s *uint32, lifo bool, skipframes int) 此函数执行原子增信号量的值，然后通知被runtime_Semacquire阻塞的协程 说到底，就是用 信号量 和 gopark 来控制 goroutine 是运行还是挂起，wg.</description>
    </item>
    <item>
      <title>Golang reflect 的使用</title>
      <link>https://runzhen.github.io/posts/golang-reflect/</link>
      <pubDate>Sat, 03 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-reflect/</guid>
      <description>所谓反射 (refection) 是指程序在运行过程中获取变量的类型、属性。 在 Golang 中，有时我们会看到 reflect.ValueOf() 或者 reflect.TypeOf() 这两个函数，这就是反射出一个变量的值和类型。 gPRC 的实现中也大量运用了反射。
本文主要介绍如何使用 reflect 包，关于 Go 内部是如何实现的将在下一篇文章中介绍。
TypeOf 和 ValueOf 先看一个最简单的例子
type User struct { Name string Age int } func main() { u := User{&amp;#34;Dick&amp;#34;, 18} t := reflect.TypeOf(u) v := reflect.ValueOf(u) fmt.Printf(&amp;#34;u type = %T, %v\n&amp;#34;, u, u) fmt.Printf(&amp;#34;t type = %T, %v\n&amp;#34;, t, t) fmt.Printf(&amp;#34;v type = %T, %v\n&amp;#34;, v, v) // 获取 v 的值 // v.Age , 错误，因为 v 是 reflect.</description>
    </item>
    <item>
      <title>Golang io 包的实现</title>
      <link>https://runzhen.github.io/posts/golang-io-package/</link>
      <pubDate>Fri, 02 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-io-package/</guid>
      <description>Golang 的 io package 包含 3 个文件 io.go, multi.go, pipe.go, 其中最主要的时 io.go。
当我们打开 io.go 的源码后，发现这个文件里面定义了大量的接口，实际上，io 包的作用就是如此 - 定义基本的 Read / Write inteface，而把具体的实现交给其他 package，比如 strings package 中就专门实现了 reader/writer，在后面的文章中再分析 strings 包。
接下来就看看 io 包中到底包含了哪些东西。
io.go 首先时定义了 4 个基础操作，读，写，关闭，seek
type Reader interface { Read(p []byte) (n int, err error) } type Writer interface { Write(p []byte) (n int, err error) } type Closer interface { Close() error } type Seeker interface { Seek(offset int64, whence int) (int64, error) } 基于这 4 个基础 interface，两两组合，有扩展了下面几个 interface</description>
    </item>
    <item>
      <title>Golang Context 的实现</title>
      <link>https://runzhen.github.io/posts/golang-context/</link>
      <pubDate>Sun, 28 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-context/</guid>
      <description>有这样一个场景： 父 goroutine 创建了多个子 goroutine 来处理某个请求，当这些子 goroutine 中任何一个出错的时候，我们希望所有的 goroutine 都停止。 该如何实现呢？
熟悉 Go 语言的可能首先想到用 context，而 context 主要是依靠 channel 来实现以上功能。
看了一下具体的实现，主要思想是:
每种类型的 ctx 都实现了 context.Context 接口的 Done() 函数 Done() &amp;lt;-chan struct{} 函数返回一个只读的 channel 而且没有地方向这个channel里写数据。所以直接调用这个只读channel会被阻塞。 一般通过搭配 select 来使用。一旦 channel 关闭，就会立即读出零值。 谁来关闭这个 channel 呢？ 用户主动调用返回的 CancelFunc，或者 timeout 超时 另外，在使用上配合 select 语句阻塞处理 Done() 才能起到预期的效果。
下面举两个如何使用 context 的例子，第一个例子如下
func main() { ctx, cancel := context.WithTimeout(context.Background(), 1*time.Second) defer cancel() go handle(ctx) // 等待3秒再结束（只是为了让 main 不提前 exit，与本文无关） select { case &amp;lt;- time.</description>
    </item>
    <item>
      <title>Golang 读写锁的实现</title>
      <link>https://runzhen.github.io/posts/golang-rw-lock/</link>
      <pubDate>Thu, 18 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-rw-lock/</guid>
      <description>type RWMutex struct { w Mutex // held if there are pending writers writerSem uint32 // semaphore for writers to wait for completing readers readerSem uint32 // semaphore for readers to wait for completing writers readerCount int32 // number of pending readers readerWait int32 // number of departing readers } writerSem 是写入操作的信号量 readerSem 是读操作的信号量 readerCount 是当前读操作的个数 readerWait 当前写入操作需要等待读操作解锁的个数 其中 semaphore 就是操作系统课程里面学到的信号量的概念。
读写锁的实现非常简单，源码在 /usr/local/go/src/sync/rwmutex.go 下，我们可以逐一分析它的各个函数
读者加读锁 首先是读锁，读者进入临界区之前，把 readerCount 加一，
如果这个值小于 0，则调用runtime_SemacquireMutex 把自己所在的 goroutine 挂起。 如果大于等于 0， 则加读锁成功 func (rw *RWMutex) RLock() { if atomic.</description>
    </item>
    <item>
      <title>epoll 在 Golang net 库的使用</title>
      <link>https://runzhen.github.io/posts/golang-net-epoll/</link>
      <pubDate>Sun, 31 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-net-epoll/</guid>
      <description>本文主要关注以下几个问题:
Golang runtime 是怎么调用 epoll 的系统调用的 ？ Golang net 库如何封装 epoll，使得开发者几乎不用直接操作 epoll ? C 如何调用 epoll 首先回顾一下用 C 语言怎么使用 epoll
int s = socket(AF_INET, SOCK_STREAM, 0); bind(s...) listen(s...) int epfd = epoll_create(128); //创建eventpoll对象 ev.events = _EPOLLIN | _EPOLLOUT | _EPOLLRDHUP | _EPOLLET epoll_ctl(epfd, EPOLL_CTL_ADD, s, &amp;amp;ev);//注册事件 //轮询就绪事件 while(true){ //返回值n为就绪的事件数,events为事件列表 int n = epoll_wait(epfd, &amp;amp;events[0], len(events), 1000) for( i := 0; i &amp;lt; n; i++ ) { ev := &amp;amp;events[i] //处理事件 } } C 语言中调用 epoll 的方式比较底层，总的来说分下面三个步骤</description>
    </item>
    <item>
      <title>gRPC-go Server 端实现</title>
      <link>https://runzhen.github.io/posts/grpc-go-server-code/</link>
      <pubDate>Mon, 18 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/grpc-go-server-code/</guid>
      <description>在上一篇文章中，介绍了 grpc 建立 TCP 连接的过程，侧重点在 Client 端，而关于 Server 端建立 TCP 的过程相对是比较简单的。
Server端 listen on 本地端口，并且接收来自 client 的连接请求，一旦建立 TCP 连接后，接下来的步骤是什么呢？ 建立 HTTP2 server，并收发数据。
本文尝试回答一下几个问题：
Server 怎么利用 http2 的 stream 传输数据？ 从 stream 里读的数据存放在哪？ Stream 读到的数据如何传给用户 Server 要发送的数据又是从哪发送的？ 创建 http2Server 首先从用户的代码入手，用户的代码最后会调用 grpcServer.Serve(lis), 稍微追踪几个函数就能发现调用链是 handleRawConn() 到 serveStreams()。
从 handleRawConn() 中我们发现 newHTTP2Transport 会创建一个新的 http2Server。
serveStreams() 中的 HandleStreams() 是 type ServerTransport interface 的一个函数，而 type http2Server struct 实现了这个接口。
值得注意的是，有两个结构体实现了 ServerTransport，分别是
transport/handler_server.go 的 serverHandlerTransport transport/http2_server.go 的 http2Server 一般我们在 main 函数中调用 grpcServer.</description>
    </item>
    <item>
      <title>Bittorrent 协议及工作原理</title>
      <link>https://runzhen.github.io/posts/how-bt-torrent-works/</link>
      <pubDate>Sat, 24 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/how-bt-torrent-works/</guid>
      <description>在 2000 年左右开始接触互联网的同学都应该记得用 BT 种子下载电影和小电影那段的时光。之前只是大概知道 BT 的工作原理，但并没有仔细研究过，所以一直很好奇。
随便在网上搜索下，可以知道 BT 大概是这样工作的:
BitTorrent 协议把提供下载的文件虚拟分成大小相等的块，块大小必须为 2k 的整数次方，并把每个块的索引信息和 Hash 验证码 写入 .torrent 文件（即种子文件，也简称为“种子”）中，作为被下载文件的“索引”。 下载者要下载文件内容，需要先得到相应的 .torrent 文件，然后使用 BT 客户端软件进行下载。
下载时，BT 客户端首先解析 .torrent 文件得到 Tracker 地址，然后连接 Tracker 服务器。Tracker 服务器回应下载者的请求，提供下载者其他下载者（包括发布者）的 IP。或者，BT客户端也可解析 .torrent 文件得到 nodes 路由表，然后连接路由表中的有效节点，由网络节点提供下载者其他下载者的 IP。
torrent 文件包含了什么 根据 bittorrent.org官方文档，种子文件也被称为metainfo files, 主要包含以下信息：
announce, The URL of the tracker. info, This maps to a dictionary. 所以种子文件就是告诉你，去 announce 这个地址找文件，具体文件信息包含在 info 里面。
Info 结构体有以下基本内容：
name key maps to a UTF-8 encoded string.</description>
    </item>
    <item>
      <title>Unicode 字符编码</title>
      <link>https://runzhen.github.io/posts/unicode-utf8-encode/</link>
      <pubDate>Sun, 16 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/unicode-utf8-encode/</guid>
      <description>ASCII 我们熟悉的 ASCII 码可以说是字符编码的始祖了。它规定了常用的数字、符号、英文字母与二进制之间的对应关系。
ASCII 的缺点是字符集太少了，只能表示英文和数字，无法表示像中文，日文这样的符号。因此人们就设计出了 Unicode 字符集，囊括了几乎所有人类语言文字的符号。
Unicode Unicode 是一个字符集，而不是一种编码方式。 Unicode 相当于是给人类所有的符号一个独一无二的 ID，只要大家都是用这个 ID 表示字符，就不会出现乱码的问题。
因为 Unicode 是一个字符集，因此它不存在所谓的 “用几个字节表示 unicode” 这样的问题，这是具体的编码方式需要处理的事。
Unicode 把 ID 划分成了 17 组 (Plane)，每组有 65536 个字符，编号可以用 U+[XX]YYYY 这样的形式表示，每一位是一个十六进制数字，其中 XX 代表组编号，从 0 到 0x10，一共17个，YYYY 代表这一组中的字符编号，一共 65536 个。
其中第 0 组叫 Basic Multilingual Plane，简称 BMP，它是 Unicode 中最基础和最常用的一部分，码点范围是U+0000 ~ U+FFFF，包含了我们常用的英文和汉字。
UFT-8 UTF-8 是 Unicode 具体的编码方式，除此之外还要 UTF-16, UTF-32 等等。
为什么需要编码方式呢？ 直接用 Unicode 的 ID 不就行了吗？ 因为我们需要节省存储空间。
UTF-8 是一种变长的编码方式，它可以使用 1-4 个字节表示一个符号，编码规则如下</description>
    </item>
    <item>
      <title>用 Grafana 展示监控状态</title>
      <link>https://runzhen.github.io/posts/grafana-dashboard/</link>
      <pubDate>Sun, 02 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/grafana-dashboard/</guid>
      <description>运维或者 SRE 部门经常会弄一个大屏幕展示各种系统状态，看上去很好玩，于是我也用类似的开源软件监控一下家里的主机。
整个过程非常简单，主要是安装三个软件 Node exporter，Prometheus，Grafana。
Node exporter 既然要展示系统状态，那么第一步就是要获得系统的状态数据，比如 CPU 使用率，内存使用率，网络流量等。
Prometheus 官方提供了一个使用 go 语言编写的程序 node_exporter，直接下载项目主页上 release 里的二进制即可。node_exporter 最好直接安装在物理主机上，因为这样才能采集到最准确的数据。
运行 node_exporter 以后，会自动启动一个 http server 并且监听 9100 端口，如果有 client 过来访问， server 返回主机的监控信息。比如：
$ curl http://localhost:9100/metrics node_network_transmit_packets_total{device=&amp;#34;veth126cb08&amp;#34;} 28859 node_network_transmit_packets_total{device=&amp;#34;veth1276a16&amp;#34;} 1383 node_network_transmit_packets_total{device=&amp;#34;veth749c501&amp;#34;} 1.108492e+06 返回信息的格式是符合 Prometheus 定义的标准的，因此 Prometheus 能够处理并以简单的图标的形式展现这些数据。
看到这里大家应该不难想到，如果我自己写一个程序 HelloWorld，并且把程序的状态按照一定的格式导出，那么同样可以通过 Prometheus + Grafana 展现。
Prometheus Prometheus 是一个功能齐全的数据库，还提供了 PromSQL 语言方便用户查询，以及一个简单的网页前端。
最简单快捷的方式当然是启动一个容器，唯一需要注意的是把配置文件 prometheus.yml 挂载到容器的 /etc/prometheus/ 目录下。
$ docker run -d -p 9090:9090 \ -v /home/prometheus/:/etc/prometheus/ prom/prometheus 配置文件中需要在 scrape_configs 部分添加 noder exporter 的 IP 地址和端口。</description>
    </item>
    <item>
      <title>CPU affinity</title>
      <link>https://runzhen.github.io/posts/taskset/</link>
      <pubDate>Sun, 15 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/taskset/</guid>
      <description>CPU affinity &amp;ndash; CPU 亲和性，指进程更希望运行在哪个 CPU core 上。
指定 core 有什么好处呢？
比如，可以自己决定哪些程序可以独占 CPU 资源，保证这个程序性能的最大化； 指定 CPU 以后可以提高 Cache 的命中率，常用于一些对性能非常高要求的程序，例如 nginx。 命令行指令 taskset 在 Linux 系统中，我们可以用 taskset 命令指定一个进程运行在哪个核心上。
比如我们写一个程序用 while(1) 制造死循环，那么运行这个程序的时候 CPU 会飙到 100%
用以下这条命令运行这个程序
taskset -c 3 ./a.out 意思是把 a.out 运行在从 0 开始数起的第 3 个核心上。
于是，用 htop 命令查看，会看到第 4 个核 CPU 使用率是 100%。
编程的 API 那么在程序的代码里怎么用呢？ 先来看看 glibc 提供的系统 API
#include &amp;lt;sched.h&amp;gt; int sched_setaffinity(pid_t pid, unsigned int cpusetsize, cpu_set_t *mask); int sched_getaffinity(pid_t pid, unsigned int cpusetsize, cpu_set_t *mask); void CPU_CLR(int cpu, cpu_set_t *set); int CPU_ISSET(int cpu, cpu_set_t *set); void CPU_SET(int cpu, cpu_set_t *set); void CPU_ZERO(cpu_set_t *set); nginx 的 config 文件中，可以为每个工作进程绑定CPU</description>
    </item>
    <item>
      <title>Docker 中编译 vim8.0</title>
      <link>https://runzhen.github.io/posts/build-your-vim8/</link>
      <pubDate>Fri, 16 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/build-your-vim8/</guid>
      <description>vim 的最新版 vim8.0 提供了很多新的特性，而且一些流行的 vim 插件很多功能也依赖于 8.0 版本，如果你要使用 vim8.0，那么最好的办法当然是使用操作系提供的软件包管理器一键安装，省时省力。
但是总有那么一些蛋疼的情况 ——你需要自己编译 vim。
本文就是记录下具体的步骤，并且把编译源码时需要安装的依赖软件全部做成 docker 镜像。
事情起因 某台服务器上我要用 vim8.0 的新特性，但是在服务器上我没有任何超级权限，只能读写我自己的 home 目录。
所以没法直接安装vim，只能从源码编译。
制作编译 vim 的docker image 编译 vim 需要系统中安装很多依赖软件，比如 vim 最基本的要包括 python2.7，luajit 等。
都 2019 年了，最好的方式当然是制作一个 docker 镜像，具体的步骤就不一一解释，贴上 Dockerfile 以示诚意。
FROM ubuntu:18.04 RUN apt-get update &amp;amp;&amp;amp; apt-get install -y \ liblua5.1-dev \ luajit \ libluajit-5.1 \ python-dev \ ruby-dev \ libperl-dev \ libncurses5-dev \ libatk1.0-dev \ libx11-dev \ libxpm-dev \ libxt-dev \ gnupg2 \ curl \ &amp;amp;&amp;amp; gpg2 --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDB \ &amp;amp;&amp;amp; curl -sSL https://get.</description>
    </item>
    <item>
      <title>How TCP backlog works in Linux</title>
      <link>https://runzhen.github.io/posts/tcp-backlog/</link>
      <pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/tcp-backlog/</guid>
      <description>原文： http://veithen.io/2014/01/01/how-tcp-backlog-works-in-linux.html
当一个应用程序使用 listen() 系统调用把一个 socket fd 设置成 LISTEN 状态时，也需要指定一个 backlog 值。通常我们可以认为这个 backlog 代表这个 socket fd 可以接受最大的连接请求数。
#include &amp;lt;sys/types.h&amp;gt; #include &amp;lt;sys/socket.h&amp;gt; int listen(int sockfd, int backlog); 因为 TCP 的三次握手，在 server 端 accept() 系统调用返回，并且tcp 状态在变成 ESTABLISHED 之前，会有一个短暂的 SYN RECEIVED 状态。那么这个状态的 tcp 链接应该放在哪个 queue 里面呢？
单个 queue，其大小就是 listen() 参数 backlog。当一个 SYN 包到达时，server 返回一个 SYN/ACK 给 client，并且把这个链接放入 queue。当 client 的 ACK 到达时，TCP 的状态变成 ESTABLISHED。这就意味着这一个 queue 有两种不同的状态：SYN RECEIVED 和 ESTABLISHED。只有在 ESTABLISHED 状态的链接才能被 accept() 返回给用户程序。</description>
    </item>
    <item>
      <title>Golang 操作共享内存</title>
      <link>https://runzhen.github.io/posts/share-memory-golang/</link>
      <pubDate>Fri, 02 Aug 2019 00:33:33 -0700</pubDate>
      <guid>https://runzhen.github.io/posts/share-memory-golang/</guid>
      <description>前言 进程间通信的方式有很多种，如果两个进程分别在不同的机器上，那么使用 socket 通信；如果在同一台机器上，共享内存机制是一种快速高效的方式。
本文实现一个 go 语言二进制程序和 C 语言二进制程序通过共享内存交换数据。
提到共享内存主要有两种：
System V 标准的 shmget/shmdt 等接口 POSIX 标准的 shm_open 等接口 另外 Linux 下 mmap() 匿名映射也是最常用的进程间共享内存方法。
创建了共享内存以后，一般会显示在系统的 /dev/shm 目录下。Linux 默认 /dev/shm 为实际物理内存的1/2, 比如我的机器上物理内存为 16G，运行 df 命令后可以看到 /dev/shm 的大小为 7.8G 。
$ df -h Filesystem Size Used Avail Use% Mounted on tmpfs 1.6G 3.2M 1.6G 1% /run tmpfs 7.8G 4.0K 7.8G 1% /dev/shm tmpfs, ramfs 和 ramdisk
tmpfs是一个虚拟内存文件系统，在Linux内核中，虚拟内存资源由物理内存(RAM)和交换分区组成，Tmpfs可以使用物理内存，也可以使用交换分区。
ramdisk 是一个块设备，只不过它是存在于内存上的。
ramfs 也是文件系统，不过已经被 tmpfs 替代了。</description>
    </item>
    <item>
      <title>ASLR 内核虚拟地址随机化</title>
      <link>https://runzhen.github.io/posts/address-space-layout-randomize/</link>
      <pubDate>Sun, 07 Jul 2019 14:01:40 -0700</pubDate>
      <guid>https://runzhen.github.io/posts/address-space-layout-randomize/</guid>
      <description>ASLR 全称 Address Space Layout Randomization，是一项 Linux 内核的安全措施，使应用程序每次加载到内存后，函数地址都不同。
试用一下 先来直观的感受下什么是 ASLR。目前大多数 linux 系统都默认开启了这个选项，可以用一下两个命令确认一下系统是否支持 ASLR。
$ cat /proc/sys/kernel/randomize_va_space 2 $ sysctl kernel.randomize_va_space kernel.randomize_va_space = 2 其中 0 表示关闭，1 表示有约束的随机，2 表示完全随机化。
然后随便找一个可执行程序，用 ldd 命令显示它加载的动态链接库，可以看到两次运行 ldd 结果各个库的地址不一样。
$ ldd /bin/sleep linux-vdso.so.1 (0x00007ffd49764000) libc.so.6 =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f02783ae000) /lib64/ld-linux-x86-64.so.2 (0x00007f02789a8000) $ ldd /bin/sleep linux-vdso.so.1 (0x00007ffc10996000) libc.so.6 =&amp;gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f12c3534000) /lib64/ld-linux-x86-64.so.2 (0x00007f12c3b2e000) 应用程序如何使用 ASLR 在这篇文章中提到，除了 kernel 开启以外，应用程序在编译的时候也必须添加编译选项 gcc -fPIE -pie test.c 。
但是在我的实际测试中，似乎并不需要额外添加编译选项，看来 gcc 默认开启了 ASLR。</description>
    </item>
    <item>
      <title>Orphan, Zombie and Docker</title>
      <link>https://runzhen.github.io/posts/orphan-zombie-and-docker/</link>
      <pubDate>Sun, 16 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/orphan-zombie-and-docker/</guid>
      <description>孤儿进程的产生 孤儿进程： 父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。通常，孤儿进程将被进程号为1的进程(进程号为 1 的是 init 进程)所收养，并由该进程调用 wait 对孤儿进程收尸。
#include &amp;lt;stdio.h&amp;gt; #include &amp;lt;stdlib.h&amp;gt; #include &amp;lt;errno.h&amp;gt; #include &amp;lt;unistd.h&amp;gt; int main() { pid_t pid; pid = fork(); if (pid == 0) { printf(&amp;#34;I&amp;#39;m child process, pid:%d ppid:%d\n&amp;#34;, getpid(), getppid()); sleep(5); printf(&amp;#34;I&amp;#39;m child process, pid:%d ppid:%d\n&amp;#34;, getpid(), getppid()); } else { printf(&amp;#34;I&amp;#39;m father process, pid:%d ppid:%d\n&amp;#34;, getpid(), getppid()); sleep(1); printf(&amp;#34;father process is exited.\n&amp;#34;); } return 0; } 运行结果如下所示：
I&amp;#39;m father process, pid:25354 ppid:13981I&amp;#39;m child process, pid:25355 ppid:25354father process is exited.</description>
    </item>
    <item>
      <title>nginx 文件锁、自旋锁的实现</title>
      <link>https://runzhen.github.io/posts/nginx-lock/</link>
      <pubDate>Tue, 18 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/nginx-lock/</guid>
      <description>在上一篇博客 Linux 共享内存以及 nginx 中的实现的示例中，我们看到每次多个进程同时对共享内存中的 count 加一，导致每次运行结果都不一样，那么解决的方法就是对临界区加锁了，所以本文就来研究一下 nginx 中的几种加锁方式。
文件锁 文件锁的原理就是在磁盘上创建一个文件（操作系统创建一个文件描述符），然后多个进程竞争去获取这个文件的访问权限，因此同一时刻只有一个进程能够访问临界区。
可以看出，进程并不会真正在这个文件中写什么东西，我们只是想要一个文件描述符 FD 而已，因此 nginx 会在创建了文件后把这个文件删除，只留下文件描述符。
多个进程打开同一个文件，各个进程看到的文件描述 FD 值可能会不一样。例如文件 test.txt 在 进程1 中是 101， 而在进程2中是 201
使用文件锁举例 使用文件锁主要用到两个 libc 提供的结构体和函数。
struct flock; 提供一些锁的基本信息，比如读锁 F_RDLCK, 还是写锁 F_WRLCK fcntl(): 对文件描述符进行操作的库函数。 那么如何用这个函数来实现锁的功能呢？
先看一个加锁的代码：
void mtx_file_lock(struct fdmutex *m) { struct flock fl; memset(&amp;amp;fl, 0, sizeof(struct flock)); fl.l_type = F_WRLCK; fl.l_whence = SEEK_SET; if (fcntl(m-&amp;gt;fd, F_SETLKW, &amp;amp;fl) == -1) { printf(&amp;#34;[-] PID %d, lock failed (%s).</description>
    </item>
    <item>
      <title>Linux 共享内存以及 nginx 中的实现</title>
      <link>https://runzhen.github.io/posts/share-memory/</link>
      <pubDate>Tue, 04 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/share-memory/</guid>
      <description>共享内存方法简介 Linux/Unix系统中，共享内存可以通过两个系统调用来获得，mmap 和 shmget/shm_open，其中 shmget 和 shm_open 分别属于不同的标准：
POSIX 共享内存（shm_open()、shm_unlink()） System V 共享内存（shmget()、shmat()、shmdt()） shmget 和 shm_open 类似的地方在于都是创建一个共享内存，挂载到 /dev/shm 目录下，并且返回一个文件描述符，fd。
区别是 POSIX 没有提供将 fd 映射到进程地址空间的方法，而 System V 方式则直接提供了 shmat()，之后再 nginx 的实现中会再次看到。
mmap 语义上比 shmget 更通用，因为它最一般的做法，是将一个打开的实体文件，映射到一段连续的内存中，各个进程可以根据各自的权限对该段内存进行相应的读写操作，其他进程则可以看到其他进程写入的结果。
而 shmget 在语义上相当于是匿名的 mmap，即不关注实体文件，直接在内存中开辟这块共享区域，mmap 通过设置调用时的参数，也可达到这种效果，一种方法是映射/dev/zero 设备,另一种是使用MAP_ANON选项。
mmap() 的函数原型如下，具体参数含义在最后的参考资料中给出。
void *mmap(void *addr, size_t len, int prot, int flags, int fd, off_t offset);
nginx 中的实现 nginx 中是怎么实现的呢？ 我们看一下源码 src/os/unix/ngx_shmem.c。
一目了然，简单粗暴有木有！ 分三种情况
如果mmap系统调用支持 MAP_ANON选项，则使用 MAP_ANON 如果1不满足，如果mmap系统调用支持映射/dev/zero设备，则映射/dev/zero来实现。 如果1和2都不满足，且如果支持shmget的话，则使用该shmget来实现。 #if (NGX_HAVE_MAP_ANON) ngx_int_t ngx_shm_alloc(ngx_shm_t *shm) { shm-&amp;gt;addr = (u_char *) mmap(NULL, shm-&amp;gt;size, PROT_READ|PROT_WRITE, MAP_ANON|MAP_SHARED, -1, 0); return NGX_OK; } #elif (NGX_HAVE_MAP_DEVZERO) ngx_int_t ngx_shm_alloc(ngx_shm_t *shm) { fd = open(&amp;#34;/dev/zero&amp;#34;, O_RDWR); shm-&amp;gt;addr = (u_char *) mmap(NULL, shm-&amp;gt;size, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0); return (shm-&amp;gt;addr == MAP_FAILED) ?</description>
    </item>
    <item>
      <title>使用 socketpair 实现进程间通信</title>
      <link>https://runzhen.github.io/posts/socketpair/</link>
      <pubDate>Tue, 28 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/socketpair/</guid>
      <description>socketpair 牛刀小试 int socketpair(int d, int type, int protocol, int sv[2]);第1个参数d，表示协议族，只能为 AF_LOCAL 或者 AF_UNIX；第2个参数 type，表示类型，只能为0。第3个参数 protocol，表示协议，可以是 SOCK_STREAM 或者 SOCK_DGRAM AF_UNIX 指的就是 Unix Domain socket，那么它与通常网络编程里面的 TCP socket 有什么区别呢？ 查阅了资料后发现：
Unix Domain socket 是同一台机器上不同进程间的通信机制。 IP(TCP/IP) socket 是网络上不同主机之间进程的通讯机制。 socketpair() 只支持 AF_LOCAL 或者 AF_UNIX，不支持 TCP/IP，也就是 AF_INET， 所以用 socketpair() 的话无法进行跨主机的进程间通信。
先看一个简单的示例：
int main() { int fd[2], retpid; int pid , status; char input[MAX_LEN]; if (socketpair(AF_UNIX, SOCK_STREAM, 0, fd) &amp;lt; 0) { printf(&amp;#34;call socketpair() failed, exit\n&amp;#34;); return -1; } pid = fork(); if (pid) { /* parent */ printf(&amp;#34;Parent process, pid = %d\n&amp;#34;, getpid()); while (1) { fgets(input, MAX_LEN, stdin); write(fd[0], input, MAX_LEN); } } else { /* child */ printf(&amp;#34;Child process, pid = %d\n&amp;#34;, getpid()); int nread = 0; while (1) { nread = read(fd[1], input, MAX_LEN); input[nread] = &amp;#39;\0&amp;#39;; printf(&amp;#34;Child: nread = %d, data = %s\n&amp;#34;, nread, input); } } retpid = wait(&amp;amp;status); if (retpid) { printf(&amp;#34;Parent: reap child process pid = %d\n&amp;#34;, retpid); } return 0; } 编译后运行，可以看到每次在终端输入信息，子进程都会回显到屏幕上。</description>
    </item>
    <item>
      <title>SO_REUSEPORT 和 epoll 的 Thundering Herd</title>
      <link>https://runzhen.github.io/posts/port-reuse/</link>
      <pubDate>Sat, 23 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/port-reuse/</guid>
      <description>SO_REUSEPORT 顾名思义就是重用端口，是指不同的 socket 可以 bind 到同一个端口上。 Linux 内核 3.9 版本引入了这个新特性，有兴趣的同学可以移步到这个链接查看更加详细的内容。 https://lwn.net/Articles/542629/
Reuse Port 我们先通过一段简单的代码来看看怎么使用这个选项（完整的代码在这里下载）。
int serv_sock = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP); // 一定要在 bind() 函数之前设定好 SO_REUSEPORT setsockopt(serv_sock, SOL_SOCKET, SO_REUSEPORT, &amp;amp;enable, sizeof(int)); bind(serv_sock, (struct sockaddr*)&amp;amp;serv_addr, sizeof(serv_addr)); listen(serv_sock, 20); accept(serv_sock, (struct sockaddr*)&amp;amp;clnt_addr, &amp;amp;clnt_addr_size); 将上面的代码编译生成两个可执行文件，分别启动运行，并监听相同的端口。
./port_reuse1 127.0.0.1 1234
再用 telnet/nc 等工具发送请求到 1234 端口上，多重复几次，会看到两个进程轮流的处理客户端发来的请求。
这里说一个题外话，上面的例子是手动启动两个进程。而我发现如果是进程自动 fork() 生成 2 个进程的话，似乎不用设置 SO_REUSEPORT 也能自动监听同一个端口。这是为什么？
Thundering Herd / 惊群现象 The thundering herd problem occurs when a large number of processes waiting for an event are awoken when that event occurs, but only one process is able to proceed at a time.</description>
    </item>
    <item>
      <title>Linux 内核在 x86-64 上的内存分区</title>
      <link>https://runzhen.github.io/posts/zone-highmem/</link>
      <pubDate>Thu, 02 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/zone-highmem/</guid>
      <description>如果稍微了解过 Linux 内核的内存管理，那么对内存分区的概念一定不陌生，Linux内核把物理内存分成了3个区，
0 – 16M 为ZONE_DMA区, 16M – 896M 为ZONE_NORMAL区， 高于896M 为ZONE_HIGHMEM区 我没有去考证过为什么要取896这个数字，但是可以肯定的是这样的划分在当时看来是合理的，然而计算机发展今非昔比，现在4G的物理内存已经成为PC的标配了，CPU也进入了64位时代，很多事情都发生着改变。
在CPU还是32位的时代，CPU最大的物理寻址范围是0-4G， 在这里为了方便讨论，我们不考虑物理地址扩展（PAE）。进程的虚拟地址空间也是 4G，Linux内核把 0-3G虚拟地址空间作为用户空间，3G-4G虚拟地址空间作为内核空间。
目前几乎所有介绍Linux内存管理的书籍还是停留在32位寻址的时代，所以大家对下面这张图一定很熟悉！ （这个图画得非常详细，本篇文章我们关注的重点是 3个分区 以及最右边的线性地址空间，也就是虚拟地址空间之间的关系，另外，应该是ZONE_DMA， ZONE_NORMAL, ZONE_HIGHMEM, 图中把ZONE 写成了ZUNE）
然而，现在是64位的时代了, 64位CPU的寻址空间是多大呢？ 16EB， 1EB = 1024 TB = 1024 * 1024 GB，我想很多人这辈子还没见过大于1TB的内存吧，事实上也是这样，几乎没有哪个服务器能有16EB的内存，实现64位长的地址只会增加系统的复杂度和地址转换的成本，所以目前的x86_64架构CPU都遵循AMD的 Canonical Form, 即只有虚拟地址的最低48位才会在地址转换时被使用, 且任何虚拟地址的48位至63位必须与47位一致, 也就是说总的虚拟地址空间为256TB。
那么在64位架构下，如何分配虚拟地址空间的呢？
0000000000000000 – 00007fffffffffff(128TB)为用户空间, ffff800000000000 – ffffffffffffffff(128TB)为内核空间。 而且内核空间中有很多空洞, 越过第一个空洞后, ffff880000000000 – ffffc7ffffffffff(64TB) 才是直接映射物理内存的区域, 也就是说默认的PAGE_OFFSET为 ffff880000000000.
请关注下图的最左边，这就是目前64位的虚拟地址布局。
在本文的一开头提到的物理内存分区 ZONE_DMA， ZONE_NORMAL， ZONE_HIGHMEM 就是与内核虚拟地址的直接映射有关的，如果读者不了解 内核直接映射物理地址这个概念的话，建议你去google一下，这个很简单的一一映射的概念。
既然现在内核直接映射的物理内存区域有64TB， 而且一般情况下，极少有计算机的内存能达到64TB（别说64TB了，1TB内存的也很少很少），所以整个内核虚拟地址空间都能够一一映射到计算机的物理内存上，因此，不再需要 ZONE_HIGHMEM这个分区了，现在对物理内存的划分，只有ZONE_DMA， ZONE_NORMAL。
如果你想有个更加直观的了解的话，请打开 /boot/config*** 文件，例如我的是 /boot/config-3.</description>
    </item>
  </channel>
</rss>
