<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Golang on Mind in the Wind</title>
    <link>https://runzhen.github.io/tags/golang/</link>
    <description>Recent content in Golang on Mind in the Wind</description>
    <image>
      <title>Mind in the Wind</title>
      <url>https://runzhen.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://runzhen.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.125.5</generator>
    <language>en</language>
    <lastBuildDate>Sun, 02 Apr 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://runzhen.github.io/tags/golang/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>设置 GOMAXPROCS 提高程序性能</title>
      <link>https://runzhen.github.io/posts/go-maxprocs/</link>
      <pubDate>Sun, 02 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/go-maxprocs/</guid>
      <description>从 cgroup 的介绍中，我们知道了通过设置 /sys/fs/cgroup/ 的值，并且使用 cgroup-tools 启动程序同时指定一个 cgroup，可以达到控制进程使用系统资源的目的。
起因 一个 Go 程序运行在 k8s 环境中，在某一行代码前后设置 start timestamp 和 end timestamp，发现有时候 p99 的 latency 非常高，正常情况下在 1-3 ms，极端情况下有 50-90 ms。百思不得其解，猜测各种可能加查阅资料后，发现应该是没有正确的设置 runtime.GOMAXPROCS。设置为 1 后，极高 latency 的情况明显减少。
为什么 出现这个问题有三个条件，缺一不可：
是 Go 程序，并且采用系统默认 GOMAXPROCS 运行在 k8s 或者 docker 这样的容器环境 宿主机上有多个 CPU 核 GOMAXPROCS 是什么 回忆一下 Go 并发的 GPM 模型：
G代表 goroutine，即用户创建的 goroutines P代表 Logical Processor，是类似于 CPU 核心的概念，其用来控制并发的 M 数量 M是操作系统线程。在绝大多数时候，P的数量和M的数量是相等的。每创建一个P, 就会创建一个对应的M 而 go 的 runtime GOMAXPROCS 代表的就是 P 的数量，其底层就是 runtime 直接调用 Linux 系统调用 sched_getaffinity()</description>
    </item>
    <item>
      <title>看懂 Go 汇编 三</title>
      <link>https://runzhen.github.io/posts/go-asm3/</link>
      <pubDate>Wed, 26 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/go-asm3/</guid>
      <description>本文主要收集一些例子，以后阅读 Go 汇编时遇到忘记的指令可以查询。
例子 1 package main func main() { l := []int{9, 45, 23, 67, 78} t := 0 for _, v := range l { t += v } println(t) } 截取了一段汇编如下
0x0026 00038 (3.go:4) MOVUPS X15, &amp;#34;&amp;#34;..autotmp_5+40(SP) 0x002c 00044 (3.go:4) MOVUPS X15, &amp;#34;&amp;#34;..autotmp_5+48(SP) 0x0032 00050 (3.go:4) MOVUPS X15, &amp;#34;&amp;#34;..autotmp_5+64(SP) 0x0038 00056 (3.go:4) LEAQ &amp;#34;&amp;#34;..autotmp_5+40(SP), AX 0x003d 00061 (3.go:4) MOVQ AX, &amp;#34;&amp;#34;..autotmp_4+80(SP) 0x0042 00066 (3.go:4) TESTB AL, (AX) 其中</description>
    </item>
    <item>
      <title>看懂 Go 汇编 二</title>
      <link>https://runzhen.github.io/posts/go-asm2/</link>
      <pubDate>Mon, 24 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/go-asm2/</guid>
      <description>本文翻译自 https://github.com/teh-cmc/go-internals/tree/master/chapter1_assembly_primer
先看一个简单的 code
// go tool compile -N -l -S once.go // go build -gcflags -S once.go package main //go:noinline func add(a, b int32) (int32, bool) { return a + b, true } func main() { add(10, 32) } 生成汇编
GOOS=linux GOARCH=amd64 go tool compile -S x.go 在我的机器 Ubuntu kernel 5.4.0, Go version go1.18.3 amd64 上出来的结果与原文中还是有些差异的，但为了文章通顺，下面还是用的原文的结果。
0x0000 TEXT	&amp;#34;&amp;#34;.add(SB), NOSPLIT, $0-16 0x0000 FUNCDATA	$0, gclocals·f207267fbf96a0178e8758c6e3e0ce28(SB) 0x0000 FUNCDATA	$1, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB) 0x0000 MOVL	&amp;#34;&amp;#34;.</description>
    </item>
    <item>
      <title>看懂 Go 汇编 一</title>
      <link>https://runzhen.github.io/posts/go-asm1/</link>
      <pubDate>Sun, 23 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/go-asm1/</guid>
      <description>寄存器 学过 X86 汇编的同学都知道汇编有AX，BX等寄存器，除此之外，Go 还添加了 PC、FP、SP、SB四个伪寄存器。如下图所示，其中第二列为 GO 添加的4 个伪寄存器，第三列为 X86 寄存器。
看到这里，尘封已久的汇编语言知识需要拿出来复习一下。
FLAGS 是状态寄存器。 IP 是指令寄存器。 AX、BX、CX、DX、SI、DI、BP、SP 是通用寄存器。在X86-64中又增加了八个以R8-R15 方式命名的通用寄存器。 另外 GO 的 4 个伪寄存器作用如下：
FP: Frame pointer：伪FP寄存器对应函数的栈帧指针，一般用来访问函数的参数和返回值；golang语言中，函数的参数和返回值，函数中的局部变量，函数中调用子函数的参数和返回值都是存储在栈中的，我们把这一段栈内存称为栈帧（frame），伪FP寄存器对应栈帧的底部，但是伪FP只包括函数的参数和返回值这部分内存，其他部分由伪SP寄存器表示；注意golang中函数的返回值也是通过栈帧返回的，这也是golang函数可以有多个返回值的原因； PC: Program counter：指令计数器，用于分支和跳转，它是汇编的IP寄存器的别名； SB: Static base pointer：一般用于声明函数或者全局变量，对应代码区（text）内存段底部； SP: Stack pointer：指向当前栈帧的局部变量的开始位置，一般用来引用函数的局部变量，这里需要注意汇编中也有一个SP寄存器，它们的区别是：1.伪SP寄存器指向栈帧（不包括函数参数和返回值部分）的底部，真SP寄存器对应栈的顶部；所以伪SP寄存器一般用于寻址函数局部变量，真SP寄存器一般用于调用子函数时，寻址子函数的参数和返回值（后面会有具体示例演示）；2.当需要区分伪寄存器和真寄存器的时候只需要记住一点：伪寄存器一般需要一个标识符和偏移量为前缀，如果没有标识符前缀则是真寄存器。比如(SP)、+8(SP)没有标识符前缀为真SP寄存器，而a(SP)、b+8(SP)有标识符为前缀表示伪寄存器； Symbols 符号 有些符号比如 R1、LR 是不同架构预定义的寄存器。除此之外，还有 GO 定义的 4 个伪寄存器。
FP: Frame pointer: arguments and locals. PC: Program counter: jumps and branches. SB: Static base pointer: global symbols. SP: Stack pointer: the highest address within the local stack frame.</description>
    </item>
    <item>
      <title>Go 中 for range 和 slice 的坑</title>
      <link>https://runzhen.github.io/posts/go-for-range-and-slice/</link>
      <pubDate>Fri, 29 Jul 2022 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/go-for-range-and-slice/</guid>
      <description>for range 的实现 下面这段代码会永无止境的循环吗 ？
package main import ( &amp;#34;fmt&amp;#34; ) func main() { sl := []int{1,2,3,4} for _, v := range sl{ sl = append(sl, v) } fmt.Println(sl) } 要验证它很简单，运行一下即可得到结果，最后的结果是
[1 2 3 4 1 2 3 4] 要理解为什么会有这样的结果不难，首先我们需要清楚一点 go 语言中的赋值语句都是赋值，那么就意味着
如果赋值的是一个指针, 那么拷贝的是指针指向对象的地址(就是一个数值, 至于这个数值有多大, 具体要看运行的平台)也就是指针的值 如果赋值的是一个对象, 那么就会拷贝这个对象 然后，我们再来看一下，当 for range 遇到不同的迭代对象时，编译器是如何展开代码的
数组 range_temp := range len_temp := len(range) for index_temp = 0; index_temp &amp;lt; len_temp; index_temp++ { value_temp = range_temp[index_temp] index = index_temp value = value_temp original body } slice 切片 for_temp := range len_temp := len(for_temp) for index_temp = 0; index_temp &amp;lt; len_temp; index_temp++ { value_temp = for_temp[index_temp] index = index_temp value = value_temp original body } map // Lower a for range over a map.</description>
    </item>
    <item>
      <title>记一次 go-redis 的 debug 过程</title>
      <link>https://runzhen.github.io/posts/go-redis-latency-debug/</link>
      <pubDate>Sun, 01 May 2022 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/go-redis-latency-debug/</guid>
      <description>Background 本文背景是这样的: 有一个线上服务使用了 go-redis 库连接 redis，目前 QPS 也不是很高，大约每秒一次的样子，但是通过 log 发现每次 redis 操作花费的时间如下：
redis call cost: 0 ms redis call cost: 2 ms redis call cost: 0 ms redis call cost: 1 ms redis call cost: 0 ms redis call cost: 17 ms redis call cost: 0 ms .... 正常一个简单的 redis get 操作耗费 0-3ms 时间可以理解，但是为什么会出现 17 ms 呢？ 而且出现的频率还不低，大概每 30 个正常的中会出现一个。
尝试 debug 首先总结一下场景和条件
service 部署在 k8s 中，大概 10 个 pod 在运行。 整个 service 的 QPS 大概一秒一个，很低。 高延迟的情况大概每 30 个 log 出现一个。 service 使用简单的 redis get()，没有复杂操作。 但是 service 本身是有很多 go routine 并发的。 所以可能出现问题的地方</description>
    </item>
    <item>
      <title>gRPC client 如何实现 TCP 重连</title>
      <link>https://runzhen.github.io/posts/how-does-grpc-client-reconnect-tcp/</link>
      <pubDate>Mon, 04 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/how-does-grpc-client-reconnect-tcp/</guid>
      <description>之前写过一篇 gRPC-go 建立 TCP 连接的过程 博客，主要研究了 client 程序启动后，如何与 server 建立 TCP 连接。
今天，在思考 redis-go 的连接池实现的时候，突然想到：
当 gRPC 的 TCP 连接断开后，能自动重连吗？ 如果可以，是如何实现的 ？ 首先要注意，这里指的是 TCP 连接，而不是 http2 中的 stream。 我们知道，gRPC 数据的传输使用 http2 的多路复用，也就是在一个 TCP 连接上有多个全双工的 http2 stream，这里的 stream 如果被断开后怎么重连与 http2 的实现有关，不在本文讨论范围。
对于上面第一个问题，使用 gRPC 的经验告诉我是可以自动重连的，不妨再做个简单的测试，client 端代码如下：
func main() { conn, _ := grpc.Dial(&amp;#34;127.0.0.1:8080&amp;#34;, grpc.WithInsecure()) defer conn.Close() cli := protobuf.NewTestClient(conn) req := &amp;amp;protobuf.EchoRequest{ Msg: &amp;#34;hi&amp;#34;, } for i := 0; i &amp;lt; 10000; i++ { time.</description>
    </item>
    <item>
      <title>Gin HTTP 框架学习笔记</title>
      <link>https://runzhen.github.io/posts/gin-framework/</link>
      <pubDate>Fri, 03 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/gin-framework/</guid>
      <description>最近要做一个 REST API server，在网上搜索了一遍以后，发现常用的是 Gin 和 Echo，并且很多人都说 golang 本身提供的 http server 已经足够强大，gin 和 echo 也只是在外包了一层。
我看 Gin 的源码行数比 Echo 少很多，而且测试覆盖率也高很多，因此决定学习一下 Gin，本文目标有以下这些
学习如何设计一个 REST 风格的 server ？ 学习 Gin 在 go 自带的 http server 基础上做了哪些工作？ 启动 Gin http server 在使用 Gin 框架的时候，最后都会调用 gin.Run(&amp;quot;:8080&amp;quot;) ，这样你的 http server 就可以就收 client 请求了，
func (engine *Engine) Run(addr ...string) (err error) { defer func() { debugPrintError(err) }() address := resolveAddress(addr) debugPrint(&amp;#34;Listening and serving HTTP on %s\n&amp;#34;, address) err = http.</description>
    </item>
    <item>
      <title>错误使用 time.After() 导致内存泄漏</title>
      <link>https://runzhen.github.io/posts/golang-timer-leak/</link>
      <pubDate>Sun, 08 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-timer-leak/</guid>
      <description>今天看到了一篇有关 timer 泄露的文章，觉得很有意思，于是把它记录下来。
一般没有问题的写法 说道 time.After() 会导致内存泄露，很多人一定会觉得奇怪，因为代码里经常会用到它，也没见有内存泄漏啊？
是的，一般我们这样写的话是没有问题的
func main() { ch := make(chan int) go func() { ch &amp;lt;- 1 }() select { case _ = &amp;lt;-ch: case &amp;lt;-time.After(time.Second * 1): fmt.Println(&amp;#34;timeout&amp;#34;) } } 有问题的写法 那么，什么样的写法有问题呢？ 当使用 for loop 的时候，比如这样
for { select { case _ = &amp;lt;-ch: // do something... continue case &amp;lt;-time.After(300 * time.Millisecond): fmt.Printf(&amp;#34;time.After() fire！\n&amp;#34;) } } 很不幸的是，上面这样的写法也非常常见，我自己就写过这样的代码。那么它真的会造成内存泄露吗？试一下便知道
前一篇博客中已经介绍了如何使用 pprof 对 Go 程序进行 profiling，简单提一下步骤
在代码中引入 _ &amp;quot;net/http/pprof&amp;quot;, 并开启一个http server 导出 metrics 运行你的 binary 执行 go tool pprof -http=:8081 http://localhost:6060/debug/pprof/heap 浏览器就会自动打开 localhost:8081 显示结果了 测试代码如下：</description>
    </item>
    <item>
      <title>Golang pprof 的使用姿势</title>
      <link>https://runzhen.github.io/posts/golang-pprof-usage/</link>
      <pubDate>Sun, 18 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-pprof-usage/</guid>
      <description>首先，在代码中引入 pprof 的方式非常简单，只要把下面这段代码放到 main 函数中即可
_ &amp;#34;net/http/pprof&amp;#34; go func() { if err := http.ListenAndServe(&amp;#34;:9090&amp;#34;, nil); err != nil { panic(err) } os.Exit(0) }() 然后启动你的程序，再用以下这些命令去对应的端口做 profiling
// cpu profile 默认从当前开始收集 30s 的 cpu 使用情况，需要等待 30s go tool pprof http://47.93.238.9:9090/debug/pprof/profile # wait 120s go tool pprof http://47.93.238.9:9090/debug/pprof/profile?seconds=120 // 以下 second 参数不起作用，因为采样是一瞬间完成的 go tool pprof http://47.93.238.9:9090/debug/pprof/heap go tool pprof http://47.93.238.9:9090/debug/pprof/goroutine go tool pprof http://47.93.238.9:9090/debug/pprof/block go tool pprof http://47.93.238.9:9090/debug/pprof/mutex 还有一种是 import &amp;quot;runtime/pprof“的方式，这种不太常用，不在本文范围。
运行了 go tool pprof 命令以后，会进入到一个交互界面，</description>
    </item>
    <item>
      <title>Golang Channel 用法总结</title>
      <link>https://runzhen.github.io/posts/golang-channel-usage/</link>
      <pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-channel-usage/</guid>
      <description>之前的博客中已经粗略探究了一下 golang channel 的实现原理，本文总结一下使用 channel 的各种姿势。
先看一下对不同状态的 channel 的读，写，关闭操作的结果
1. 使用 for range 读取 channel 场景： 当需要不断从 channel 里读数据时
这是最常用的方式，又安全又便利，当channel 被关闭时，for 循环自动退出。 用法不再赘述。
2. 使用 _, ok 判断 channel 是否关闭 场景: 读 channel，但需要判断 channel 是否已关闭。
读 channel 的操作 &amp;lt;- chan 既可以返回一个值，也可以返回两个值，这里就是用的两个返回值的方式。
举例：
if v, ok := &amp;lt;- ch; ok { // can read channel fmt.Println(v) } 读到数据，并且通道没有关闭时，ok 的值为 true。 通道关闭，无数据读到时，ok 的值为 false。 3. 与 select 搭配使用 场景: 需要对多个通道进行处理，或者设置超时
举例：
func (h *Handler) handle(job *Job) { select { case h.</description>
    </item>
    <item>
      <title>如何设计一个连接池</title>
      <link>https://runzhen.github.io/posts/go-redis-conn-pool/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/go-redis-conn-pool/</guid>
      <description>事情的起因是我在 k8s 中部署了一个 redis，然后 service A 使用 go-redis 库连接 redis。
这个时候我想到： service Pod 和 redis Pod 启动的顺序是不一定的，可能是 service Pod 先启动，此时 redis pod 还没有启动；又或者 redis pod 中途 restart 了。 go-redis 库能正确的处理重连吗？
简单的用 kubectl 命令删除、 重启了 redis，发现 service Pod 能自动恢复连接，说明 go-redis 正确进行了处理，那么它是怎么做的呢 ？
在寻找答案之前，先来想想如果是我自己实现，需要哪些功能？ 该怎么实现？
conn pool 需要自动删除已经断开的、坏掉的连接。 (开一个 goroutine 定期检查即可) 能自动新建连接，补齐一定数量的 conn。 (也不难，goroutine 即可) 如何检测一个 conn 是不是出错了？ 对外的接口是 Get 和 Put，除了正常的用 mutext 控制并发以外，还有什么特殊的操作吗？ go-redis 源码位于 redis/v8/internal/pool/pool.go , 首先看 pool.Options 数据结构
type Options struct { PoolSize int // 连接池数量 MinIdleConns int // 最小空闲连接数 MaxConnAge time.</description>
    </item>
    <item>
      <title>Goroutine 的 PMG 模型</title>
      <link>https://runzhen.github.io/posts/golang-runtime-pmg-1/</link>
      <pubDate>Mon, 19 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-runtime-pmg-1/</guid>
      <description>稍微了解过 Go runtime 的人想必都听过 goroutine 的 PMG 模型，哪么它到底代表什么意思呢？ Golang 源码中又是如何实现的？
前言 关于 PMG 的解释网上有很对，随便 copy 一个：
M 代表 Machine，系统线程，它由操作系统管理的，goroutine就是跑在M之上的；M 是一个很大的结构，里面维护小对象内存cache（mcache）、当前执行的goroutine、随机数发生器等等非常多的信息。 P 是 Processor，处理器，它的主要用途就是用来执行goroutine的，它维护了一个goroutine队列，即runqueue。Processor是让我们从N:1调度到M:N调度的重要部分。 G 代表 goroutine 它包含了栈，指令指针，以及其他对调度goroutine很重要的信息，例如其阻塞的channel。 通常 go 程序中可以用 GOMAXPROCS 设置 Processor 的个数； 而 M 则是 clone系统调用创建的，或者用linux pthread 库创建出来的线程实体。 M 与 P 是一对一的关系。
基本结构体 打开 src/runtime/runtime2.go 文件，p,m,g 三个结构体的定义是按顺序在一起的，除此之外还有一个 schedt，与 goroutine 的调度相关。
g 结构体 G 就是 goroutine 的意思，每个 Goroutine 对应一个 g 结构体，它有自己的栈内存, G 存储 Goroutine 的运行堆栈、状态以及任务函数。 当一个 goroutine 退出时，g 会被放到一个空闲的对象池中以用于后续的 goroutine 的使用， 以减少内存分配开销。</description>
    </item>
    <item>
      <title>Golang Channel 的实现</title>
      <link>https://runzhen.github.io/posts/golang-channel/</link>
      <pubDate>Sat, 10 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-channel/</guid>
      <description>Channel 可以说是 Go 语言最具特色的设计了，我们经常会看到一些老鸟这样教育菜鸟：
Do not communicate by sharing memory; instead, share memory by communicating.
那么熟练使用 Golang 就离不开 channel，有必要了解一下 channel 是怎么实现的。
channel 的源代码在 Golang 的 src/runtime/chan.go 目录下，先看结构体:
type hchan struct { qcount uint // 循环列表元素个数 dataqsiz uint // 循环队列的大小 buf unsafe.Pointer // 循环队列的指针 elemsize uint16 // channel 中元素的大小 closed uint32 // 是否已close elemtype *_type // channel 中元素类型 sendx uint // send 在buffer中的索引 recvx uint // recv 在buffer中的索引 recvq waitq // receiver 的等待队列 sendq waitq // sender 的等待队列 lock mutex } type waitq struct { first *sudog last *sudog } 其中</description>
    </item>
    <item>
      <title>Golang WaitGroup 的实现</title>
      <link>https://runzhen.github.io/posts/golang-waitgroup/</link>
      <pubDate>Mon, 05 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-waitgroup/</guid>
      <description>sync.WaitGroup 的作用就是让主函数等待所有 goroutine 都执行完毕，再退出。
一个最简单的例子如下，如果没有 wg，那么 main 会在 goroutine 执行之前就退出，从而不会看到任何 output。
func main() { wg := sync.WaitGroup{} for i := 0; i &amp;lt; 3; i++ { wg.Add(1) go func(i int) { fmt.Println(i) wg.Done() }(i) } wg.Wait() } 那么 WaitGroup 是如何实现的呢？
万变不离其宗，其底层还是基于 go runtime 提供的信号量机制，也就是 runtime_Semrelease() 和 runtime_Semacquire()， 在之前的文章 Golang RWMutex 的实现 和 netpoll 的实现 中都有它们的影子存在。
runtime_Semacquire(s *uint32) 此函数会阻塞直到信号量*s的值大于0，原子减这个值。 runtime_Semrelease(s *uint32, lifo bool, skipframes int) 此函数执行原子增信号量的值，然后通知被runtime_Semacquire阻塞的协程 说到底，就是用 信号量 和 gopark 来控制 goroutine 是运行还是挂起，wg.</description>
    </item>
    <item>
      <title>Golang reflect 的使用</title>
      <link>https://runzhen.github.io/posts/golang-reflect/</link>
      <pubDate>Sat, 03 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-reflect/</guid>
      <description>所谓反射 (refection) 是指程序在运行过程中获取变量的类型、属性。 在 Golang 中，有时我们会看到 reflect.ValueOf() 或者 reflect.TypeOf() 这两个函数，这就是反射出一个变量的值和类型。 gPRC 的实现中也大量运用了反射。
本文主要介绍如何使用 reflect 包，关于 Go 内部是如何实现的将在下一篇文章中介绍。
TypeOf 和 ValueOf 先看一个最简单的例子
type User struct { Name string Age int } func main() { u := User{&amp;#34;Dick&amp;#34;, 18} t := reflect.TypeOf(u) v := reflect.ValueOf(u) fmt.Printf(&amp;#34;u type = %T, %v\n&amp;#34;, u, u) fmt.Printf(&amp;#34;t type = %T, %v\n&amp;#34;, t, t) fmt.Printf(&amp;#34;v type = %T, %v\n&amp;#34;, v, v) // 获取 v 的值 // v.Age , 错误，因为 v 是 reflect.</description>
    </item>
    <item>
      <title>Golang io 包的实现</title>
      <link>https://runzhen.github.io/posts/golang-io-package/</link>
      <pubDate>Fri, 02 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-io-package/</guid>
      <description>Golang 的 io package 包含 3 个文件 io.go, multi.go, pipe.go, 其中最主要的时 io.go。
当我们打开 io.go 的源码后，发现这个文件里面定义了大量的接口，实际上，io 包的作用就是如此 - 定义基本的 Read / Write inteface，而把具体的实现交给其他 package，比如 strings package 中就专门实现了 reader/writer，在后面的文章中再分析 strings 包。
接下来就看看 io 包中到底包含了哪些东西。
io.go 首先时定义了 4 个基础操作，读，写，关闭，seek
type Reader interface { Read(p []byte) (n int, err error) } type Writer interface { Write(p []byte) (n int, err error) } type Closer interface { Close() error } type Seeker interface { Seek(offset int64, whence int) (int64, error) } 基于这 4 个基础 interface，两两组合，有扩展了下面几个 interface</description>
    </item>
    <item>
      <title>Golang Context 的实现</title>
      <link>https://runzhen.github.io/posts/golang-context/</link>
      <pubDate>Sun, 28 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-context/</guid>
      <description>有这样一个场景： 父 goroutine 创建了多个子 goroutine 来处理某个请求，当这些子 goroutine 中任何一个出错的时候，我们希望所有的 goroutine 都停止。 该如何实现呢？
熟悉 Go 语言的可能首先想到用 context，而 context 主要是依靠 channel 来实现以上功能。
看了一下具体的实现，主要思想是:
每种类型的 ctx 都实现了 context.Context 接口的 Done() 函数 Done() &amp;lt;-chan struct{} 函数返回一个只读的 channel 而且没有地方向这个channel里写数据。所以直接调用这个只读channel会被阻塞。 一般通过搭配 select 来使用。一旦 channel 关闭，就会立即读出零值。 谁来关闭这个 channel 呢？ 用户主动调用返回的 CancelFunc，或者 timeout 超时 另外，在使用上配合 select 语句阻塞处理 Done() 才能起到预期的效果。
下面举两个如何使用 context 的例子，第一个例子如下
func main() { ctx, cancel := context.WithTimeout(context.Background(), 1*time.Second) defer cancel() go handle(ctx) // 等待3秒再结束（只是为了让 main 不提前 exit，与本文无关） select { case &amp;lt;- time.</description>
    </item>
    <item>
      <title>Golang 读写锁的实现</title>
      <link>https://runzhen.github.io/posts/golang-rw-lock/</link>
      <pubDate>Thu, 18 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-rw-lock/</guid>
      <description>type RWMutex struct { w Mutex // held if there are pending writers writerSem uint32 // semaphore for writers to wait for completing readers readerSem uint32 // semaphore for readers to wait for completing writers readerCount int32 // number of pending readers readerWait int32 // number of departing readers } writerSem 是写入操作的信号量 readerSem 是读操作的信号量 readerCount 是当前读操作的个数 readerWait 当前写入操作需要等待读操作解锁的个数 其中 semaphore 就是操作系统课程里面学到的信号量的概念。
读写锁的实现非常简单，源码在 /usr/local/go/src/sync/rwmutex.go 下，我们可以逐一分析它的各个函数
读者加读锁 首先是读锁，读者进入临界区之前，把 readerCount 加一，
如果这个值小于 0，则调用runtime_SemacquireMutex 把自己所在的 goroutine 挂起。 如果大于等于 0， 则加读锁成功 func (rw *RWMutex) RLock() { if atomic.</description>
    </item>
    <item>
      <title>epoll 在 Golang net 库的使用</title>
      <link>https://runzhen.github.io/posts/golang-net-epoll/</link>
      <pubDate>Sun, 31 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/golang-net-epoll/</guid>
      <description>本文主要关注以下几个问题:
Golang runtime 是怎么调用 epoll 的系统调用的 ？ Golang net 库如何封装 epoll，使得开发者几乎不用直接操作 epoll ? C 如何调用 epoll 首先回顾一下用 C 语言怎么使用 epoll
int s = socket(AF_INET, SOCK_STREAM, 0); bind(s...) listen(s...) int epfd = epoll_create(128); //创建eventpoll对象 ev.events = _EPOLLIN | _EPOLLOUT | _EPOLLRDHUP | _EPOLLET epoll_ctl(epfd, EPOLL_CTL_ADD, s, &amp;amp;ev);//注册事件 //轮询就绪事件 while(true){ //返回值n为就绪的事件数,events为事件列表 int n = epoll_wait(epfd, &amp;amp;events[0], len(events), 1000) for( i := 0; i &amp;lt; n; i++ ) { ev := &amp;amp;events[i] //处理事件 } } C 语言中调用 epoll 的方式比较底层，总的来说分下面三个步骤</description>
    </item>
    <item>
      <title>Goroutine Pool 实现高并发</title>
      <link>https://runzhen.github.io/posts/goroutine-pool/</link>
      <pubDate>Tue, 13 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/goroutine-pool/</guid>
      <description>本文是读完 Handling 1 Million Requests per Minute with Go 之后，根据自己的理解，对文中提到的并发模型和实现再梳理一遍。
前言 假设有一个 http server 接收 client 发来的 request，如果用下面的这样的代码，会有什么问题呢？
func payloadHandler(w http.ResponseWriter, r *http.Request) { // Go through each payload and queue items individually to be posted to S3 for _, payload := range content.Payloads { go payload.UploadToS3() // &amp;lt;----- DON&amp;#39;T DO THIS } } 显而易见，有 2 个问题：
接收一个 request 就开启一个 goroutine 处理，当 request 数量在短时间内暴增的话，光是 goroutine 的数量都足以让 server 崩溃。 每个 goroutine 都会与后端建立 TCP 连接，既耗费三次握手的时间，也会造成后端有大量 TCP 连接 所以，我们的目标是 没有蛀牙</description>
    </item>
    <item>
      <title>gRPC-go 建立 TCP 连接的过程</title>
      <link>https://runzhen.github.io/posts/grpc-client-tcp-connection/</link>
      <pubDate>Sun, 11 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://runzhen.github.io/posts/grpc-client-tcp-connection/</guid>
      <description>首先看一个最简单的建立 client server 之间 gRPC 连接的代码，以这个代码为例，分析一下 TCP 是在何时建立的。
Server 端的代码相对来说很容易，一个最简单的 server 代码如下：
func main() { lis, _ := net.Listen(&amp;#34;tcp&amp;#34;, fmt.Sprintf(&amp;#34;:%d&amp;#34;, 8080)) grpcServer := grpc.NewServer() protobuf.RegisterTestServer(grpcServer, &amp;amp;server{}) grpcServer.Serve(lis) } 在 grpc/server.go 中的 Serve() 函数调用了 lis.Accept() 并阻塞，当 client 端发来 TCP 请求时，Accept() 返回 Conn 结构，并开启 goroutine handleRawConn() 进行后续的处理。
就 TCP 来说，server 端的代码简单易懂，相比之下 client 端则不一样，一个基本的 Client 代码如下：
func main() { conn, err := grpc.Dial(&amp;#34;localhost:8080&amp;#34;, grpc.WithInsecure()) defer conn.Close() cli := protobuf.NewTestClient(conn) } 而要弄清楚 Client 端如何建立 TCP 却不容易，这是因为 grpc client 有 resolve DNS 以及做 load balancer 的功能，因此代码复杂很多。</description>
    </item>
    <item>
      <title>Golang 操作共享内存</title>
      <link>https://runzhen.github.io/posts/share-memory-golang/</link>
      <pubDate>Fri, 02 Aug 2019 00:33:33 -0700</pubDate>
      <guid>https://runzhen.github.io/posts/share-memory-golang/</guid>
      <description>前言 进程间通信的方式有很多种，如果两个进程分别在不同的机器上，那么使用 socket 通信；如果在同一台机器上，共享内存机制是一种快速高效的方式。
本文实现一个 go 语言二进制程序和 C 语言二进制程序通过共享内存交换数据。
提到共享内存主要有两种：
System V 标准的 shmget/shmdt 等接口 POSIX 标准的 shm_open 等接口 另外 Linux 下 mmap() 匿名映射也是最常用的进程间共享内存方法。
创建了共享内存以后，一般会显示在系统的 /dev/shm 目录下。Linux 默认 /dev/shm 为实际物理内存的1/2, 比如我的机器上物理内存为 16G，运行 df 命令后可以看到 /dev/shm 的大小为 7.8G 。
$ df -h Filesystem Size Used Avail Use% Mounted on tmpfs 1.6G 3.2M 1.6G 1% /run tmpfs 7.8G 4.0K 7.8G 1% /dev/shm tmpfs, ramfs 和 ramdisk
tmpfs是一个虚拟内存文件系统，在Linux内核中，虚拟内存资源由物理内存(RAM)和交换分区组成，Tmpfs可以使用物理内存，也可以使用交换分区。
ramdisk 是一个块设备，只不过它是存在于内存上的。
ramfs 也是文件系统，不过已经被 tmpfs 替代了。</description>
    </item>
  </channel>
</rss>
