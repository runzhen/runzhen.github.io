<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.125.5"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Mind in the Wind</title>
<meta name=keywords content="Blog,Portfolio,PaperMod"><meta name=description content="ExampleSite description"><meta name=author content="Me"><link rel=canonical href=https://runzhen.github.io/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.a8f620eb24ba9442cd3765590f9208a0752be50e5a7b9dd9e3e555c8cb5e74e6.css integrity="sha256-qPYg6yS6lELNN2VZD5IIoHUr5Q5ae53Z4+VVyMtedOY=" rel="preload stylesheet" as=style><link rel=icon href=https://runzhen.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://runzhen.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://runzhen.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://runzhen.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://runzhen.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://runzhen.github.io/index.xml><link rel=alternate hreflang=en href=https://runzhen.github.io/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="Mind in the Wind"><meta property="og:description" content="ExampleSite description"><meta property="og:type" content="website"><meta property="og:url" content="https://runzhen.github.io/"><meta property="og:image" content="https://runzhen.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="Mind in the Wind"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://runzhen.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Mind in the Wind"><meta name=twitter:description content="ExampleSite description"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Mind in the Wind","url":"https://runzhen.github.io/","description":"ExampleSite description","thumbnailUrl":"https://runzhen.github.io/%3Clink%20/%20abs%20url%3E","sameAs":["/index.xml","https://github.com/run"]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://runzhen.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://runzhen.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://runzhen.github.io/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://runzhen.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://runzhen.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://runzhen.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Kubeflow 部署 MNIST</h2></header><div class=entry-content><p>在阅读本文之前，假设已经在 GCP 上安装好了 Kubeflow。
首先进入 Kubeflow，点击 Notebook Server，新建一个 Jupyter Notebook。
新建的时候会让你输入 Name 和 Namespace，在 Kubeflow 中，每个用户都在 k8s 集群上有自己的 Namespace。
这里输入的 Name 对应的 Notebook Pod 最后会在自己的 Namespace 下。
新的 Notebook 里面是空的，我们需要下载一些例子。打开 terminal
然后输入 git clone 命令：
git clone https://github.com/kubeflow/examples.git
回到默认界面会看到刚刚 clone 的项目，打开 mnist 目录下的 mnist_gcp.ipynb
开始 首先第一个问题，当我打开这个 Jupyter Notebook 的 WebUI 时，它运行在哪里？
Notebook 是在哪个 Pod $ kubectl -n Your-namespace get pod NAME READY STATUS RESTARTS AGE fairing-builder-chvkq-6s4cn 0/1 Completed 0 3d23h mnist-model-7886dcbb5b-t2kk8 1/1 Running 0 3d22h mnist-tensorboard-774c585b7c-65766 2/2 Running 0 21h mnist-train-2596-chief-0 0/1 Completed 0 3d22h mnist-train-2596-worker-0 0/1 Completed 0 3d22h mnist-ui-7f95c8498b-xqsfs 2/2 Running 0 3d22h test1-0 2/2 Running 0 3d23h test1-0 是之前在 UI里面创建 Notebook server 时定下的名字，于是test1-0...</p></div><footer class=entry-footer><span title='2020-03-22 00:00:00 +0000 UTC'>2020-03-22</span>&nbsp;·&nbsp;Me</footer><a class=entry-link aria-label="post link to Kubeflow 部署 MNIST" href=https://runzhen.github.io/posts/mnist_step_by_step/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[Istio] 使用 istio 控制转发流量</h2></header><div class=entry-content><p>概念 首先介绍几个概念，Ingress 指的是进入到 k8s 集群中的 traffic，比如一个 client 发起的 HTTP 请求，经过层层网络最终到达了 k8s cluster 的外部，那么让不让它进入到 cluster 内部就是 ingress controller 做的事。
Kubernetes 原生提供了自己的 Ingress Controller，此外还有很多第三方的 Ingress Controller，istio 就是其中之一。需要注意的是，本文所有部署都是基于 GKE，其他的云平台可能略有不同。
在 k8s 中安装了 istio 之后，就可以用 istio 来控制所有进入 cluster 的流量。如何安装 istio 不在本文的范围，读者可以参考 istio 官方文档。
安装 istio 完成之后，kubectl get namespace 命令可以看到有个名叫 istio-system 的空间，所有 istio 组件的 pod 都在这个空间中。
然后用如下命令查看 istio ingressgateway
$ kubectl -n istio-system get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT istio-ingressgateway LoadBalancer 10.0.23.180 35.222.xxx.xxx 15020:32011/TCP,80:30444/TCP 我们会看到这个名叫 istio-ingressgateway 的 LoadBalancer 它有公网 IP 地址 35....</p></div><footer class=entry-footer><span title='2020-03-15 00:00:00 +0000 UTC'>2020-03-15</span>&nbsp;·&nbsp;Me</footer><a class=entry-link aria-label="post link to [Istio] 使用 istio 控制转发流量" href=https://runzhen.github.io/posts/use-istio/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>用 Grafana 展示监控状态</h2></header><div class=entry-content><p>运维或者 SRE 部门经常会弄一个大屏幕展示各种系统状态，看上去很好玩，于是我也用类似的开源软件监控一下家里的主机。
整个过程非常简单，主要是安装三个软件 Node exporter，Prometheus，Grafana。
Node exporter 既然要展示系统状态，那么第一步就是要获得系统的状态数据，比如 CPU 使用率，内存使用率，网络流量等。
Prometheus 官方提供了一个使用 go 语言编写的程序 node_exporter，直接下载项目主页上 release 里的二进制即可。node_exporter 最好直接安装在物理主机上，因为这样才能采集到最准确的数据。
运行 node_exporter 以后，会自动启动一个 http server 并且监听 9100 端口，如果有 client 过来访问， server 返回主机的监控信息。比如：
$ curl http://localhost:9100/metrics node_network_transmit_packets_total{device="veth126cb08"} 28859 node_network_transmit_packets_total{device="veth1276a16"} 1383 node_network_transmit_packets_total{device="veth749c501"} 1.108492e+06 返回信息的格式是符合 Prometheus 定义的标准的，因此 Prometheus 能够处理并以简单的图标的形式展现这些数据。
看到这里大家应该不难想到，如果我自己写一个程序 HelloWorld，并且把程序的状态按照一定的格式导出，那么同样可以通过 Prometheus + Grafana 展现。
Prometheus Prometheus 是一个功能齐全的数据库，还提供了 PromSQL 语言方便用户查询，以及一个简单的网页前端。
最简单快捷的方式当然是启动一个容器，唯一需要注意的是把配置文件 prometheus.yml 挂载到容器的 /etc/prometheus/ 目录下。
$ docker run -d -p 9090:9090 \ -v /home/prometheus/:/etc/prometheus/ prom/prometheus 配置文件中需要在 scrape_configs 部分添加 noder exporter 的 IP 地址和端口。...</p></div><footer class=entry-footer><span title='2020-02-02 00:00:00 +0000 UTC'>2020-02-02</span>&nbsp;·&nbsp;Me</footer><a class=entry-link aria-label="post link to 用 Grafana 展示监控状态" href=https://runzhen.github.io/posts/grafana-dashboard/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Kubernetes 部署 Tensorflow Serving</h2></header><div class=entry-content><p>本文记录如何把 TensorFlow ResNet 模型部署在本地 Kubernetes 集群上，并提供一个 grpc 端口供集群外部访问。
本文不牵涉 ResNet（Deep residual networks）模型的实现细节，只讨论部署。
本文来源于 TensorFlow 官网上的一个例子，但正如大多数项目的文档一样，文档落后于项目的发展，因此有一些小坑，这里记录一下。
下载 ResNet 模型数据 这一步没什么好说的，按照步骤下载就行了
mkdir /tmp/resnet curl -s http://download.tensorflow.org/models/official/20181001_resnet/savedmodels/resnet_v2_fp32_savedmodel_NHWC_jpg.tar.gz | \ tar --strip-components=2 -C /tmp/resnet -xvz 制作并启动 ResNet serving 因为我们要把这个 serving 部署到 k8s，所以制作 docker 镜像是必须的。
先启动运行一个空 serving 镜像：
docker run -d --name serving_base tensorflow/serving 然后把刚刚下载的 /tmp/resnet 文件夹下的所有内容拷贝到容器中：
docker cp /tmp/resnet serving_base:/models/resnet 最后，commit 生成一个自己的 image
docker commit --change "ENV MODEL_NAME resnet" serving_base resnet_serving docker kill serving_base docker rm serving_base 然后我们试着运行一下这个镜像，要是看到类似如下输出，证明启动正常。...</p></div><footer class=entry-footer><span title='2020-01-20 20:22:33 -0800 PST'>2020-01-20</span>&nbsp;·&nbsp;Me</footer><a class=entry-link aria-label="post link to Kubernetes 部署 Tensorflow Serving" href=https://runzhen.github.io/posts/kubernetes-tensorflow-serving/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Kubernetes DNS</h2></header><div class=entry-content><p>在介绍 Kubernetes 中的 DNS 之前，我们先来看看 Kubernetes 中的另一个概念 Service，以及为什么需要 Service。
什么是 Service 我们知道 k8s 集群中应用的部署是以 Pod 为单位的，在 Pod 内执行 ifconfig 可以看到每个 Pod 都有自己的 IP，这个 IP 在集群内部是唯一的，其他 Pod 都能 ping 这个地址。
这样的设计使得 Pod 里的应用程序可以直接交互。
另一方面，Pod 的生命周期是短暂的，因此 Pod IP 也是不断变化的，而且 Pod 也会有多个副本。
那么问题来了： 其他程序访问这个 Pod 时该用哪个 IP 地址呢 ？
这个时候就需要 Service 出场了， Service 对外只会提供一个 IP，一个请求到来时 Service 决定该转发到后面哪一个 Pod 上。
我们可以理解为 Service 加上的一组 Pod 可以看做是一个微服务。
Service 对外提供 ClusterIP, NodePort 等访问方式。
Kubernetes DNS 上面提到 Service 对外提供一个唯一的 IP，但这个 IP 偶尔也会随着 service 的更新改变，所以还需要一个 k8s 集群内部的 DNS 把服务对应到 IP 上。...</p></div><footer class=entry-footer><span title='2020-01-19 21:47:47 -0800 PST'>2020-01-19</span>&nbsp;·&nbsp;Me</footer><a class=entry-link aria-label="post link to Kubernetes DNS" href=https://runzhen.github.io/posts/kubernetes-dns/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>安装 Kubernetes Dashboard</h2></header><div class=entry-content><p>Kubernetes Dashboard 是一个 Web UI 的集群管理工具。项目主页在这里
首先根据它的主页上 README 里面的内容直接 kubectl apply -f recommended.yaml，这样集群中就会创建并运行 dashboard 的 POD。
接下来的问题是如何从外界访问到这个 UI。
我的 k8s 集群环境是一台物理主机上的三台虚拟机，每个虚拟机都是 Headless 启动，也就是说纯命令行没有桌面环境，无法打开浏览器，因此项目主页上说的 kubectl proxy 访问 http://localhost:8001 的方式不适用。
我希望最终能从物理主机上访问到这个 WebUI。
使用 NodePort 我们查看一下 kubernetes-dashboard 使用 recommended.yaml 部署之后 service 的类型是 ClusterIP
$ kubectl get svc --all-namespaces NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.96.0.1 &lt;none> 443/TCP 17d default redis-nodeport NodePort 10.96.39.42 &lt;none> 6379:31250/TCP 15d kube-system kube-dns ClusterIP 10.96.0.10 &lt;none> 53/UDP,53/TCP,9153/TCP 17d kubernetes-dashboard dashboard-metrics-scraper ClusterIP 10....</p></div><footer class=entry-footer><span title='2020-01-18 23:07:24 -0800 PST'>2020-01-18</span>&nbsp;·&nbsp;Me</footer><a class=entry-link aria-label="post link to 安装 Kubernetes Dashboard" href=https://runzhen.github.io/posts/kubernetes-dashboard/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>本地搭建三节点 Kubenetes</h2></header><div class=entry-content><p>赶在 2019 年快结束之际，写一篇博客作为本年度的收官之作吧。
前言 虽然现在各大云平台厂商都提供了一键搭建 kubenetes 的服务，但缺点是费用太贵了，如果仅仅把它作为自己没事折腾的小玩具非常不划算，另外虽然也可以用公司的账号，但并不想把自己的折腾的东西跟工作混为一谈。
所以决定自己在主机上手动搭建一个。
准备工作 一般 kubenetes 至少要三台 linux 主机组建成一个 cluster，因为手头没有三台 linux 物理主机，所以要用虚拟机代替。
kubenetes 各个虚拟机节点的规划如下：
主机名 主机 IP OS 集群角色 192-168-56-10.master 192.168.56.10 Ubuntu 18.04 master 192-168-56-11.node 192.168.56.11 Ubuntu 18.04 node1 192-168-56-12.node 192.168.56.12 Ubuntu 18.04 node2 准备工作主要为下面几步：
物理主机内存 16G，操作系统为 Ubuntu 18.04 安装 Virtualbox 6.1 创建三台虚拟机，每台内存 4G 对三台虚拟机做基本配置 VirtualBox 虚拟机配置 默认只有一个 NAT 适配器，我们需要添加一个 Host-Only Adapter。NAT 适配器是虚拟机用来访问互联网的，Host-Only 适配器是用来虚拟机之间通信的。上面表格所指的 主机 IP 也是这个 Host only IP。
vbox 上配置一个 host only adapter， 默认名字 vboxnet0，网络地址 192....</p></div><footer class=entry-footer><span title='2019-12-28 00:00:00 +0000 UTC'>2019-12-28</span>&nbsp;·&nbsp;Me</footer><a class=entry-link aria-label="post link to 本地搭建三节点 Kubenetes" href=https://runzhen.github.io/posts/kubenetes-cluster-from-scratch/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>CPU affinity</h2></header><div class=entry-content><p>CPU affinity – CPU 亲和性，指进程更希望运行在哪个 CPU core 上。
指定 core 有什么好处呢？
比如，可以自己决定哪些程序可以独占 CPU 资源，保证这个程序性能的最大化； 指定 CPU 以后可以提高 Cache 的命中率，常用于一些对性能非常高要求的程序，例如 nginx。 命令行指令 taskset 在 Linux 系统中，我们可以用 taskset 命令指定一个进程运行在哪个核心上。
比如我们写一个程序用 while(1) 制造死循环，那么运行这个程序的时候 CPU 会飙到 100%
用以下这条命令运行这个程序
taskset -c 3 ./a.out 意思是把 a.out 运行在从 0 开始数起的第 3 个核心上。
于是，用 htop 命令查看，会看到第 4 个核 CPU 使用率是 100%。
编程的 API 那么在程序的代码里怎么用呢？ 先来看看 glibc 提供的系统 API
#include &lt;sched.h> int sched_setaffinity(pid_t pid, unsigned int cpusetsize, cpu_set_t *mask); int sched_getaffinity(pid_t pid, unsigned int cpusetsize, cpu_set_t *mask); void CPU_CLR(int cpu, cpu_set_t *set); int CPU_ISSET(int cpu, cpu_set_t *set); void CPU_SET(int cpu, cpu_set_t *set); void CPU_ZERO(cpu_set_t *set); nginx 的 config 文件中，可以为每个工作进程绑定CPU...</p></div><footer class=entry-footer><span title='2019-09-15 00:00:00 +0000 UTC'>2019-09-15</span>&nbsp;·&nbsp;Me</footer><a class=entry-link aria-label="post link to CPU affinity" href=https://runzhen.github.io/posts/taskset/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>从 Kubernetes 中访问 Memorystore</h2></header><div class=entry-content><p>Memorystore 是 Google Cloud 在 2018 年推出的托管 Redis 服务，让用户一键生成 Redis 实例，必要的时候再一键 scale，省去了维护 Redis 的烦恼。
本文在 k8s 中部署一个简单的小程序访问 Memorystore 数据库，获取 counter 值，并开启一个 http server 对外提供这个值。
准备 GCP 提供了一个命令行工具 gcloud，几乎所有的 web 操作都有对应的 CLI，非常方便。不同操作系统对应的安装包可以在这里下载
我的笔记本就叫它 “local host”，安装好 gcloud 之后，以下所有的操作都在 local 进行，命令执行的结果直接部署到 cloud 中。
现在开始前期准备工作。首先，在 GCP web 界面一键创建 Memorystore，之后我们能在 MemoryStore 的 Instances 里面看到这个实例，它的 IP 地址是 10.0.16.3 端口 6379。
很显然，10.0.16.3 这个 IP 是无法直接访问的，而如果你在相同的 GCP Project 里面创建了一个 VM instance，GCP 会自动创建一条路由，让你的 VM 可以 telnet 10.0.16.3 6379。
然后，创建一个 k8s 集群，这一步也同样可以在 web 界面里做，如果要用 GCP 提供的 gcloud 命令行的话如下：...</p></div><footer class=entry-footer><span title='2019-08-25 14:49:42 -0700 PDT'>2019-08-25</span>&nbsp;·&nbsp;Me</footer><a class=entry-link aria-label="post link to 从 Kubernetes 中访问 Memorystore" href=https://runzhen.github.io/posts/gke-access-memorystore/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Docker 中编译 vim8.0</h2></header><div class=entry-content><p>vim 的最新版 vim8.0 提供了很多新的特性，而且一些流行的 vim 插件很多功能也依赖于 8.0 版本，如果你要使用 vim8.0，那么最好的办法当然是使用操作系提供的软件包管理器一键安装，省时省力。
但是总有那么一些蛋疼的情况 ——你需要自己编译 vim。
本文就是记录下具体的步骤，并且把编译源码时需要安装的依赖软件全部做成 docker 镜像。
事情起因 某台服务器上我要用 vim8.0 的新特性，但是在服务器上我没有任何超级权限，只能读写我自己的 home 目录。
所以没法直接安装vim，只能从源码编译。
制作编译 vim 的docker image 编译 vim 需要系统中安装很多依赖软件，比如 vim 最基本的要包括 python2.7，luajit 等。
都 2019 年了，最好的方式当然是制作一个 docker 镜像，具体的步骤就不一一解释，贴上 Dockerfile 以示诚意。
FROM ubuntu:18.04 RUN apt-get update && apt-get install -y \ liblua5.1-dev \ luajit \ libluajit-5.1 \ python-dev \ ruby-dev \ libperl-dev \ libncurses5-dev \ libatk1.0-dev \ libx11-dev \ libxpm-dev \ libxt-dev \ gnupg2 \ curl \ && gpg2 --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDB \ && curl -sSL https://get....</p></div><footer class=entry-footer><span title='2019-08-16 00:00:00 +0000 UTC'>2019-08-16</span>&nbsp;·&nbsp;Me</footer><a class=entry-link aria-label="post link to Docker 中编译 vim8.0" href=https://runzhen.github.io/posts/build-your-vim8/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://runzhen.github.io/page/6/>«&nbsp;Prev&nbsp;
</a><a class=next href=https://runzhen.github.io/page/8/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://runzhen.github.io/>Mind in the Wind</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>