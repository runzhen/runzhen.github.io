<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.125.5"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Mind in the Wind</title>
<meta name=keywords content="Blog,Portfolio,PaperMod"><meta name=description content="ExampleSite description"><meta name=author content="Me"><link rel=canonical href=https://runzhen.github.io/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.a8f620eb24ba9442cd3765590f9208a0752be50e5a7b9dd9e3e555c8cb5e74e6.css integrity="sha256-qPYg6yS6lELNN2VZD5IIoHUr5Q5ae53Z4+VVyMtedOY=" rel="preload stylesheet" as=style><link rel=icon href=https://runzhen.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://runzhen.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://runzhen.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://runzhen.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://runzhen.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://runzhen.github.io/index.xml><link rel=alternate hreflang=en href=https://runzhen.github.io/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="Mind in the Wind"><meta property="og:description" content="ExampleSite description"><meta property="og:type" content="website"><meta property="og:url" content="https://runzhen.github.io/"><meta property="og:image" content="https://runzhen.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="Mind in the Wind"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://runzhen.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Mind in the Wind"><meta name=twitter:description content="ExampleSite description"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Mind in the Wind","url":"https://runzhen.github.io/","description":"ExampleSite description","thumbnailUrl":"https://runzhen.github.io/%3Clink%20/%20abs%20url%3E","sameAs":["/index.xml","https://github.com/run"]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://runzhen.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://runzhen.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://runzhen.github.io/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://runzhen.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://runzhen.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://runzhen.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Docker 的 privileged 模式</h2></header><div class=entry-content><p>无论是 docker 启动一个 container 还是在 k8s 中 deploy 一个 Pod 都可以指定 privileged 参数，之前在 Pod 的 spec YAML file 也里曾经用过，但是一直没有仔细想过加上这个参数后有什么不一样，今天就来研究一下。
首先来看一个最直观的对比，先运行一个没有 privileged 的容器：
$ docker run --rm -it ubuntu:18.04 bash root@e6f5f42c5b7e:/# ls /dev/ console core fd full mqueue null ptmx pts root@e6f5f42c5b7e:/# fdisk -l 再来看看如果加上了 privileged 会有什么不一样：
$ docker run --rm -it --privileged ubuntu:18.04 bash root@8e28f79eec9e:/# ls /dev/ tty11 tty2 tty28 tty36 tty44 tty52 tty60 ... ... root@8e28f79eec9e:/# fdisk -l Disk /dev/loop0: 97....</p></div><footer class=entry-footer><span title='2020-10-18 00:00:00 +0000 UTC'>2020-10-18</span>&nbsp;·&nbsp;Me</footer><a class=entry-link aria-label="post link to Docker 的 privileged 模式" href=https://runzhen.github.io/posts/docker-privileged/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Goroutine Pool 实现高并发</h2></header><div class=entry-content><p>本文是读完 Handling 1 Million Requests per Minute with Go 之后，根据自己的理解，对文中提到的并发模型和实现再梳理一遍。
前言 假设有一个 http server 接收 client 发来的 request，如果用下面的这样的代码，会有什么问题呢？
func payloadHandler(w http.ResponseWriter, r *http.Request) { // Go through each payload and queue items individually to be posted to S3 for _, payload := range content.Payloads { go payload.UploadToS3() // &lt;----- DON'T DO THIS } } 显而易见，有 2 个问题：
接收一个 request 就开启一个 goroutine 处理，当 request 数量在短时间内暴增的话，光是 goroutine 的数量都足以让 server 崩溃。 每个 goroutine 都会与后端建立 TCP 连接，既耗费三次握手的时间，也会造成后端有大量 TCP 连接 所以，我们的目标是 没有蛀牙...</p></div><footer class=entry-footer><span title='2020-10-13 00:00:00 +0000 UTC'>2020-10-13</span>&nbsp;·&nbsp;Me</footer><a class=entry-link aria-label="post link to Goroutine Pool 实现高并发" href=https://runzhen.github.io/posts/goroutine-pool/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>gRPC-go 建立 TCP 连接的过程</h2></header><div class=entry-content><p>首先看一个最简单的建立 client server 之间 gRPC 连接的代码，以这个代码为例，分析一下 TCP 是在何时建立的。
Server 端的代码相对来说很容易，一个最简单的 server 代码如下：
func main() { lis, _ := net.Listen("tcp", fmt.Sprintf(":%d", 8080)) grpcServer := grpc.NewServer() protobuf.RegisterTestServer(grpcServer, &amp;server{}) grpcServer.Serve(lis) } 在 grpc/server.go 中的 Serve() 函数调用了 lis.Accept() 并阻塞，当 client 端发来 TCP 请求时，Accept() 返回 Conn 结构，并开启 goroutine handleRawConn() 进行后续的处理。
就 TCP 来说，server 端的代码简单易懂，相比之下 client 端则不一样，一个基本的 Client 代码如下：
func main() { conn, err := grpc.Dial("localhost:8080", grpc.WithInsecure()) defer conn.Close() cli := protobuf.NewTestClient(conn) } 而要弄清楚 Client 端如何建立 TCP 却不容易，这是因为 grpc client 有 resolve DNS 以及做 load balancer 的功能，因此代码复杂很多。...</p></div><footer class=entry-footer><span title='2020-10-11 00:00:00 +0000 UTC'>2020-10-11</span>&nbsp;·&nbsp;Me</footer><a class=entry-link aria-label="post link to gRPC-go 建立 TCP 连接的过程" href=https://runzhen.github.io/posts/grpc-client-tcp-connection/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Spanner Distributed Database 阅读</h2></header><div class=entry-content><p>Introduction Spanner 数据库中的数据是分片(Shard)分散存储在多个数据中心的，数据是用 Paxos 算法(状态机)保证一致性。如果数据中心发生变化，Spanner 自动做 reshard。
Spanner 中的数据是存储在 schematized semi-relational table 上的。 在 commit 的时候把 timestamp 作为数据的 version。老版本的数据可以被垃圾回收，client 也可以读老数据。
Spanner 有两个特性是一般的分布式系统非常难实现的，
读和写操作的外部一致性。 在一个时间戳下，读操作是全局一致的。 能实现以上两点，是因为 Spanner 可以分配全球范围内保持一致的 commit timestamp。Spanner 的 timestamp 靠 TrueTime API 实现，甚至可以用 GPS 和原子钟来提高 TrueTime API 的精度。
Implementation 一个 Spanner 集群被称为一个 universe，如下图所示
Spanner 被组织成许多个 zone 的集合，zone 是管理部署的基本单元。一个数据中心可能会有多个 zone，zone 也是物理隔离的单元，例如，两个不同应用的数据就会被分散在两个 zone 上。
Zonemaster 把数据分配给 spanserver，spanserver 是真正存数据的地方 Client 从 location proxy 定位数据在哪个 spanserver 上 Universe master 主要是一个管理界面 Placement driver 周期性的与 spanserver 交互，进行负载均衡 Spanserver Software Stack 每一台 spanserver 上会存 100 到 1000 张表，每一张表上都有一个 Paxos 状态机，每一个 Paxos 状态机都会把自己的 metadata 和 log 存在 tablet 上。...</p></div><footer class=entry-footer><span title='2020-10-03 00:00:00 +0000 UTC'>2020-10-03</span>&nbsp;·&nbsp;Me</footer><a class=entry-link aria-label="post link to Spanner Distributed Database 阅读" href=https://runzhen.github.io/posts/spanner-paper-reading/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Kubernetes 的 Volume 和 StorageClass</h2></header><div class=entry-content><p>Kubernetes 的 Pod 可以 mount 很多种 Volume，常见的 volume 有
emptyDir hostPath configMap, secret persistentVolumeClaim nfs, gitRepo, cephfs, iscsi, cinder 等 其中，emptyDir 是最简单的一种，用于挂载一些临时文件，比如同一个 Pod 中两个 container 需要通过 unix socket 通信，那么把 socket 放在 emptyDir 中是最简单的方法。
甚至可以指定把这个抽象的目录放在内存，从而加快速度。
volumes: - name: html emptyDir: medium: Memory hostPath 是把数据直接存在 kubernetes 某个 worker node 上，这种方法一般不推荐使用，因为当 Pod 被调度到其他节点上后，数据就丢失了。
那么什么样的情况适合挂载 hostPath 呢？一些系统级的组件，需要挂载 node 上系统本身自带的一些文件时，比如需要读取 host 的 cert 目录，或者 etc 目录。常见的有 kube-system 空间下的 coreDNS 组件等。
当 Pod 中的程序需要把数据持久化到外部存储时，最推荐的用法是先在系统中定义 StorageClass，然后配合 persistentVolumeClaim (PVC) 和 persistentVolume (PV) 一起动态的分配空间。...</p></div><footer class=entry-footer><span title='2020-09-28 00:00:00 +0000 UTC'>2020-09-28</span>&nbsp;·&nbsp;Me</footer><a class=entry-link aria-label="post link to Kubernetes 的 Volume 和 StorageClass" href=https://runzhen.github.io/posts/kubernetes-volume-pv-pvc/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Kubernetes Headless Service</h2></header><div class=entry-content><p>问题起源于我用 envoy 对 grpc 做 Layer 7 负载均衡的时候，发现 traffic 永远被转发到了一个特定的 Pod，显然是配置出错了。
环境如下：
同一个 namespace 下部署了 2 个 grpc server，一个 envoy
$ kubectl get pod NAME READY STATUS RESTARTS AGE grpc-envoy-7684f49cb-9fv4h 1/1 Running 0 4h49m grpc-server-668bdd6576-2bvkz 1/1 Running 0 4h51m grpc-server-668bdd6576-tqzj4 1/1 Running 0 4h51m envoy 的 service 配置如下
apiVersion: v1 kind: Service metadata: name: grpc-envoy namespace: default labels: app: grpc-envoy spec: type: NodePort ports: - name: grpc port: 8080 targetPort: grpc nodePort: 30061 protocol: TCP selector: app: grpc-envoy grpc-server 的 service 配置如下...</p></div><footer class=entry-footer><span title='2020-09-20 00:00:00 +0000 UTC'>2020-09-20</span>&nbsp;·&nbsp;Me</footer><a class=entry-link aria-label="post link to Kubernetes Headless Service" href=https://runzhen.github.io/posts/kubernetes-headless-service/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Kubernetes Pod 中的 Pause 容器</h2></header><div class=entry-content><p>在一个运行 kubernetes 的节点上，我们能看到很多名叫 “pause” 的 container。比如
$ sudo docker ps | grep pause a4218d1d379b k8s.gcr.io/pause:3.1 "/pause" a2109bf3f0db k8s.gcr.io/pause:3.1 "/pause" 57cfa42e95d3 k8s.gcr.io/pause:3.1 "/pause" 仔细观察一下不难发现，每一个 Pod 都会对应一个 pause container。
在查阅了网上的一些资料以后，我总结了一下它大概有两个作用，
它是 Pod 中第一个启动的 container ，由它创建新的 linux namespace，其他 container 启动后再加入到这些 namespace 中。 在 Pod 的环境中充当 init process 的角色，它的 PID 是 1，负责回收所有僵尸进程。 说个题外话，在 docker 中，一个 container 启动时，Dockerfile 的 ENTRYPOINT 中指定的命令会成为这个 container 的 init process，PID 为 1.
顺便来看一下 pause 容器的实现，一共只有几十行 C 语言代码
static void sigdown(int signo) { psignal(signo, "Shutting down, got signal"); exit(0); } static void sigreap(int signo) { while (waitpid(-1, NULL, WNOHANG) > 0) ; } int main(int argc, char **argv) { if (getpid() !...</p></div><footer class=entry-footer><span title='2020-09-13 00:00:00 +0000 UTC'>2020-09-13</span>&nbsp;·&nbsp;Me</footer><a class=entry-link aria-label="post link to Kubernetes Pod 中的 Pause 容器" href=https://runzhen.github.io/posts/k8s-pod-pause-container/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Unicode 字符编码</h2></header><div class=entry-content><p>ASCII 我们熟悉的 ASCII 码可以说是字符编码的始祖了。它规定了常用的数字、符号、英文字母与二进制之间的对应关系。
ASCII 的缺点是字符集太少了，只能表示英文和数字，无法表示像中文，日文这样的符号。因此人们就设计出了 Unicode 字符集，囊括了几乎所有人类语言文字的符号。
Unicode Unicode 是一个字符集，而不是一种编码方式。 Unicode 相当于是给人类所有的符号一个独一无二的 ID，只要大家都是用这个 ID 表示字符，就不会出现乱码的问题。
因为 Unicode 是一个字符集，因此它不存在所谓的 “用几个字节表示 unicode” 这样的问题，这是具体的编码方式需要处理的事。
Unicode 把 ID 划分成了 17 组 (Plane)，每组有 65536 个字符，编号可以用 U+[XX]YYYY 这样的形式表示，每一位是一个十六进制数字，其中 XX 代表组编号，从 0 到 0x10，一共17个，YYYY 代表这一组中的字符编号，一共 65536 个。
其中第 0 组叫 Basic Multilingual Plane，简称 BMP，它是 Unicode 中最基础和最常用的一部分，码点范围是U+0000 ~ U+FFFF，包含了我们常用的英文和汉字。
UFT-8 UTF-8 是 Unicode 具体的编码方式，除此之外还要 UTF-16, UTF-32 等等。
为什么需要编码方式呢？ 直接用 Unicode 的 ID 不就行了吗？ 因为我们需要节省存储空间。
UTF-8 是一种变长的编码方式，它可以使用 1-4 个字节表示一个符号，编码规则如下...</p></div><footer class=entry-footer><span title='2020-08-16 00:00:00 +0000 UTC'>2020-08-16</span>&nbsp;·&nbsp;Me</footer><a class=entry-link aria-label="post link to Unicode 字符编码" href=https://runzhen.github.io/posts/unicode-utf8-encode/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Kubernetes Dashboard 添加 Auth</h2></header><div class=entry-content><p>组里的 k8s cluster Dashboard 一直没有设置登录，导致所有可以 ping IP 的人都可以登录管理界面，这样显然是不合理的，应该要设置几个不同权限的账户，并且开启 Dashboard 的 Basic Auth。
关于开启 Dashboard Basic Auth 网上有不少资料，但是在实际操作中还是遇到几个莫名其妙的坑。
创建用户文件 根据实际需求，并不需要使用 LDAP 等等复杂的登录方式，我们只需要一个 admin 账户，再加上低权限的只读 view 账户，以及有修改权限的 edit 账户就足够。
因此，只要添加 /etc/kubernetes/pki/basic_auth_file 文件即可。
vi /etc/kubernetes/pki/basic_auth_file password,username,1 需要注意的一个坑是用户名密码的顺序是反过来的（如上面所示），否则在 dashboard 上怎么输入都提示不对。
修改 API Server 配置 修改 /etc/kubernetes/manifests/kube-apiserver.yaml 加入一个启动参数
vim /etc/kubernetes/manifests/kube-apiserver.yaml - --basic-auth-file=/etc/kubernetes/pki/basic_auth_file 这里又遇到一个坑，在我们的 k8s Master 节点这个目录下有两个文件，第一个叫 kube-apiserver.yaml， 另一个是 kube-apiserver_xxx.yaml。
本来我以为第一个是实际的配置文件，第二个应该是其他人配置时 copy 的一个备份。
然而实际并不是，真正被使用的是第二个文件，这点让人匪夷所思，我花了好久才发现这个坑，但是我始终没找到哪里指定了让 api server 读取 kube-apiserver_xxx.yaml 而不是 kube-apiserver.yaml 。
重启 API Server 很多资料上都会把重启 api server 一笔带过，但是都不写具体怎么操作，k8s 上并不是简单删除一个 pod 就算重启的。...</p></div><footer class=entry-footer><span title='2020-08-04 00:00:00 +0000 UTC'>2020-08-04</span>&nbsp;·&nbsp;Me</footer><a class=entry-link aria-label="post link to Kubernetes Dashboard 添加 Auth" href=https://runzhen.github.io/posts/k8s-dashboard-auth/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>在虚拟机中使用 GPU 计算</h2></header><div class=entry-content><p>本文介绍如何在 Linux 虚拟机中直接使用 GPU 做科学计算，要达到这个目的，需要满足下面几个条件：
物理主机使用 VMWare ESXi 作为虚拟化的 VMM，并且版本最好大于等于 6.5 使用的是 Nvidia GPU 的显卡 Linux 虚拟机 OS 没有限制，我使用的是 ubuntu ESXi 开启显卡直通 假设已经安装好了 ESXi，通过 WebUI 进入 Host 的 Manage 界面，点击 Hardware，如图
把 nVidia 开头的这几个全部选中，然后 “Active”， 表示开启 PCI 设备的直通 (passthrough)。
重启物理主机。
配置虚拟机 创建一个新虚拟机，或者修改已有的虚拟机，
点击 Edit，VM Options ，在 Advanced 里面点击 Edit configuration 。
增加一条配置参数 hypervisor.cpuid.v0, 对应的值为 FALSE，这一步的目的是让驱动把虚拟机当做物理机来处理。
另一需要修改的地方让虚拟机硬件配置内存大小下面勾选 “Reserve all guest memory (All locked)”，让虚拟机启动时一次性获取物理主机内存，而不是按需获取。
到这里，主机和虚拟机的配置就全部完成了，接下来是驱动软件的安装
虚拟机安装驱动 重启并进入虚拟机 CLI，首先可以确认一下 GPU 已经被直通给了虚拟机，这一步不是必须要做，但检查一下没坏处。
$ lshw | grep display $ sudo apt install ubuntu-drivers-common $ ubuntu-drivers devices 接下来，禁用系统自带的开源 nouveau 驱动...</p></div><footer class=entry-footer><span title='2020-06-13 00:00:00 +0000 UTC'>2020-06-13</span>&nbsp;·&nbsp;Me</footer><a class=entry-link aria-label="post link to 在虚拟机中使用 GPU 计算" href=https://runzhen.github.io/posts/nvidia-gpu-pass-through/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://runzhen.github.io/page/5/>«&nbsp;Prev&nbsp;
</a><a class=next href=https://runzhen.github.io/page/7/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://runzhen.github.io/>Mind in the Wind</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>